---
title: Mixture of Experts (MoE)
---

# Mixture of Experts (MoE)

MoE models use sparse activation—only a subset of "expert" subnetworks are active per input—enabling larger capacity with manageable compute costs.