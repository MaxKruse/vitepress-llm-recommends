import{_ as o,c as t,o as r,af as n}from"./chunks/framework.CuQK53I8.js";const h=JSON.parse('{"title":"Inference","description":"","frontmatter":{"title":"Inference"},"headers":[],"relativePath":"inference/index.md","filePath":"inference/index.md"}'),a={name:"inference/index.md"};function i(s,e,l,c,d,u){return r(),t("div",null,[...e[0]||(e[0]=[n('<h1 id="inference" tabindex="-1">Inference <a class="header-anchor" href="#inference" aria-label="Permalink to ‚ÄúInference‚Äù">‚Äã</a></h1><p>This section covers inference strategies, optimization techniques, and deployment considerations for AI models‚Äîparticularly those that can run <strong>locally</strong> on consumer hardware.</p><blockquote><p>‚ö†Ô∏è <strong>Important Disclaimer Regarding Ollama</strong><br> While Ollama offers a convenient interface for running local LLMs, <strong>serious concerns have been raised</strong> about its approach to open source. The project has been criticized for:</p><ul><li><strong>Lack of true open-source commitment</strong>: Despite using open weights from community models, Ollama itself is <strong>not fully open source</strong>‚Äîits server component is proprietary.</li><li><strong>Commercialization of community work</strong>: Ollama packages and redistributes models created and shared by the open-source community, often without clear attribution or contribution back to those projects.</li><li><strong>Pattern of behavior</strong>: The founders have a history tied to Docker, a project that similarly started as open-source infrastructure but later shifted toward aggressive commercialization, leaving many community contributors disillusioned.</li></ul><p>Given these issues, <strong>we strongly advise caution</strong>. If you value software freedom, transparency, and sustainable open-source ecosystems, consider using fully open alternatives like <code>llama.cpp</code> or transparent tools like <code>LM Studio</code>.</p><p><strong>Support tools that support the community‚Äînot those that extract from it.</strong></p></blockquote><h2 id="local-inference-engines" tabindex="-1">Local Inference Engines <a class="header-anchor" href="#local-inference-engines" aria-label="Permalink to ‚ÄúLocal Inference Engines‚Äù">‚Äã</a></h2><p>Running models locally offers privacy, reduced latency, and offline capabilities. Below is a quick guide to popular open-source inference engines and when you might choose each.</p><h3 id="llama-cpp" tabindex="-1"><a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noreferrer">llama.cpp</a> <a class="header-anchor" href="#llama-cpp" aria-label="Permalink to ‚Äúllama.cpp‚Äù">‚Äã</a></h3><ul><li><strong>Best for</strong>: Flexible local inference across <strong>CPU-only</strong>, <strong>GPU-only</strong>, or <strong>hybrid CPU+GPU</strong> setups.</li><li><strong>Pros</strong>: Fully open source, supports a wide range of architectures (x86, ARM, Apple Silicon), and offers multiple backends including CUDA (NVIDIA), Metal (Apple) and Vulkan,. Excellent support for quantized GGUF models, enabling efficient inference even on modest hardware.</li><li><strong>Use when</strong>: You want maximum control, portability, and performance across diverse hardware‚Äîlaptops, desktops, servers, or mobile devices.</li></ul><h3 id="lm-studio" tabindex="-1"><a href="https://lmstudio.ai/" target="_blank" rel="noreferrer">LM Studio</a> <a class="header-anchor" href="#lm-studio" aria-label="Permalink to ‚ÄúLM Studio‚Äù">‚Äã</a></h3><ul><li><strong>Best for</strong>: Desktop GUI users who want to explore and chat with local models.</li><li><strong>Pros</strong>: Intuitive interface, model discovery, built-in chat and local server mode.</li><li><strong>Note</strong>: LM Studio is built on top of <strong>llama.cpp</strong> for Windows/Linux and <strong>MLX</strong> (Apple&#39;s machine learning framework) on macOS, giving it strong performance and compatibility with GGUF models while providing a polished user experience.</li></ul><h3 id="vllm" tabindex="-1"><a href="https://vllm.readthedocs.io/" target="_blank" rel="noreferrer">vLLM</a> <a class="header-anchor" href="#vllm" aria-label="Permalink to ‚ÄúvLLM‚Äù">‚Äã</a></h3><ul><li><strong>Best for</strong>: High-throughput GPU inference (not ideal for low-resource devices).</li><li><strong>Pros</strong>: Extremely fast, supports continuous batching and PagedAttention.</li><li><strong>Use when</strong>: You&#39;re deploying on a server with one or more powerful NVIDIA GPUs and need scalability and low latency for many concurrent requests.</li></ul><h2 id="quick-decision-guide" tabindex="-1">Quick Decision Guide <a class="header-anchor" href="#quick-decision-guide" aria-label="Permalink to ‚ÄúQuick Decision Guide‚Äù">‚Äã</a></h2><table tabindex="0"><thead><tr><th>Scenario</th><th>Recommended Engine(s)</th></tr></thead><tbody><tr><td>CPU-only or mixed CPU/GPU inference</td><td><code>llama.cpp</code></td></tr><tr><td>Desktop GUI for chatting/testing</td><td><code>LM Studio</code></td></tr><tr><td>Server deployment with NVIDIA GPU(s)</td><td><code>vLLM</code></td></tr></tbody></table><blockquote><p>üí° <strong>Tip</strong>: Most local LLMs today use <strong>quantized</strong> versions (e.g., GGUF, AWQ, GPTQ) to reduce memory usage and run on consumer hardware. Always check model compatibility with your chosen engine.</p></blockquote><p>For up-to-date benchmarks and model support, consult each project&#39;s documentation and community resources.</p>',15)])])}const m=o(a,[["render",i]]);export{h as __pageData,m as default};
