import{_ as S,x as y,h as r,c as i,o as c,j as a,af as R,a as v,ag as b,ah as w,G as M,H as I,t as u,O as A,n as Q}from"./chunks/framework.t7omRjYa.js";const q={class:"controls"},T={class:"control-group"},U=["value"],V={class:"control-group"},F=["value"],N={class:"result"},Y=JSON.parse('{"title":"Instruct Models","description":"","frontmatter":{"title":"Instruct Models"},"headers":[],"relativePath":"recommendations/instruct/index.md","filePath":"recommendations/instruct/index.md"}'),C={name:"recommendations/instruct/index.md"},G=Object.assign(C,{setup(L){const s=y(16),d=y(8),_=[16,32,64,128],B=[0,4,6,8,12,16,24,32],x=[{ramMin:128,vramMin:32,model:"Mistral Small Q8 or Qwen3 30B Instruct 2507 BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:24,model:"Mistral Small Q6 or Qwen3 30B Instruct 2507 BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:0,model:"Qwen3 30B Instruct 2507 BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:64,vramMin:32,model:"Mistral Small Q8 or Qwen3 30B Instruct 2507 BF16",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:24,model:"Mistral Small Q6 or Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:16,model:"Qwen3 30B Instruct Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:64,vramMin:12,model:"Qwen3 30B Instruct Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:64,vramMin:8,model:"Qwen3 30B Instruct Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:64,vramMin:6,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:64,vramMin:4,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:64,vramMin:0,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:32,vramMin:32,model:"Mistral Small Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:24,model:"Mistral Small Q6 or Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:16,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:32,vramMin:8,model:"Qwen3 30B Instruct 2507 Q6",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:32,vramMin:4,model:"Qwen3 4B Instruct 2507 BF16",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:16,vramMin:32,model:"Mistral Small Q8",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:24,model:"Mistral Small Q6",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:12,model:"Qwen3 4B Instruct 2507 BF16",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:4,model:"Qwen3 4B Instruct 2507 Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"}],n=r(()=>{const l=x.find(e=>s.value>=e.ramMin&&d.value>=e.vramMin);return l?{model:l.model,color:l.color,bg:l.bg}:{model:"Not recommended",color:"var(--vp-c-text-3)",bg:"transparent"}}),m=r(()=>n.value.model!=="Not recommended"),o=r(()=>n.value.model.toLowerCase()),f=r(()=>o.value.includes("bf16")||o.value.includes("gpt oss")),g=r(()=>o.value.includes("q6")||o.value.includes("q8")),p=r(()=>o.value.includes("q4")),h=r(()=>o.value.includes("4b")),k=r(()=>m.value?h.value?{"recommended-4b":!0}:f.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:p.value?{"recommended-warning":!0}:{}:{"not-recommended":!0}),P=r(()=>m.value?h.value?{"recommended-4b":!0}:f.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:p.value?{"recommended-warning":!0}:{}:{"not-recommended":!0});return(l,e)=>(c(),i("div",null,[e[5]||(e[5]=a("h1",{id:"instruct-tuned-models",tabindex:"-1"},[v("Instruct-Tuned Models "),a("a",{class:"header-anchor",href:"#instruct-tuned-models","aria-label":"Permalink to “Instruct-Tuned Models”"},"​")],-1)),e[6]||(e[6]=a("p",null,"Instruct-tuned models are AI assistants trained specifically to understand and follow your directions—whether you're asking for help writing an email, summarizing a document, planning a trip, or explaining a complex idea. Unlike raw base models, these are fine-tuned to respond helpfully, clearly, and on-topic.",-1)),e[7]||(e[7]=a("p",null,[v("Use the selector below to find the best "),a("strong",null,"instruct-tuned"),v(" model that matches your computer’s capabilities:")],-1)),a("div",{class:Q(["model-selector",k.value])},[a("div",q,[a("div",T,[e[2]||(e[2]=a("label",{for:"ram-select"},"RAM (GB)",-1)),b(a("select",{id:"ram-select","onUpdate:modelValue":e[0]||(e[0]=t=>s.value=t)},[(c(),i(M,null,I(_,t=>a("option",{key:t,value:t},u(t),9,U)),64))],512),[[w,s.value,void 0,{number:!0}]])]),a("div",V,[e[3]||(e[3]=a("label",{for:"vram-select"},"VRAM (GB)",-1)),b(a("select",{id:"vram-select","onUpdate:modelValue":e[1]||(e[1]=t=>d.value=t)},[(c(),i(M,null,I(B,t=>a("option",{key:t,value:t},u(t),9,F)),64))],512),[[w,d.value,void 0,{number:!0}]])])]),a("div",N,[e[4]||(e[4]=a("strong",null,"Recommended model:",-1)),a("span",{class:Q(["model-name",P.value]),style:A({backgroundColor:n.value.bg,color:n.value.color})},u(n.value.model),7)])],2),e[8]||(e[8]=R('<h2 id="how-to-use-instruct-models-effectively" tabindex="-1" data-v-14696f68>How to Use Instruct Models Effectively <a class="header-anchor" href="#how-to-use-instruct-models-effectively" aria-label="Permalink to “How to Use Instruct Models Effectively”" data-v-14696f68>​</a></h2><p data-v-14696f68>These models shine when given clear, thoughtful instructions. Follow these tips to get the most helpful, accurate, and reliable responses—whether you&#39;re drafting messages, researching topics, or organizing your day.</p><h3 id="_1-always-choose-an-instruct-model" tabindex="-1" data-v-14696f68>1. <strong data-v-14696f68>Always Choose an “Instruct” Model</strong> <a class="header-anchor" href="#_1-always-choose-an-instruct-model" aria-label="Permalink to “1. Always Choose an “Instruct” Model”" data-v-14696f68>​</a></h3><ul data-v-14696f68><li data-v-14696f68>Look for <strong data-v-14696f68><code data-v-14696f68>instruct</code></strong> in the model name (e.g., <code data-v-14696f68>qwen3 30b instruct</code>, <code data-v-14696f68>mistral small</code>). These are specially trained to follow directions.</li><li data-v-14696f68>Non-instruct models may ignore your request or give generic, off-topic replies.</li></ul><h3 id="_2-pick-the-right-version-for-your-hardware" tabindex="-1" data-v-14696f68>2. <strong data-v-14696f68>Pick the Right Version for Your Hardware</strong> <a class="header-anchor" href="#_2-pick-the-right-version-for-your-hardware" aria-label="Permalink to “2. Pick the Right Version for Your Hardware”" data-v-14696f68>​</a></h3><table tabindex="0" data-v-14696f68><thead data-v-14696f68><tr data-v-14696f68><th data-v-14696f68>Version</th><th data-v-14696f68>Best For</th><th data-v-14696f68>Notes</th></tr></thead><tbody data-v-14696f68><tr data-v-14696f68><td data-v-14696f68><code data-v-14696f68>bf16</code> / <code data-v-14696f68>f16</code></td><td data-v-14696f68>Best quality, powerful GPUs</td><td data-v-14696f68>Highest accuracy; needs lots of VRAM</td></tr><tr data-v-14696f68><td data-v-14696f68><code data-v-14696f68>q6</code> / <code data-v-14696f68>q8</code></td><td data-v-14696f68>Great balance of speed and clarity</td><td data-v-14696f68>Works well on most modern laptops or desktops</td></tr><tr data-v-14696f68><td data-v-14696f68><code data-v-14696f68>q4</code></td><td data-v-14696f68>Limited VRAM (e.g., older or entry-level GPUs)</td><td data-v-14696f68>Faster but may miss finer details or nuance</td></tr></tbody></table><h3 id="_3-use-your-gpu-fully-if-available" tabindex="-1" data-v-14696f68>3. <strong data-v-14696f68>Use Your GPU Fully (If Available)</strong> <a class="header-anchor" href="#_3-use-your-gpu-fully-if-available" aria-label="Permalink to “3. Use Your GPU Fully (If Available)”" data-v-14696f68>​</a></h3><ul data-v-14696f68><li data-v-14696f68>In apps like <strong data-v-14696f68>LM Studio</strong>, enable full GPU offloading to keep things fast and smooth.</li><li data-v-14696f68>Example: For <code data-v-14696f68>qwen3 30b instruct</code>, try loading <strong data-v-14696f68>all layers</strong> onto the GPU if you have enough VRAM.</li><li data-v-14696f68>Partial use of the GPU can cause slowdowns as the system swaps data between memory and graphics card.</li></ul><h3 id="_4-mind-the-context-length" tabindex="-1" data-v-14696f68>4. <strong data-v-14696f68>Mind the Context Length</strong> <a class="header-anchor" href="#_4-mind-the-context-length" aria-label="Permalink to “4. Mind the Context Length”" data-v-14696f68>​</a></h3><ul data-v-14696f68><li data-v-14696f68>All recommended models support <strong data-v-14696f68>16,000+ tokens</strong> of context—enough for long emails, reports, or multi-turn conversations.</li><li data-v-14696f68>Very long inputs use more memory; if your system feels sluggish, shorten your prompt or reduce the model’s quantization.</li></ul><h3 id="_5-be-clear-and-specific-in-your-requests" tabindex="-1" data-v-14696f68>5. <strong data-v-14696f68>Be Clear and Specific in Your Requests</strong> <a class="header-anchor" href="#_5-be-clear-and-specific-in-your-requests" aria-label="Permalink to “5. Be Clear and Specific in Your Requests”" data-v-14696f68>​</a></h3><ul data-v-14696f68><li data-v-14696f68>❌ <em data-v-14696f68>“Tell me about climate change.”</em></li><li data-v-14696f68>✅ <em data-v-14696f68>“Explain the main causes of climate change in simple terms, as if I’m 15 years old.”</em></li><li data-v-14696f68>The more precise your instruction—goal, audience, format, or examples—the better the response.</li></ul><h3 id="_6-avoid-not-recommended-setups" tabindex="-1" data-v-14696f68>6. <strong data-v-14696f68>Avoid “Not Recommended” Setups</strong> <a class="header-anchor" href="#_6-avoid-not-recommended-setups" aria-label="Permalink to “6. Avoid “Not Recommended” Setups”" data-v-14696f68>​</a></h3><ul data-v-14696f68><li data-v-14696f68>If the tool says <strong data-v-14696f68>“Not recommended,”</strong> your device likely can’t run even the smallest instruct model well.</li><li data-v-14696f68>Trying to force it may result in <strong data-v-14696f68>slow replies, crashes, or unreliable answers</strong>.</li><li data-v-14696f68>In that case, consider using a cloud-based service (like OpenRouter or Together.ai) or upgrading your hardware.</li></ul><h3 id="_7-watch-your-system-resources" tabindex="-1" data-v-14696f68>7. <strong data-v-14696f68>Watch Your System Resources</strong> <a class="header-anchor" href="#_7-watch-your-system-resources" aria-label="Permalink to “7. Watch Your System Resources”" data-v-14696f68>​</a></h3><ul data-v-14696f68><li data-v-14696f68><strong data-v-14696f68>Windows</strong>: Open Task Manager → Performance → GPU</li><li data-v-14696f68><strong data-v-14696f68>Linux</strong>: Use <code data-v-14696f68>nvidia-smi</code> (NVIDIA) or <code data-v-14696f68>radeontop</code> (AMD)</li><li data-v-14696f68>If VRAM usage goes above <strong data-v-14696f68>90%</strong>, try a lighter model version (e.g., switch from <code data-v-14696f68>q8</code> to <code data-v-14696f68>q6</code>) or shorten your input.</li></ul><p data-v-14696f68>By matching the right model to your hardware and giving clear, purposeful instructions, you’ll enjoy a smooth, intelligent assistant experience—perfect for everyday tasks, learning, planning, and more.</p>',17))]))}}),z=S(G,[["__scopeId","data-v-14696f68"]]);export{Y as __pageData,z as default};
