import{_ as t,c as a,o as i,af as r}from"./chunks/framework.t7omRjYa.js";const f=JSON.parse('{"title":"Model Types","description":"","frontmatter":{"title":"Model Types"},"headers":[],"relativePath":"model-types/index.md","filePath":"model-types/index.md"}'),o={name:"model-types/index.md"};function s(n,e,l,d,p,c){return i(),a("div",null,[...e[0]||(e[0]=[r('<h1 id="model-types" tabindex="-1">Model Types <a class="header-anchor" href="#model-types" aria-label="Permalink to â€œModel Typesâ€">â€‹</a></h1><p>Modern AI systems leverage different architectural approaches to balance performance, efficiency, and capability. This section explores two fundamental model paradigms:</p><h2 id="dense-models" tabindex="-1">Dense Models <a class="header-anchor" href="#dense-models" aria-label="Permalink to â€œDense Modelsâ€">â€‹</a></h2><p><strong>Traditional, unified architectures</strong> where every parameter participates in every inference.</p><ul><li>Consistent computational requirements</li><li>Simpler training and deployment</li></ul><p><a href="./dense/">Learn more about Dense Models â†’</a></p><h2 id="mixture-of-experts-moe" tabindex="-1">Mixture-of-Experts (MoE) <a class="header-anchor" href="#mixture-of-experts-moe" aria-label="Permalink to â€œMixture-of-Experts (MoE)â€">â€‹</a></h2><p><strong>Sparse, conditional architectures</strong> that activate specialized sub-networks (&quot;experts&quot;) based on input.</p><ul><li>Dynamic computational scaling</li><li>Specialized knowledge routing</li></ul><p><a href="./mixture-of-experts/">Learn more about Mixture-of-Experts â†’</a></p><hr><blockquote><p>ðŸ’¡ <strong>Key Trade-off</strong>: Dense models offer simplicity and predictability, while MoE architectures provide scalability and efficiency at the cost of implementation complexity.</p></blockquote>',12)])])}const u=t(o,[["render",s]]);export{f as __pageData,u as default};
