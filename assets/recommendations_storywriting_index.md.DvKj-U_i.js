import{M as n}from"./chunks/ModelSelector.DQGW46EL.js";import{c as o,o as i,j as t,J as s,af as l,a}from"./chunks/framework.CuQK53I8.js";const p=JSON.parse('{"title":"Storywriting","description":"","frontmatter":{"title":"Storywriting"},"headers":[],"relativePath":"recommendations/storywriting/index.md","filePath":"recommendations/storywriting/index.md"}'),m={name:"recommendations/storywriting/index.md"},f=Object.assign(m,{setup(d){const r=[{ramMin:128,vramMin:32,models:[{"GPT OSS 120B":{parameters:120,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q8_K_XL"}}],usefulness:1},{ramMin:128,vramMin:16,models:[{"GPT OSS 120B":{parameters:120,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q4_K_XL"}}],usefulness:.9},{ramMin:128,vramMin:12,models:[{"GPT OSS 120B":{parameters:120,quantization:"MXFP4"}},{"Gemma 3 12B":{parameters:12,quantization:"Q6_K_XL"}}],usefulness:.8},{ramMin:128,vramMin:0,models:[{"GPT OSS 120B":{parameters:120,quantization:"MXFP4"}}],usefulness:.7},{ramMin:64,vramMin:32,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q8_K_XL"}}],usefulness:.8},{ramMin:64,vramMin:24,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q6_K_XL"}}],usefulness:.7},{ramMin:64,vramMin:12,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 12B":{parameters:12,quantization:"Q6_K_XL"}}],usefulness:.6},{ramMin:64,vramMin:0,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}}],usefulness:.5},{ramMin:32,vramMin:32,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q8_K_XL"}}],usefulness:.7},{ramMin:32,vramMin:24,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q6_K_XL"}}],usefulness:.6},{ramMin:32,vramMin:16,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 12B":{parameters:12,quantization:"Q8_K_XL"}}],usefulness:.5},{ramMin:32,vramMin:12,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 12B":{parameters:12,quantization:"Q6_K_XL"}}],usefulness:.4},{ramMin:32,vramMin:8,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 12B":{parameters:12,quantization:"Q4_K_XL"}}],usefulness:.3},{ramMin:32,vramMin:6,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}}],usefulness:.25},{ramMin:16,vramMin:32,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q8_K_XL"}}],usefulness:.5},{ramMin:16,vramMin:24,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q6_K_XL"}}],usefulness:.4},{ramMin:16,vramMin:16,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 12B":{parameters:12,quantization:"Q8_K_XL"}}],usefulness:.3},{ramMin:16,vramMin:12,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 12B":{parameters:12,quantization:"Q6_K_XL"}}],usefulness:.25},{ramMin:16,vramMin:8,models:[{"Gemma 3 12B":{parameters:12,quantization:"Q4_K_XL"}}],usefulness:.2}];return(u,e)=>(i(),o("div",null,[e[0]||(e[0]=t("h1",{id:"storywriting-creative-writing",tabindex:"-1"},[a("Storywriting & Creative Writing "),t("a",{class:"header-anchor",href:"#storywriting-creative-writing","aria-label":"Permalink to “Storywriting & Creative Writing”"},"​")],-1)),e[1]||(e[1]=t("p",null,[t("strong",null,"Narrative-tuned models"),a(" optimized for immersive storytelling, rich character arcs, evocative prose, and genre-aware stylistic control. These models excel at crafting original fiction, expanding existing worlds, generating dialogue with emotional nuance, and maintaining long-range plot coherence—even across tens of thousands of tokens.")],-1)),e[2]||(e[2]=t("p",null,[a("Use the selector below to find the best "),t("strong",null,"creative-writing-optimized"),a(" model for your hardware:")],-1)),s(n,{modelDefinitions:r}),e[3]||(e[3]=l('<blockquote><p><strong>“Not recommended” means unreliable narrative output</strong> If the selector returns “Not recommended,” your system likely lacks the resources to run even the smallest narrative-tuned model effectively. In such cases, stories may suffer from <strong>incoherent plot shifts</strong>, <strong>flat characters</strong>, or <strong>repetitive phrasing</strong>—often worse than drafting manually.</p></blockquote><hr><h2 id="how-to-use-narrative-tuned-models-for-storywriting" tabindex="-1">How to Use Narrative-Tuned Models for Storywriting <a class="header-anchor" href="#how-to-use-narrative-tuned-models-for-storywriting" aria-label="Permalink to “How to Use Narrative-Tuned Models for Storywriting”">​</a></h2><p>These models are designed for <strong>long-form creative expression</strong>, not factual QA or code generation. Follow these guidelines to maximize emotional depth, stylistic control, and narrative consistency.</p><h3 id="_1-quantization-style-trade-offs" tabindex="-1">1. <strong>Quantization &amp; Style Trade-offs</strong> <a class="header-anchor" href="#_1-quantization-style-trade-offs" aria-label="Permalink to “1. Quantization &amp; Style Trade-offs”">​</a></h3><table tabindex="0"><thead><tr><th>Quant</th><th>Use Case</th><th>Creative Impact</th></tr></thead><tbody><tr><td><code>bf16</code> / <code>f16</code></td><td>Literary nuance, poetic rhythm, complex metaphors</td><td>Best for lyrical prose, experimental fiction, or deep POV</td></tr><tr><td><code>q8</code></td><td>Balanced fluency</td><td>Ideal for most genres (fantasy, sci-fi, romance) with strong pacing</td></tr><tr><td><code>q6</code> / <code>q4</code></td><td>Low-resource systems</td><td>May flatten emotional arcs or repeat phrases—use only when necessary</td></tr></tbody></table><blockquote><p>📝 <strong>Tip</strong>: For dialogue-heavy scenes, <code>q8</code> or higher preserves vocal distinctiveness better than <code>q4</code>.</p></blockquote><h3 id="_2-full-gpu-offload-is-essential-for-long-context" tabindex="-1">2. <strong>Full GPU Offload Is Essential for Long Context</strong> <a class="header-anchor" href="#_2-full-gpu-offload-is-essential-for-long-context" aria-label="Permalink to “2. Full GPU Offload Is Essential for Long Context”">​</a></h3><ul><li><strong>Narrative context lives in VRAM</strong>—always enable <strong>full GPU offload</strong> (e.g., 48/48 layers).</li><li>This ensures the model can reference earlier plot points, character traits, or worldbuilding details without slowing down.</li></ul><h3 id="_3-prompt-with-worldbuilding-voice-cues" tabindex="-1">3. <strong>Prompt with Worldbuilding &amp; Voice Cues</strong> <a class="header-anchor" href="#_3-prompt-with-worldbuilding-voice-cues" aria-label="Permalink to “3. Prompt with Worldbuilding &amp; Voice Cues”">​</a></h3><p>Creative models respond best to <strong>rich, sensory prompts</strong>:</p><ul><li><p>❌ <em>“Write a fantasy story.”</em></p></li><li><p>✅ <em>“Write a 500-word scene where a disillusioned clockwork mage in a steampunk city discovers her automaton companion has developed emotions. Use close third-person POV, melancholic tone, and include tactile details (oil, brass, rain).”</em></p></li><li><p>Include:</p><ul><li>Genre &amp; subgenre (e.g., “cozy fantasy,” “cyberpunk noir”)</li><li>Character backstory or emotional state</li><li>Desired pacing (e.g., “slow-burn tension,” “rapid action sequence”)</li><li>Stylistic references (e.g., “like Neil Gaiman meets Becky Chambers”)</li></ul><blockquote><p>**For additional resources, check out <a href="https://sillytavernai.com/" target="_blank" rel="noreferrer">SillyTavernAI</a> and their community.</p></blockquote></li></ul><h3 id="_4-avoid-none-configurations" tabindex="-1">4. <strong>Avoid “None” Configurations</strong> <a class="header-anchor" href="#_4-avoid-none-configurations" aria-label="Permalink to “4. Avoid “None” Configurations”">​</a></h3><ul><li>If your hardware yields <strong>“none”</strong>, do <strong>not</strong> force a model load via heavy CPU offloading.</li><li>You&#39;ll get <strong>incoherent timelines</strong>, <strong>OOC (out-of-character) dialogue</strong>, or <strong>generic tropes</strong>—worse than writing unassisted.</li><li>Alternatives: <ul><li>Use cloud inference (e.g., <a href="https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it" target="_blank" rel="noreferrer">aistudio.google.com</a> )</li><li>Upgrade to ≥8 GB VRAM for basic story continuity</li></ul></li></ul><h3 id="_7-monitor-context-window-usage" tabindex="-1">7. <strong>Monitor Context Window Usage</strong> <a class="header-anchor" href="#_7-monitor-context-window-usage" aria-label="Permalink to “7. Monitor Context Window Usage”">​</a></h3><ul><li>Creative writing benefits from <strong>long context</strong> (16K+ tokens) to maintain continuity.</li><li>If VRAM usage exceeds 90%, consider: <ul><li>Reducing context length (but risk losing plot threads)</li><li>Switching to a lower quant (e.g., <code>bf16</code> → <code>q8</code>)</li><li>Using <strong>chunked generation</strong> (write scene-by-scene with memory prompts)</li></ul></li></ul><hr><p>By aligning your hardware, model choice, and prompt design with these principles, you&#39;ll unlock <strong>emotionally resonant, stylistically rich storytelling</strong>—whether you&#39;re drafting a novel, exploring alternate universes, or breathing life into original characters.</p>',18))]))}});export{p as __pageData,f as default};
