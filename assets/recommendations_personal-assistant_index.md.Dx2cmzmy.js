import{_ as A,x as f,h as o,c as d,o as c,af as b,j as e,ag as y,ah as k,G as M,H as _,t as v,O as G,n as w}from"./chunks/framework.t7omRjYa.js";const C={class:"controls"},O={class:"control-group"},I=["value"],Q={class:"control-group"},q=["value"],z={class:"result"},V=JSON.parse('{"title":"Personal Assistant","description":"","frontmatter":{"title":"Personal Assistant"},"headers":[],"relativePath":"recommendations/personal-assistant/index.md","filePath":"recommendations/personal-assistant/index.md"}'),U={name:"recommendations/personal-assistant/index.md"},R=Object.assign(U,{setup(F){const i=f(16),l=f(8),S=[16,32,64,128],x=[0,4,6,8,12,16,24,32],B=[{ramMin:128,vramMin:32,model:"GPT OSS 120B or Qwen3 30B Instruct BF16 or Mistral Small 3.2 Q8",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:24,model:"GPT OSS 120B or Qwen3 30B Instruct BF16 or Mistral Small 3.2 Q6",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:0,model:"GPT OSS 120B or Qwen3 30B Instruct 2507 BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:64,vramMin:24,model:"GPT OSS 20B or Qwen3 30B Instruct 2507 BF16",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:0,model:"GPT OSS 20B or Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:24,model:"GPT OSS 20B or Gemma 3 27B Q4",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:8,model:"GPT OSS 20B or Gemma 3 12B Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:0,model:"GPT OSS 20B",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:16,vramMin:32,model:"GPT OSS 20B or Gemma 3 27B Q8",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:24,model:"GPT OSS 20B or Gemma 3 27B Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:12,model:"GPT OSS 20B",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:8,model:"Gemma 3 12B Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:4,model:"Qwen3 4B Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"}],n=o(()=>{const s=B.find(a=>i.value>=a.ramMin&&l.value>=a.vramMin);return s?{model:s.model,color:s.color,bg:s.bg}:{model:"Not recommended",color:"var(--vp-c-text-3)",bg:"transparent"}}),u=o(()=>n.value.model!=="Not recommended"),r=o(()=>n.value.model.toLowerCase()),m=o(()=>r.value.includes("bf16")||r.value.includes("gpt oss")),g=o(()=>r.value.includes("q6")||r.value.includes("q8")),p=o(()=>r.value.includes("q4")),h=o(()=>r.value.includes("4b")),P=o(()=>u.value?h.value?{"recommended-4b":!0}:m.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:p.value?{"recommended-warning":!0}:{}:{"not-recommended":!0}),T=o(()=>u.value?h.value?{"recommended-4b":!0}:m.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:p.value?{"recommended-warning":!0}:{}:{"not-recommended":!0});return(s,a)=>(c(),d("div",null,[a[5]||(a[5]=b('<h1 id="personal-assistant-use-case" tabindex="-1" data-v-51815719>Personal Assistant Use Case <a class="header-anchor" href="#personal-assistant-use-case" aria-label="Permalink to ‚ÄúPersonal Assistant Use Case‚Äù" data-v-51815719>‚Äã</a></h1><p data-v-51815719>Recommendations for models that excel at memory, context retention, and personalized interactions.</p><blockquote data-v-51815719><p data-v-51815719>üí° <strong data-v-51815719>Note</strong>: For personal assistant tasks‚Äîsuch as recalling preferences, maintaining conversation history, managing schedules, or adapting tone over time‚Äî<strong data-v-51815719>instruct-tuned models with strong long-context handling</strong> are preferred over thinking-tuned variants. These models prioritize coherence, empathy, and user-specific adaptation over raw analytical power.</p></blockquote><p data-v-51815719>Use the selector below to find the best <strong data-v-51815719>assistant-like</strong> model for your hardware:</p>',4)),e("div",{class:w(["model-selector",P.value])},[e("div",C,[e("div",O,[a[2]||(a[2]=e("label",{for:"ram-select"},"RAM (GB)",-1)),y(e("select",{id:"ram-select","onUpdate:modelValue":a[0]||(a[0]=t=>i.value=t)},[(c(),d(M,null,_(S,t=>e("option",{key:t,value:t},v(t),9,I)),64))],512),[[k,i.value,void 0,{number:!0}]])]),e("div",Q,[a[3]||(a[3]=e("label",{for:"vram-select"},"VRAM (GB)",-1)),y(e("select",{id:"vram-select","onUpdate:modelValue":a[1]||(a[1]=t=>l.value=t)},[(c(),d(M,null,_(x,t=>e("option",{key:t,value:t},v(t),9,q)),64))],512),[[k,l.value,void 0,{number:!0}]])])]),e("div",z,[a[4]||(a[4]=e("strong",null,"Recommended model:",-1)),e("span",{class:w(["model-name",T.value]),style:G({backgroundColor:n.value.bg,color:n.value.color})},v(n.value.model),7)])],2),a[6]||(a[6]=b('<blockquote data-v-51815719><p data-v-51815719><strong data-v-51815719>‚ÄúNot recommended‚Äù means poor conversational memory and unreliable personalization</strong> On under-resourced systems, models may forget prior context within a few exchanges or fail to maintain consistent user preferences‚Äîmaking them ineffective as true personal assistants.</p></blockquote><hr data-v-51815719><h2 id="how-to-use-instruct-tuned-models-as-a-personal-assistant" tabindex="-1" data-v-51815719>How to Use Instruct-Tuned Models as a Personal Assistant <a class="header-anchor" href="#how-to-use-instruct-tuned-models-as-a-personal-assistant" aria-label="Permalink to ‚ÄúHow to Use Instruct-Tuned Models as a Personal Assistant‚Äù" data-v-51815719>‚Äã</a></h2><p data-v-51815719>Personal assistant models thrive on <strong data-v-51815719>long-term context awareness</strong>, <strong data-v-51815719>empathetic tone</strong>, and <strong data-v-51815719>user-specific adaptation</strong>. These prioritize natural dialogue, recall simulation, and task coordination.</p><h3 id="_1-choose-instruct-tuned-models" tabindex="-1" data-v-51815719>1. <strong data-v-51815719>Choose Instruct-Tuned Models</strong> <a class="header-anchor" href="#_1-choose-instruct-tuned-models" aria-label="Permalink to ‚Äú1. Choose Instruct-Tuned Models‚Äù" data-v-51815719>‚Äã</a></h3><ul data-v-51815719><li data-v-51815719>Use <strong data-v-51815719><code data-v-51815719>instruct</code></strong> variants (e.g., <code data-v-51815719>qwen3 4b instruct</code>, <code data-v-51815719>gpt-oss 20b</code>)‚Äîthey&#39;re fine-tuned for: <ul data-v-51815719><li data-v-51815719>Following multi-turn instructions</li><li data-v-51815719>Remembering stated preferences (‚ÄúI prefer morning summaries‚Äù)</li><li data-v-51815719>Managing to-do lists, reminders, or journaling prompts</li><li data-v-51815719>Adapting tone (casual, professional, supportive)</li></ul></li><li data-v-51815719>Avoid <code data-v-51815719>thinking</code> models‚Äîthey may over-analyze simple requests or ignore emotional nuance.</li></ul><h3 id="_2-prioritize-context-length-quantization" tabindex="-1" data-v-51815719>2. <strong data-v-51815719>Prioritize Context Length &amp; Quantization</strong> <a class="header-anchor" href="#_2-prioritize-context-length-quantization" aria-label="Permalink to ‚Äú2. Prioritize Context Length &amp; Quantization‚Äù" data-v-51815719>‚Äã</a></h3><table tabindex="0" data-v-51815719><thead data-v-51815719><tr data-v-51815719><th data-v-51815719>Quant</th><th data-v-51815719>Assistant Impact</th></tr></thead><tbody data-v-51815719><tr data-v-51815719><td data-v-51815719><code data-v-51815719>bf16</code> / <code data-v-51815719>f16</code></td><td data-v-51815719>Best for full personality retention over long chats; ideal if you use 32K+ context</td></tr><tr data-v-51815719><td data-v-51815719><code data-v-51815719>q8</code></td><td data-v-51815719>Excellent balance‚Äîretains nuance while fitting in moderate VRAM</td></tr><tr data-v-51815719><td data-v-51815719><code data-v-51815719>q6</code> / <code data-v-51815719>q4</code></td><td data-v-51815719>Usable for basic tasks, but may ‚Äúforget‚Äù early conversation details in long sessions</td></tr></tbody></table><blockquote data-v-51815719><p data-v-51815719>üîπ <strong data-v-51815719>Tip</strong>: For personal assistants, <strong data-v-51815719>context length matters more than raw parameter count</strong>. A well-quantized 20B model with 32K context often outperforms a 30B model limited to 4K.</p></blockquote><h3 id="_3-enable-full-gpu-offload" tabindex="-1" data-v-51815719>3. <strong data-v-51815719>Enable Full GPU Offload</strong> <a class="header-anchor" href="#_3-enable-full-gpu-offload" aria-label="Permalink to ‚Äú3. Enable Full GPU Offload‚Äù" data-v-51815719>‚Äã</a></h3><ul data-v-51815719><li data-v-51815719>Always use <strong data-v-51815719>full GPU offload</strong> (e.g., 48/48 layers) in LM Studio to keep conversation fast and responsive.</li><li data-v-51815719>If LM Studio pre-selects offload settings for your model, <strong data-v-51815719>do not override them</strong>.</li></ul><h3 id="_4-simulate-memory-with-prompt-engineering" tabindex="-1" data-v-51815719>4. <strong data-v-51815719>Simulate Memory with Prompt Engineering</strong> <a class="header-anchor" href="#_4-simulate-memory-with-prompt-engineering" aria-label="Permalink to ‚Äú4. Simulate Memory with Prompt Engineering‚Äù" data-v-51815719>‚Äã</a></h3><p data-v-51815719>Since local models lack true persistent memory:</p><ul data-v-51815719><li data-v-51815719><strong data-v-51815719>Seed your prompt</strong> with key facts:<div class="language-text" data-v-51815719><button title="Copy Code" class="copy" data-v-51815719></button><span class="lang" data-v-51815719>text</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr" data-v-51815719><code data-v-51815719><span class="line" data-v-51815719><span data-v-51815719>You are my personal assistant. I&#39;m a developer who enjoys Blender 3D. I eat meals regularly and track my learning goals. Today is Thursday, October 23, 2025.</span></span></code></pre></div></li><li data-v-51815719>Use <strong data-v-51815719>structured recall cues</strong>: <ul data-v-51815719><li data-v-51815719>‚ÄúBased on our last conversation about Blender shaders‚Ä¶‚Äù</li><li data-v-51815719>‚ÄúRemind me of my food preferences before suggesting dinner ideas.‚Äù</li></ul></li></ul><h3 id="_5-avoid-none-configurations" tabindex="-1" data-v-51815719>5. <strong data-v-51815719>Avoid ‚ÄúNone‚Äù Configurations</strong> <a class="header-anchor" href="#_5-avoid-none-configurations" aria-label="Permalink to ‚Äú5. Avoid ‚ÄúNone‚Äù Configurations‚Äù" data-v-51815719>‚Äã</a></h3><ul data-v-51815719><li data-v-51815719>Systems returning ‚Äúnone‚Äù lack the capacity to maintain even short-term conversational state.</li><li data-v-51815719>Forcing a load via CPU offload leads to: <ul data-v-51815719><li data-v-51815719>Slow responses that break conversational flow</li></ul></li><li data-v-51815719><strong data-v-51815719>Minimum viable setup</strong>: ‚â•6 GB VRAM + <code data-v-51815719>qwen3 4b instruct q4</code> for basic assistant duties.</li></ul><h3 id="_6-combine-with-external-memory-advanced" tabindex="-1" data-v-51815719>6. <strong data-v-51815719>Combine with External Memory (Advanced)</strong> <a class="header-anchor" href="#_6-combine-with-external-memory-advanced" aria-label="Permalink to ‚Äú6. Combine with External Memory (Advanced)‚Äù" data-v-51815719>‚Äã</a></h3><p data-v-51815719>For true long-term memory:</p><ul data-v-51815719><li data-v-51815719>Log key interactions to a local file or database</li><li data-v-51815719>Inject summarized memory into each new session:<div class="language-text" data-v-51815719><button title="Copy Code" class="copy" data-v-51815719></button><span class="lang" data-v-51815719>text</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr" data-v-51815719><code data-v-51815719><span class="line" data-v-51815719><span data-v-51815719>[Memory Summary: User is learning Blender geometry nodes. Last discussed procedural terrain generation on Oct 20. Prefers vegetarian meal suggestions.]</span></span></code></pre></div></li><li data-v-51815719>This turns even a 4B model into a surprisingly capable companion.</li></ul><h3 id="_7-monitor-resource-usage" tabindex="-1" data-v-51815719>7. <strong data-v-51815719>Monitor Resource Usage</strong> <a class="header-anchor" href="#_7-monitor-resource-usage" aria-label="Permalink to ‚Äú7. Monitor Resource Usage‚Äù" data-v-51815719>‚Äã</a></h3><ul data-v-51815719><li data-v-51815719>Keep VRAM usage <strong data-v-51815719>below 90%</strong> to avoid swapping, which destroys real-time responsiveness.</li><li data-v-51815719>On Windows, disable background apps (Discord, browsers) to free up the extra 1-2 GB needed for smooth 16K context.</li></ul><hr data-v-51815719><p data-v-51815719>By selecting the right instruct-tuned model for your hardware and structuring interactions to simulate memory, you can create a <strong data-v-51815719>responsive, personalized, and helpful local AI assistant</strong>‚Äîwithout relying on cloud services or sacrificing privacy.</p>',23))]))}}),E=A(R,[["__scopeId","data-v-51815719"]]);export{V as __pageData,E as default};
