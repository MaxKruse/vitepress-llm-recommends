import{_ as I,x as p,h as c,c as s,o as l,af as f,j as t,ag as y,ah as _,G as w,H as P,t as v,O as T,n as x}from"./chunks/framework.t7omRjYa.js";const q={class:"controls"},R={class:"control-group"},B=["value"],z={class:"control-group"},V=["value"],Q={class:"result"},G=JSON.parse('{"title":"Coding","description":"","frontmatter":{"title":"Coding"},"headers":[],"relativePath":"recommendations/coding/index.md","filePath":"recommendations/coding/index.md"}'),U={name:"recommendations/coding/index.md"},N=Object.assign(U,{setup(W){const d=p(16),i=p(8),M=[16,32,64,128],k=[0,4,6,8,12,16,24,32],A=[{ramMin:64,vramMin:16,model:"Qwen3 Coder 30B A3B Instruct 2507 bf16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:32,vramMin:16,model:"Qwen3 Coder 30B A3B Instruct 2507 Q8",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:32,vramMin:6,model:"Qwen3 Coder 30B A3B Instruct 2507 Q6",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:16,vramMin:4,model:"Qwen3 4B Instruct 2507 BF16",color:"var(--vp-c-purple-2)",bg:"var(--vp-c-purple-soft)"}],r=c(()=>{const n=A.find(e=>d.value>=e.ramMin&&i.value>=e.vramMin);return n?{model:n.model,color:n.color,bg:n.bg}:{model:"Not recommended",color:"var(--vp-c-text-3)",bg:"transparent"}}),u=c(()=>r.value.model!=="Not recommended"),o=c(()=>r.value.model.toLowerCase()),b=c(()=>o.value.includes("bf16")||o.value.includes("gpt oss")),m=c(()=>o.value.includes("q6")||o.value.includes("q8")),g=c(()=>o.value.includes("q4")),h=c(()=>o.value.includes("4b")),C=c(()=>u.value?h.value?{"recommended-4b":!0}:b.value?{"recommended-success":!0}:m.value?{"recommended-caution":!0}:g.value?{"recommended-warning":!0}:{}:{"not-recommended":!0}),S=c(()=>u.value?h.value?{"recommended-4b":!0}:b.value?{"recommended-success":!0}:m.value?{"recommended-caution":!0}:g.value?{"recommended-warning":!0}:{}:{"not-recommended":!0});return(n,e)=>(l(),s("div",null,[e[5]||(e[5]=f('<h1 id="coding-recommendations" tabindex="-1" data-v-2c8b48c1>Coding Recommendations <a class="header-anchor" href="#coding-recommendations" aria-label="Permalink to “Coding Recommendations”" data-v-2c8b48c1>​</a></h1><p data-v-2c8b48c1><strong data-v-2c8b48c1>Prioritizing Very High Precision in Code Generation, Completion, and Debugging</strong></p><p data-v-2c8b48c1>When working with AI-assisted coding, <strong data-v-2c8b48c1>precision is non-negotiable</strong>. Even minor hallucinations, syntactic errors, or incorrect logic can introduce bugs, security vulnerabilities, or maintenance debt. These recommendations are tailored for developers who require <strong data-v-2c8b48c1>extremely high-fidelity outputs</strong>—not just plausible-looking code, but <strong data-v-2c8b48c1>correct, production-ready, and logically sound</strong> implementations.</p><p data-v-2c8b48c1>Use the selector below to identify the best model <strong data-v-2c8b48c1>that balances your hardware constraints with the highest achievable precision</strong>:</p>',4)),t("div",{class:x(["model-selector",C.value])},[t("div",q,[t("div",R,[e[2]||(e[2]=t("label",{for:"ram-select"},"RAM (GB)",-1)),y(t("select",{id:"ram-select","onUpdate:modelValue":e[0]||(e[0]=a=>d.value=a)},[(l(),s(w,null,P(M,a=>t("option",{key:a,value:a},v(a),9,B)),64))],512),[[_,d.value,void 0,{number:!0}]])]),t("div",z,[e[3]||(e[3]=t("label",{for:"vram-select"},"VRAM (GB)",-1)),y(t("select",{id:"vram-select","onUpdate:modelValue":e[1]||(e[1]=a=>i.value=a)},[(l(),s(w,null,P(k,a=>t("option",{key:a,value:a},v(a),9,V)),64))],512),[[_,i.value,void 0,{number:!0}]])])]),t("div",Q,[e[4]||(e[4]=t("strong",null,"Recommended model:",-1)),t("span",{class:x(["model-name",S.value]),style:T({backgroundColor:r.value.bg,color:r.value.color})},v(r.value.model),7)])],2),e[6]||(e[6]=f('<h2 id="why-precision-matters-in-coding" tabindex="-1" data-v-2c8b48c1>Why Precision Matters in Coding <a class="header-anchor" href="#why-precision-matters-in-coding" aria-label="Permalink to “Why Precision Matters in Coding”" data-v-2c8b48c1>​</a></h2><p data-v-2c8b48c1>Unlike creative or conversational tasks, <strong data-v-2c8b48c1>code must be functionally correct</strong>. A model that “sounds right” but produces broken logic, incorrect APIs, or unsafe patterns is worse than no model at all. Therefore, <strong data-v-2c8b48c1>favor models and configurations that maximize precision—even at the cost of speed or resource usage</strong>.</p><h3 id="_1-use-full-gpu-offload-when-possible" tabindex="-1" data-v-2c8b48c1>1. <strong data-v-2c8b48c1>Use Full GPU Offload (When Possible)</strong> <a class="header-anchor" href="#_1-use-full-gpu-offload-when-possible" aria-label="Permalink to “1. Use Full GPU Offload (When Possible)”" data-v-2c8b48c1>​</a></h3><ul data-v-2c8b48c1><li data-v-2c8b48c1>In <strong data-v-2c8b48c1>LM Studio</strong>, always enable <strong data-v-2c8b48c1>full GPU offload</strong> (e.g., <code data-v-2c8b48c1>48/48</code> layers for Qwen3 models).</li></ul><h3 id="_2-prefer-instruct-tuned-code-specialized-models" tabindex="-1" data-v-2c8b48c1>2. <strong data-v-2c8b48c1>Prefer Instruct-Tuned, Code-Specialized Models</strong> <a class="header-anchor" href="#_2-prefer-instruct-tuned-code-specialized-models" aria-label="Permalink to “2. Prefer Instruct-Tuned, Code-Specialized Models”" data-v-2c8b48c1>​</a></h3><ul data-v-2c8b48c1><li data-v-2c8b48c1>Only use <strong data-v-2c8b48c1>code-instruct variants</strong> like <code data-v-2c8b48c1>qwen3-coder-30b-instruct</code>. These are fine-tuned on millions of correct code examples and aligned for <strong data-v-2c8b48c1>semantic and syntactic accuracy</strong>.</li><li data-v-2c8b48c1>Avoid base models or “thinking” variants—they lack the precision tuning needed for reliable code output.</li></ul><h3 id="_3-context-window-memory-stability" tabindex="-1" data-v-2c8b48c1>3. <strong data-v-2c8b48c1>Context Window &amp; Memory Stability</strong> <a class="header-anchor" href="#_3-context-window-memory-stability" aria-label="Permalink to “3. Context Window &amp; Memory Stability”" data-v-2c8b48c1>​</a></h3><ul data-v-2c8b48c1><li data-v-2c8b48c1>All recommendations assume a <strong data-v-2c8b48c1>stable ~16K context window</strong>.</li></ul><h3 id="_4-quantization-precision-vs-practicality" tabindex="-1" data-v-2c8b48c1>4. <strong data-v-2c8b48c1>Quantization: Precision vs. Practicality</strong> <a class="header-anchor" href="#_4-quantization-precision-vs-practicality" aria-label="Permalink to “4. Quantization: Precision vs. Practicality”" data-v-2c8b48c1>​</a></h3><table tabindex="0" data-v-2c8b48c1><thead data-v-2c8b48c1><tr data-v-2c8b48c1><th data-v-2c8b48c1>Quant</th><th data-v-2c8b48c1>Speed</th><th data-v-2c8b48c1><strong data-v-2c8b48c1>Precision</strong></th><th data-v-2c8b48c1>VRAM Use</th></tr></thead><tbody data-v-2c8b48c1><tr data-v-2c8b48c1><td data-v-2c8b48c1><code data-v-2c8b48c1>bf16</code> / <code data-v-2c8b48c1>f16</code></td><td data-v-2c8b48c1>★☆☆☆☆</td><td data-v-2c8b48c1>★★★★★ (<strong data-v-2c8b48c1>Highest</strong>)</td><td data-v-2c8b48c1>Highest</td></tr><tr data-v-2c8b48c1><td data-v-2c8b48c1><code data-v-2c8b48c1>q8</code></td><td data-v-2c8b48c1>★★★☆☆</td><td data-v-2c8b48c1>★★★★☆ (<strong data-v-2c8b48c1>High</strong>)</td><td data-v-2c8b48c1>Medium-High</td></tr><tr data-v-2c8b48c1><td data-v-2c8b48c1><code data-v-2c8b48c1>q6</code></td><td data-v-2c8b48c1>★★★★☆</td><td data-v-2c8b48c1>★★★☆☆ (<strong data-v-2c8b48c1>Moderate</strong>)</td><td data-v-2c8b48c1>Medium</td></tr><tr data-v-2c8b48c1><td data-v-2c8b48c1><code data-v-2c8b48c1>q4</code></td><td data-v-2c8b48c1>★★★★★</td><td data-v-2c8b48c1>★☆☆☆☆ (<strong data-v-2c8b48c1>Low - Not Recommended for Precision Work</strong>)</td><td data-v-2c8b48c1>Lowest</td></tr></tbody></table><blockquote data-v-2c8b48c1><p data-v-2c8b48c1>💡 <strong data-v-2c8b48c1>For high-stakes coding (e.g., production systems, security-sensitive logic, or complex algorithms), always prefer <code data-v-2c8b48c1>bf16</code>, <code data-v-2c8b48c1>f16</code>, or at minimum <code data-v-2c8b48c1>q8</code>.</strong> Avoid <code data-v-2c8b48c1>q4</code> unless absolutely necessary—it sacrifices too much precision.</p></blockquote><h3 id="_5-prompt-engineering-for-correctness" tabindex="-1" data-v-2c8b48c1>5. <strong data-v-2c8b48c1>Prompt Engineering for Correctness</strong> <a class="header-anchor" href="#_5-prompt-engineering-for-correctness" aria-label="Permalink to “5. Prompt Engineering for Correctness”" data-v-2c8b48c1>​</a></h3><ul data-v-2c8b48c1><li data-v-2c8b48c1><strong data-v-2c8b48c1>Be explicit and constrained</strong>:<br data-v-2c8b48c1> ❌ <em data-v-2c8b48c1>“Write a login function.”</em><br data-v-2c8b48c1> ✅ <em data-v-2c8b48c1>“Write a secure Python FastAPI login endpoint using OAuth2 password flow, with bcrypt hashing, rate limiting, and proper error responses.”</em></li><li data-v-2c8b48c1><strong data-v-2c8b48c1>Request verification steps</strong>: Ask the model to “explain why this implementation is safe” or “list potential edge cases.”</li><li data-v-2c8b48c1><strong data-v-2c8b48c1>Include error context</strong>: Paste stack traces or test failures—this helps the model reason precisely about the failure mode.</li></ul><h3 id="_6-avoid-none-or-underpowered-setups" tabindex="-1" data-v-2c8b48c1>6. <strong data-v-2c8b48c1>Avoid “None” or Underpowered Setups</strong> <a class="header-anchor" href="#_6-avoid-none-or-underpowered-setups" aria-label="Permalink to “6. Avoid “None” or Underpowered Setups”" data-v-2c8b48c1>​</a></h3><ul data-v-2c8b48c1><li data-v-2c8b48c1>If the matrix returns <strong data-v-2c8b48c1>“none”</strong>, <strong data-v-2c8b48c1>do not force execution</strong> via heavy RAM offloading.</li><li data-v-2c8b48c1>Under-resourced inference <strong data-v-2c8b48c1>increases hallucination rates</strong> and reduces logical consistency—<strong data-v-2c8b48c1>unacceptable for precision-critical workflows</strong>.</li><li data-v-2c8b48c1>Consider cloud inference (e.g., with <code data-v-2c8b48c1>bf16</code> models) if local hardware is insufficient.</li></ul><h3 id="_7-validate-and-monitor" tabindex="-1" data-v-2c8b48c1>7. <strong data-v-2c8b48c1>Validate and Monitor</strong> <a class="header-anchor" href="#_7-validate-and-monitor" aria-label="Permalink to “7. Validate and Monitor”" data-v-2c8b48c1>​</a></h3><ul data-v-2c8b48c1><li data-v-2c8b48c1><strong data-v-2c8b48c1>Never trust output blindly</strong>. Always: <ul data-v-2c8b48c1><li data-v-2c8b48c1>Run static analysis (e.g., <code data-v-2c8b48c1>mypy</code>, <code data-v-2c8b48c1>eslint</code>, <code data-v-2c8b48c1>bandit</code>)</li><li data-v-2c8b48c1>Execute unit tests</li><li data-v-2c8b48c1>Review for logic correctness</li></ul></li><li data-v-2c8b48c1>Monitor VRAM usage: sustained &gt;95% utilization can cause <strong data-v-2c8b48c1>numerical errors</strong> that silently degrade output quality.</li></ul><hr data-v-2c8b48c1><p data-v-2c8b48c1>By aligning your tooling with the <strong data-v-2c8b48c1>highest-precision models your hardware can support</strong>, you ensure that AI assistance enhances—rather than undermines—code quality, security, and maintainability. <strong data-v-2c8b48c1>When correctness is paramount, precision isn&#39;t optional—it&#39;s essential.</strong></p>',19))]))}}),E=I(N,[["__scopeId","data-v-2c8b48c1"]]);export{G as __pageData,E as default};
