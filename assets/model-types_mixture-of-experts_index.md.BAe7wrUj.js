import{_ as t,c as a,o,af as s}from"./chunks/framework.CuQK53I8.js";const c=JSON.parse('{"title":"Mixture of Experts (MoE)","description":"","frontmatter":{"title":"Mixture of Experts (MoE)"},"headers":[],"relativePath":"model-types/mixture-of-experts/index.md","filePath":"model-types/mixture-of-experts/index.md"}'),r={name:"model-types/mixture-of-experts/index.md"};function l(i,e,n,d,h,p){return o(),a("div",null,[...e[0]||(e[0]=[s('<h1 id="mixture-of-experts-moe" tabindex="-1">Mixture of Experts (MoE) <a class="header-anchor" href="#mixture-of-experts-moe" aria-label="Permalink to “Mixture of Experts (MoE)”">​</a></h1><p>MoE models use sparse activation—only a subset of &quot;expert&quot; subnetworks are active per input—enabling larger capacity with manageable compute costs.</p><h2 id="why-are-moe-models-so-fast" tabindex="-1">Why are MoE models so fast? <a class="header-anchor" href="#why-are-moe-models-so-fast" aria-label="Permalink to “Why are MoE models so fast?”">​</a></h2><p>In a traditional dense model (e.g., a 14B model), every input token must be processed against all 14B parameters.</p><p>In a Mixture of Experts model, this changes. A single &quot;Dense&quot; Gate layer acts as a router. For each token, this gate determines which &quot;experts&quot; (subnetworks) are most likely to provide the best output. It then routes the token <em>only</em> to that small subset of experts (e.g., 2 or 8 out of 128 experts in the entire model).</p><p>This means that if you have a 14B MoE model with 2B active parameters, the computation speed will be similar to that of a much smaller 2B dense model, not a 14B one. This process repeats for every single token generated:</p><ol><li>The gate layer selects the best experts for the current token.</li><li>Those experts compute the next token.</li><li>The new token is added to the sequence.</li><li>The gate layer re-evaluates and picks a new set of experts for the new token.</li></ol><h2 id="can-i-just-load-the-active-parameters-to-save-memory" tabindex="-1">Can I just load the active parameters to save memory? <a class="header-anchor" href="#can-i-just-load-the-active-parameters-to-save-memory" aria-label="Permalink to “Can I just load the active parameters to save memory?”">​</a></h2><p>No, this isn&#39;t possible because the selection of experts happens on a <strong>per-token basis</strong>, not a per-prompt basis.</p><p>If you were to only load the active experts, the process would be incredibly slow. For <em>every single token</em>, the system would have to:</p><ol><li>Load the dense gate layer from disk.</li><li>Decide which experts to use.</li><li>Unload the gate layer.</li><li>Load the required expert layers from disk.</li><li>Compute the single next token.</li><li>Unload the experts.</li><li>Repeat the entire process.</li></ol><p>This constant loading and unloading from disk (even a fast NVME) is dramatically slower than keeping all the model&#39;s layers in RAM or VRAM, where they can be accessed almost instantly.</p><h2 id="how-does-running-a-large-moe-on-a-small-gpu-work" tabindex="-1">How does running a large MoE on a small GPU work? <a class="header-anchor" href="#how-does-running-a-large-moe-on-a-small-gpu-work" aria-label="Permalink to “How does running a large MoE on a small GPU work?”">​</a></h2><p>If you are running a 30GB model on a GPU with only 4GB of VRAM, you are <strong>not</strong> running the model on your GPU. You are using <strong>CPU/GPU Offloading</strong>.</p><p>In this scenario:</p><ul><li>The vast majority of the model (e.g., 26GB+) is loaded into your system&#39;s <strong>RAM</strong>.</li><li>A small portion (4GB) is loaded into the much faster <strong>VRAM</strong>.</li><li>The CPU handles most of the work, using the RAM.</li></ul><p>Your speeds will be similar to running in a 100% CPU-only mode. While the small active parameter count (e.g., 3B) makes CPU inference faster than you might expect for a 30GB model, you are not getting the full speed benefits of GPU inference.</p><h2 id="can-i-adjust-the-number-of-active-experts" tabindex="-1">Can I adjust the number of active experts? <a class="header-anchor" href="#can-i-adjust-the-number-of-active-experts" aria-label="Permalink to “Can I adjust the number of active experts?”">​</a></h2><p>No. Models you download are pre-configured with the <strong>optimal number of active experts</strong>. This number is chosen by the model&#39;s creators to provide the best balance of speed and quality.</p><p>This &quot;optimal&quot; number is determined by testing the model&#39;s <strong>perplexity (PPL)</strong>, which is a measure of its error rate (lower is better). The creators find the &quot;local minimum&quot; PPL, which is the number of experts that produces the most accurate results.</p><ul><li><strong>Reducing active experts:</strong> This will make the model faster, but the output quality will get much worse because it doesn&#39;t have enough relevant information.</li><li><strong>Increasing active experts:</strong> This will slow the model down and also <em>reduce</em> quality. The extra experts &quot;pollute&quot; the output and confuse the others. As the analogy goes, &quot;more cooks don&#39;t make a better soup.&quot;</li></ul><p><strong>TL;DR:</strong> Don&#39;t change the number of active experts. It will reduce output quality.</p><h2 id="how-to-use-llama-cpp-correctly-with-an-moe-model" tabindex="-1">How to use llama.cpp correctly with an MoE model <a class="header-anchor" href="#how-to-use-llama-cpp-correctly-with-an-moe-model" aria-label="Permalink to “How to use llama.cpp correctly with an MoE model”">​</a></h2><h3 id="finding-the-optimal-gpu-layers" tabindex="-1">Finding the optimal GPU layers <a class="header-anchor" href="#finding-the-optimal-gpu-layers" aria-label="Permalink to “Finding the optimal GPU layers”">​</a></h3><p>You don&#39;t need to find them manually. The <code>llama.cpp</code> tool added a flag, <code>-cpu-moe</code>, to handle this.</p><p>The simple method is to:</p><ol><li>Load as many GPU layers as you can with the <code>-ngl</code> flag.</li><li>Use the <code>-n-cpu-moe &lt;N&gt;</code> flag and adjust <code>&lt;N&gt;</code> until the model loads and runs.</li></ol><p>For a more technical approach, you can calculate <code>&lt;N&gt;</code> for the flag <code>-n-cpu-moe &lt;Layer Count - N&gt;</code>:</p><ol><li>Find the model&#39;s total layers and the size per layer (e.g., 36GB file / 48 layers = 0.75GB per layer).</li><li>Find your available VRAM (e.g., Total VRAM - VRAM already in use).</li><li>Calculate <code>N</code> = <code>Available VRAM in GB</code> / <code>SizePerLayer</code></li><li>Example for a 12GB card with 4GB in use: (12 - 4) / 0.75 ≈ 11.</li><li>Your flag would be <code>-n-cpu-moe &lt;48 - 11&gt;</code>, so <code>-n-cpu-moe 37</code>.</li></ol><h3 id="speed-differences-between-quantizations" tabindex="-1">Speed Differences Between Quantizations <a class="header-anchor" href="#speed-differences-between-quantizations" aria-label="Permalink to “Speed Differences Between Quantizations”">​</a></h3><p>What matters for speed is not just the <em>number</em> of active parameters, but the total <em>file size (GB)</em> of the activated experts.</p><p>A smaller, more heavily quantized model will be faster, even with the same architecture. For example, in a CPU-only test:</p><ul><li><strong>36GB (Q8) File:</strong> 336 pp (prompt processing) / 20 t/s (token generation)</li><li><strong>11GB (Q2_K) File:</strong> 835 pp / 41 t/s</li></ul><p>The smaller 11GB file is nearly 3x faster at processing the prompt and 2x faster at generating tokens because less data needs to be moved and processed for each step.</p>',34)])])}const u=t(r,[["render",l]]);export{c as __pageData,u as default};
