import{M as n}from"./chunks/ModelSelector.DQGW46EL.js";import{c as s,o,af as t,J as r}from"./chunks/framework.CuQK53I8.js";const c=JSON.parse('{"title":"Personal Assistant","description":"","frontmatter":{"title":"Personal Assistant"},"headers":[],"relativePath":"recommendations/personal-assistant/index.md","filePath":"recommendations/personal-assistant/index.md"}'),i={name:"recommendations/personal-assistant/index.md"},p=Object.assign(i,{setup(l){const a=[{ramMin:128,vramMin:32,models:[{"GPT OSS 120B":{parameters:120,quantization:"MXFP4"}},{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"BF16"}},{"Mistral Small 3.2":{parameters:24,quantization:"Q8_K_XL"}}],usefulness:1},{ramMin:128,vramMin:24,models:[{"GPT OSS 120B":{parameters:120,quantization:"MXFP4"}},{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"BF16"}},{"Mistral Small 3.2":{parameters:24,quantization:"Q6_K_XL"}}],usefulness:.9},{ramMin:128,vramMin:0,models:[{"GPT OSS 120B":{parameters:120,quantization:"MXFP4"}},{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"BF16"}}],usefulness:.8},{ramMin:64,vramMin:24,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"BF16"}}],usefulness:.8},{ramMin:64,vramMin:0,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.6},{ramMin:32,vramMin:24,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q4_K_XL"}}],usefulness:.7},{ramMin:32,vramMin:8,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 12B":{parameters:12,quantization:"Q6_K_XL"}}],usefulness:.5},{ramMin:32,vramMin:0,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}}],usefulness:.4},{ramMin:16,vramMin:32,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q8_K_XL"}}],usefulness:.6},{ramMin:16,vramMin:24,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}},{"Gemma 3 27B":{parameters:27,quantization:"Q4_K_XL"}}],usefulness:.5},{ramMin:16,vramMin:12,models:[{"GPT OSS 20B":{parameters:20,quantization:"MXFP4"}}],usefulness:.4},{ramMin:16,vramMin:8,models:[{"Gemma 3 12B":{parameters:12,quantization:"Q4_K_XL"}}],usefulness:.3},{ramMin:16,vramMin:4,models:[{"Qwen3 4B Instruct 2507":{parameters:4,quantization:"Q4_K_XL"}}],usefulness:.2}];return(d,e)=>(o(),s("div",null,[e[0]||(e[0]=t('<h1 id="personal-assistant-use-case" tabindex="-1">Personal Assistant Use Case <a class="header-anchor" href="#personal-assistant-use-case" aria-label="Permalink to “Personal Assistant Use Case”">​</a></h1><p>Recommendations for models that excel at memory, context retention, and personalized interactions.</p><blockquote><p>💡 <strong>Note</strong>: For personal assistant tasks—such as recalling preferences, maintaining conversation history, managing schedules, or adapting tone over time—<strong>instruct-tuned models with strong long-context handling</strong> are preferred over thinking-tuned variants. These models prioritize coherence, empathy, and user-specific adaptation over raw analytical power.</p></blockquote><p>Use the selector below to find the best <strong>assistant-like</strong> model for your hardware:</p>',4)),r(n,{modelDefinitions:a}),e[1]||(e[1]=t('<blockquote><p><strong>“Not recommended” means poor conversational memory and unreliable personalization</strong> On under-resourced systems, models may forget prior context within a few exchanges or fail to maintain consistent user preferences—making them ineffective as true personal assistants.</p></blockquote><hr><h2 id="how-to-use-instruct-tuned-models-as-a-personal-assistant" tabindex="-1">How to Use Instruct-Tuned Models as a Personal Assistant <a class="header-anchor" href="#how-to-use-instruct-tuned-models-as-a-personal-assistant" aria-label="Permalink to “How to Use Instruct-Tuned Models as a Personal Assistant”">​</a></h2><p>Personal assistant models thrive on <strong>long-term context awareness</strong>, <strong>empathetic tone</strong>, and <strong>user-specific adaptation</strong>. These prioritize natural dialogue, recall simulation, and task coordination.</p><h3 id="_1-choose-instruct-tuned-models" tabindex="-1">1. <strong>Choose Instruct-Tuned Models</strong> <a class="header-anchor" href="#_1-choose-instruct-tuned-models" aria-label="Permalink to “1. Choose Instruct-Tuned Models”">​</a></h3><ul><li>Use <strong><code>instruct</code></strong> variants (e.g., <code>qwen3 4b instruct</code>, <code>gpt-oss 20b</code>)—they&#39;re fine-tuned for: <ul><li>Following multi-turn instructions</li><li>Remembering stated preferences (“I prefer morning summaries”)</li><li>Managing to-do lists, reminders, or journaling prompts</li><li>Adapting tone (casual, professional, supportive)</li></ul></li><li>Avoid <code>thinking</code> models—they may over-analyze simple requests or ignore emotional nuance.</li></ul><h3 id="_2-prioritize-context-length-quantization" tabindex="-1">2. <strong>Prioritize Context Length &amp; Quantization</strong> <a class="header-anchor" href="#_2-prioritize-context-length-quantization" aria-label="Permalink to “2. Prioritize Context Length &amp; Quantization”">​</a></h3><table tabindex="0"><thead><tr><th>Quant</th><th>Assistant Impact</th></tr></thead><tbody><tr><td><code>bf16</code> / <code>f16</code></td><td>Best for full personality retention over long chats; ideal if you use 32K+ context</td></tr><tr><td><code>q8</code></td><td>Excellent balance—retains nuance while fitting in moderate VRAM</td></tr><tr><td><code>q6</code> / <code>q4</code></td><td>Usable for basic tasks, but may “forget” early conversation details in long sessions</td></tr></tbody></table><blockquote><p>🔹 <strong>Tip</strong>: For personal assistants, <strong>context length matters more than raw parameter count</strong>. A well-quantized 20B model with 32K context often outperforms a 30B model limited to 4K.</p></blockquote><h3 id="_3-enable-full-gpu-offload" tabindex="-1">3. <strong>Enable Full GPU Offload</strong> <a class="header-anchor" href="#_3-enable-full-gpu-offload" aria-label="Permalink to “3. Enable Full GPU Offload”">​</a></h3><ul><li>Always use <strong>full GPU offload</strong> (e.g., 48/48 layers) in LM Studio to keep conversation fast and responsive.</li><li>If LM Studio pre-selects offload settings for your model, <strong>do not override them</strong>.</li></ul><h3 id="_4-simulate-memory-with-prompt-engineering" tabindex="-1">4. <strong>Simulate Memory with Prompt Engineering</strong> <a class="header-anchor" href="#_4-simulate-memory-with-prompt-engineering" aria-label="Permalink to “4. Simulate Memory with Prompt Engineering”">​</a></h3><p>Since local models lack true persistent memory:</p><ul><li><strong>Seed your prompt</strong> with key facts:<div class="language-text"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span>You are my personal assistant. I&#39;m a developer who enjoys Blender 3D. I eat meals regularly and track my learning goals. Today is Thursday, October 23, 2025.</span></span></code></pre></div></li><li>Use <strong>structured recall cues</strong>: <ul><li>“Based on our last conversation about Blender shaders…”</li><li>“Remind me of my food preferences before suggesting dinner ideas.”</li></ul></li></ul><h3 id="_5-avoid-none-configurations" tabindex="-1">5. <strong>Avoid “None” Configurations</strong> <a class="header-anchor" href="#_5-avoid-none-configurations" aria-label="Permalink to “5. Avoid “None” Configurations”">​</a></h3><ul><li>Systems returning “none” lack the capacity to maintain even short-term conversational state.</li><li>Forcing a load via CPU offload leads to: <ul><li>Slow responses that break conversational flow</li></ul></li><li><strong>Minimum viable setup</strong>: ≥6 GB VRAM + <code>qwen3 4b instruct q4</code> for basic assistant duties.</li></ul><h3 id="_6-combine-with-external-memory-advanced" tabindex="-1">6. <strong>Combine with External Memory (Advanced)</strong> <a class="header-anchor" href="#_6-combine-with-external-memory-advanced" aria-label="Permalink to “6. Combine with External Memory (Advanced)”">​</a></h3><p>For true long-term memory:</p><ul><li>Log key interactions to a local file or database</li><li>Inject summarized memory into each new session:<div class="language-text"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span>[Memory Summary: User is learning Blender geometry nodes. Last discussed procedural terrain generation on Oct 20. Prefers vegetarian meal suggestions.]</span></span></code></pre></div></li><li>This turns even a 4B model into a surprisingly capable companion.</li></ul><h3 id="_7-monitor-resource-usage" tabindex="-1">7. <strong>Monitor Resource Usage</strong> <a class="header-anchor" href="#_7-monitor-resource-usage" aria-label="Permalink to “7. Monitor Resource Usage”">​</a></h3><ul><li>Keep VRAM usage <strong>below 90%</strong> to avoid swapping, which destroys real-time responsiveness.</li><li>On Windows, disable background apps (Discord, browsers) to free up the extra 1-2 GB needed for smooth 16K context.</li></ul><hr><p>By selecting the right instruct-tuned model for your hardware and structuring interactions to simulate memory, you can create a <strong>responsive, personalized, and helpful local AI assistant</strong>—without relying on cloud services or sacrificing privacy.</p>',23))]))}});export{c as __pageData,p as default};
