import{_ as Q,x as f,h as t,c as l,o as c,af as b,j as d,ag as y,ah as w,G as _,H as M,t as v,O as S,n as P}from"./chunks/framework.t7omRjYa.js";const I={class:"controls"},T={class:"control-group"},q=["value"],R={class:"control-group"},z=["value"],V={class:"result"},D=JSON.parse('{"title":"Coding","description":"","frontmatter":{"title":"Coding"},"headers":[],"relativePath":"recommendations/coding/index.md","filePath":"recommendations/coding/index.md"}'),U={name:"recommendations/coding/index.md"},N=Object.assign(U,{setup(W){const i=f(16),s=f(8),x=[16,32,64,128],A=[0,4,6,8,12,16,24,32],C=[{ramMin:64,vramMin:16,model:"Qwen3 Coder 30B A3B Instruct bf16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:32,vramMin:16,model:"Qwen3 Coder 30B A3B Instruct bf16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:32,vramMin:8,model:"Qwen3 Coder 30B A3B Instruct Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:16,vramMin:8,model:"Qwen3 Coder 30B A3B Instruct Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:6,model:"Qwen3 Coder 30B A3B Instruct Q6",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:16,vramMin:6,model:"Qwen3 Coder 30B A3B Instruct Q6",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:16,vramMin:4,model:"Phi-4 Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:4,model:"Devstral Small Q6",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:32,vramMin:4,model:"Devstral Small Q8 or Qwen3 Coder 30B A3B Instruct Q6",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:0,model:"Qwen3 4B Instruct 2507 BF16",color:"var(--vp-c-purple-2)",bg:"var(--vp-c-purple-soft)"}],r=t(()=>{const n=C.find(e=>i.value>=e.ramMin&&s.value>=e.vramMin);return n?{model:n.model,color:n.color,bg:n.bg}:{model:"Not recommended",color:"var(--vp-c-text-3)",bg:"transparent"}}),u=t(()=>r.value.model!=="Not recommended"),o=t(()=>r.value.model.toLowerCase()),m=t(()=>o.value.includes("bf16")||o.value.includes("gpt oss")),g=t(()=>o.value.includes("q6")||o.value.includes("q8")),p=t(()=>o.value.includes("q4")),h=t(()=>o.value.includes("4b")),k=t(()=>u.value?h.value?{"recommended-4b":!0}:m.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:p.value?{"recommended-warning":!0}:{}:{"not-recommended":!0}),B=t(()=>u.value?h.value?{"recommended-4b":!0}:m.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:p.value?{"recommended-warning":!0}:{}:{"not-recommended":!0});return(n,e)=>(c(),l("div",null,[e[5]||(e[5]=b('<h1 id="coding-recommendations" tabindex="-1" data-v-3d6e31d2>Coding Recommendations <a class="header-anchor" href="#coding-recommendations" aria-label="Permalink to â€œCoding Recommendationsâ€" data-v-3d6e31d2>â€‹</a></h1><p data-v-3d6e31d2><strong data-v-3d6e31d2>Prioritizing Very High Precision in Code Generation, Completion, and Debugging</strong></p><p data-v-3d6e31d2>When working with AI-assisted coding, <strong data-v-3d6e31d2>precision is non-negotiable</strong>. Even minor hallucinations, syntactic errors, or incorrect logic can introduce bugs, security vulnerabilities, or maintenance debt. These recommendations are tailored for developers who require <strong data-v-3d6e31d2>extremely high-fidelity outputs</strong>â€”not just plausible-looking code, but <strong data-v-3d6e31d2>correct, production-ready, and logically sound</strong> implementations.</p><p data-v-3d6e31d2>Use the selector below to identify the best model <strong data-v-3d6e31d2>that balances your hardware constraints with the highest achievable precision</strong>:</p>',4)),d("div",{class:P(["model-selector",k.value])},[d("div",I,[d("div",T,[e[2]||(e[2]=d("label",{for:"ram-select"},"RAM (GB)",-1)),y(d("select",{id:"ram-select","onUpdate:modelValue":e[0]||(e[0]=a=>i.value=a)},[(c(),l(_,null,M(x,a=>d("option",{key:a,value:a},v(a),9,q)),64))],512),[[w,i.value,void 0,{number:!0}]])]),d("div",R,[e[3]||(e[3]=d("label",{for:"vram-select"},"VRAM (GB)",-1)),y(d("select",{id:"vram-select","onUpdate:modelValue":e[1]||(e[1]=a=>s.value=a)},[(c(),l(_,null,M(A,a=>d("option",{key:a,value:a},v(a),9,z)),64))],512),[[w,s.value,void 0,{number:!0}]])])]),d("div",V,[e[4]||(e[4]=d("strong",null,"Recommended model:",-1)),d("span",{class:P(["model-name",B.value]),style:S({backgroundColor:r.value.bg,color:r.value.color})},v(r.value.model),7)])],2),e[6]||(e[6]=b('<h2 id="why-precision-matters-in-coding" tabindex="-1" data-v-3d6e31d2>Why Precision Matters in Coding <a class="header-anchor" href="#why-precision-matters-in-coding" aria-label="Permalink to â€œWhy Precision Matters in Codingâ€" data-v-3d6e31d2>â€‹</a></h2><p data-v-3d6e31d2>Unlike creative or conversational tasks, <strong data-v-3d6e31d2>code must be functionally correct</strong>. A model that â€œsounds rightâ€ but produces broken logic, incorrect APIs, or unsafe patterns is worse than no model at all. Therefore, <strong data-v-3d6e31d2>favor models and configurations that maximize precisionâ€”even at the cost of speed or resource usage</strong>.</p><h3 id="_1-use-full-gpu-offload-when-possible" tabindex="-1" data-v-3d6e31d2>1. <strong data-v-3d6e31d2>Use Full GPU Offload (When Possible)</strong> <a class="header-anchor" href="#_1-use-full-gpu-offload-when-possible" aria-label="Permalink to â€œ1. Use Full GPU Offload (When Possible)â€" data-v-3d6e31d2>â€‹</a></h3><ul data-v-3d6e31d2><li data-v-3d6e31d2>In <strong data-v-3d6e31d2>LM Studio</strong>, always enable <strong data-v-3d6e31d2>full GPU offload</strong> (e.g., <code data-v-3d6e31d2>48/48</code> layers for Qwen3 models).</li></ul><h3 id="_2-prefer-instruct-tuned-code-specialized-models" tabindex="-1" data-v-3d6e31d2>2. <strong data-v-3d6e31d2>Prefer Instruct-Tuned, Code-Specialized Models</strong> <a class="header-anchor" href="#_2-prefer-instruct-tuned-code-specialized-models" aria-label="Permalink to â€œ2. Prefer Instruct-Tuned, Code-Specialized Modelsâ€" data-v-3d6e31d2>â€‹</a></h3><ul data-v-3d6e31d2><li data-v-3d6e31d2>Only use <strong data-v-3d6e31d2>code-instruct variants</strong> like <code data-v-3d6e31d2>qwen3-coder-30b-instruct</code>. These are fine-tuned on millions of correct code examples and aligned for <strong data-v-3d6e31d2>semantic and syntactic accuracy</strong>.</li><li data-v-3d6e31d2>Avoid base models or â€œthinkingâ€ variantsâ€”they lack the precision tuning needed for reliable code output.</li></ul><h3 id="_3-context-window-memory-stability" tabindex="-1" data-v-3d6e31d2>3. <strong data-v-3d6e31d2>Context Window &amp; Memory Stability</strong> <a class="header-anchor" href="#_3-context-window-memory-stability" aria-label="Permalink to â€œ3. Context Window &amp; Memory Stabilityâ€" data-v-3d6e31d2>â€‹</a></h3><ul data-v-3d6e31d2><li data-v-3d6e31d2>All recommendations assume a <strong data-v-3d6e31d2>stable ~16K context window</strong>.</li></ul><h3 id="_4-quantization-precision-vs-practicality" tabindex="-1" data-v-3d6e31d2>4. <strong data-v-3d6e31d2>Quantization: Precision vs. Practicality</strong> <a class="header-anchor" href="#_4-quantization-precision-vs-practicality" aria-label="Permalink to â€œ4. Quantization: Precision vs. Practicalityâ€" data-v-3d6e31d2>â€‹</a></h3><table tabindex="0" data-v-3d6e31d2><thead data-v-3d6e31d2><tr data-v-3d6e31d2><th data-v-3d6e31d2>Quant</th><th data-v-3d6e31d2>Speed</th><th data-v-3d6e31d2><strong data-v-3d6e31d2>Precision</strong></th><th data-v-3d6e31d2>VRAM Use</th></tr></thead><tbody data-v-3d6e31d2><tr data-v-3d6e31d2><td data-v-3d6e31d2><code data-v-3d6e31d2>bf16</code> / <code data-v-3d6e31d2>f16</code></td><td data-v-3d6e31d2>â˜…â˜†â˜†â˜†â˜†</td><td data-v-3d6e31d2>â˜…â˜…â˜…â˜…â˜… (<strong data-v-3d6e31d2>Highest</strong>)</td><td data-v-3d6e31d2>Highest</td></tr><tr data-v-3d6e31d2><td data-v-3d6e31d2><code data-v-3d6e31d2>q8</code></td><td data-v-3d6e31d2>â˜…â˜…â˜…â˜†â˜†</td><td data-v-3d6e31d2>â˜…â˜…â˜…â˜…â˜† (<strong data-v-3d6e31d2>High</strong>)</td><td data-v-3d6e31d2>Medium-High</td></tr><tr data-v-3d6e31d2><td data-v-3d6e31d2><code data-v-3d6e31d2>q6</code></td><td data-v-3d6e31d2>â˜…â˜…â˜…â˜…â˜†</td><td data-v-3d6e31d2>â˜…â˜…â˜…â˜†â˜† (<strong data-v-3d6e31d2>Moderate</strong>)</td><td data-v-3d6e31d2>Medium</td></tr><tr data-v-3d6e31d2><td data-v-3d6e31d2><code data-v-3d6e31d2>q4</code></td><td data-v-3d6e31d2>â˜…â˜…â˜…â˜…â˜…</td><td data-v-3d6e31d2>â˜…â˜†â˜†â˜†â˜† (<strong data-v-3d6e31d2>Low - Not Recommended for Precision Work</strong>)</td><td data-v-3d6e31d2>Lowest</td></tr></tbody></table><blockquote data-v-3d6e31d2><p data-v-3d6e31d2>ğŸ’¡ <strong data-v-3d6e31d2>For high-stakes coding (e.g., production systems, security-sensitive logic, or complex algorithms), always prefer <code data-v-3d6e31d2>bf16</code>, <code data-v-3d6e31d2>f16</code>, or at minimum <code data-v-3d6e31d2>q8</code>.</strong> Avoid <code data-v-3d6e31d2>q4</code> unless absolutely necessaryâ€”it sacrifices too much precision.</p></blockquote><h3 id="_5-prompt-engineering-for-correctness" tabindex="-1" data-v-3d6e31d2>5. <strong data-v-3d6e31d2>Prompt Engineering for Correctness</strong> <a class="header-anchor" href="#_5-prompt-engineering-for-correctness" aria-label="Permalink to â€œ5. Prompt Engineering for Correctnessâ€" data-v-3d6e31d2>â€‹</a></h3><ul data-v-3d6e31d2><li data-v-3d6e31d2><strong data-v-3d6e31d2>Be explicit and constrained</strong>:<br data-v-3d6e31d2> âŒ <em data-v-3d6e31d2>â€œWrite a login function.â€</em><br data-v-3d6e31d2> âœ… <em data-v-3d6e31d2>â€œWrite a secure Python FastAPI login endpoint using OAuth2 password flow, with bcrypt hashing, rate limiting, and proper error responses.â€</em></li><li data-v-3d6e31d2><strong data-v-3d6e31d2>Request verification steps</strong>: Ask the model to â€œexplain why this implementation is safeâ€ or â€œlist potential edge cases.â€</li><li data-v-3d6e31d2><strong data-v-3d6e31d2>Include error context</strong>: Paste stack traces or test failuresâ€”this helps the model reason precisely about the failure mode.</li></ul><h3 id="_6-avoid-none-or-underpowered-setups" tabindex="-1" data-v-3d6e31d2>6. <strong data-v-3d6e31d2>Avoid â€œNoneâ€ or Underpowered Setups</strong> <a class="header-anchor" href="#_6-avoid-none-or-underpowered-setups" aria-label="Permalink to â€œ6. Avoid â€œNoneâ€ or Underpowered Setupsâ€" data-v-3d6e31d2>â€‹</a></h3><ul data-v-3d6e31d2><li data-v-3d6e31d2>If the matrix returns <strong data-v-3d6e31d2>â€œnoneâ€</strong>, <strong data-v-3d6e31d2>do not force execution</strong> via heavy RAM offloading.</li><li data-v-3d6e31d2>Under-resourced inference <strong data-v-3d6e31d2>increases hallucination rates</strong> and reduces logical consistencyâ€”<strong data-v-3d6e31d2>unacceptable for precision-critical workflows</strong>.</li><li data-v-3d6e31d2>Consider cloud inference (e.g., with <code data-v-3d6e31d2>bf16</code> models) if local hardware is insufficient.</li></ul><h3 id="_7-validate-and-monitor" tabindex="-1" data-v-3d6e31d2>7. <strong data-v-3d6e31d2>Validate and Monitor</strong> <a class="header-anchor" href="#_7-validate-and-monitor" aria-label="Permalink to â€œ7. Validate and Monitorâ€" data-v-3d6e31d2>â€‹</a></h3><ul data-v-3d6e31d2><li data-v-3d6e31d2><strong data-v-3d6e31d2>Never trust output blindly</strong>. Always: <ul data-v-3d6e31d2><li data-v-3d6e31d2>Run static analysis (e.g., <code data-v-3d6e31d2>mypy</code>, <code data-v-3d6e31d2>eslint</code>, <code data-v-3d6e31d2>bandit</code>)</li><li data-v-3d6e31d2>Execute unit tests</li><li data-v-3d6e31d2>Review for logic correctness</li></ul></li><li data-v-3d6e31d2>Monitor VRAM usage: sustained &gt;95% utilization can cause <strong data-v-3d6e31d2>numerical errors</strong> that silently degrade output quality.</li></ul><hr data-v-3d6e31d2><p data-v-3d6e31d2>By aligning your tooling with the <strong data-v-3d6e31d2>highest-precision models your hardware can support</strong>, you ensure that AI assistance enhancesâ€”rather than underminesâ€”code quality, security, and maintainability. <strong data-v-3d6e31d2>When correctness is paramount, precision isn&#39;t optionalâ€”it&#39;s essential.</strong></p>',19))]))}}),G=Q(N,[["__scopeId","data-v-3d6e31d2"]]);export{D as __pageData,G as default};
