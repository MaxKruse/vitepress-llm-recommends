import{_ as G,x as h,h as r,c as i,o as l,af as f,j as e,ag as M,ah as B,G as S,H as y,t as v,O as Q,n as w}from"./chunks/framework.t7omRjYa.js";const O={class:"controls"},I={class:"control-group"},A=["value"],C={class:"control-group"},q=["value"],z={class:"result"},V=JSON.parse('{"title":"Personal Assistant","description":"","frontmatter":{"title":"Personal Assistant"},"headers":[],"relativePath":"recommendations/personal-assistant/index.md","filePath":"recommendations/personal-assistant/index.md"}'),F={name:"recommendations/personal-assistant/index.md"},U=Object.assign(F,{setup(R){const s=h(16),d=h(8),P=[16,32,64,128],k=[0,4,6,8,12,16,24,32],_=[{ramMin:128,vramMin:32,model:"GPT OSS 120B or Qwen3 30B Instruct BF16 or Mistral Small 3.2 Q8",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:24,model:"GPT OSS 120B or Qwen3 30B Instruct BF16 or Mistral Small 3.2 Q6",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:16,model:"GPT OSS 120B or Qwen3 30B Instruct BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:12,model:"GPT OSS 120B or Qwen3 30B Instruct BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:8,model:"GPT OSS 120B or Qwen3 30B Instruct BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:6,model:"GPT OSS 120B or Qwen3 30B Instruct BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:4,model:"GPT OSS 120B or Qwen3 30B Instruct BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:0,model:"GPT OSS 120B or Qwen3 30B Instruct BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:64,vramMin:32,model:"GPT OSS 20B or Qwen3 30B Instruct BF16",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:24,model:"GPT OSS 20B or Qwen3 30B Instruct BF16",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:16,model:"GPT OSS 20B or Qwen3 30B Instruct Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:12,model:"GPT OSS 20B or Qwen3 30B Instruct Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:8,model:"GPT OSS 20B or Qwen3 30B Instruct Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:6,model:"GPT OSS 20B or Qwen3 30B Instruct Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:4,model:"GPT OSS 20B or Qwen3 30B Instruct Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:0,model:"GPT OSS 20B or Qwen3 30B Instruct Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:32,model:"GPT OSS 20B or Gemma 3 27B Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:24,model:"GPT OSS 20B or Gemma 3 27B Q4",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:16,model:"GPT OSS 20B or Gemma 3 12B Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:12,model:"GPT OSS 20B or Gemma 3 12B Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:8,model:"GPT OSS 20B or Gemma 3 12B Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:6,model:"GPT OSS 20B",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:4,model:"GPT OSS 20B",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:0,model:"GPT OSS 20B",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:16,vramMin:32,model:"GPT OSS 20B or Gemma 3 27B Q8",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:24,model:"GPT OSS 20B or Gemma 3 27B Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:16,model:"GPT OSS 20B",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:12,model:"GPT OSS 20B",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:8,model:"Gemma 3 12B Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:6,model:"Qwen3 4B Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:4,model:"Qwen3 4B Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"}],n=r(()=>{const c=_.find(a=>s.value>=a.ramMin&&d.value>=a.vramMin);return c?{model:c.model,color:c.color,bg:c.bg}:{model:"Not recommended",color:"var(--vp-c-text-3)",bg:"transparent"}}),m=r(()=>n.value.model!=="Not recommended"),o=r(()=>n.value.model.toLowerCase()),u=r(()=>o.value.includes("bf16")||o.value.includes("gpt oss")),b=r(()=>o.value.includes("q6")||o.value.includes("q8")),g=r(()=>o.value.includes("q4")),p=r(()=>o.value.includes("4b")),T=r(()=>m.value?p.value?{"recommended-4b":!0}:u.value?{"recommended-success":!0}:b.value?{"recommended-caution":!0}:g.value?{"recommended-warning":!0}:{}:{"not-recommended":!0}),x=r(()=>m.value?p.value?{"recommended-4b":!0}:u.value?{"recommended-success":!0}:b.value?{"recommended-caution":!0}:g.value?{"recommended-warning":!0}:{}:{"not-recommended":!0});return(c,a)=>(l(),i("div",null,[a[5]||(a[5]=f('<h1 id="personal-assistant-use-case" tabindex="-1" data-v-adcbc991>Personal Assistant Use Case <a class="header-anchor" href="#personal-assistant-use-case" aria-label="Permalink to ‚ÄúPersonal Assistant Use Case‚Äù" data-v-adcbc991>‚Äã</a></h1><p data-v-adcbc991>Recommendations for models that excel at memory, context retention, and personalized interactions.</p><blockquote data-v-adcbc991><p data-v-adcbc991>üí° <strong data-v-adcbc991>Note</strong>: For personal assistant tasks‚Äîsuch as recalling preferences, maintaining conversation history, managing schedules, or adapting tone over time‚Äî<strong data-v-adcbc991>instruct-tuned models with strong long-context handling</strong> are preferred over thinking-tuned variants. These models prioritize coherence, empathy, and user-specific adaptation over raw analytical power.</p></blockquote><p data-v-adcbc991>Use the selector below to find the best <strong data-v-adcbc991>assistant-like</strong> model for your hardware:</p>',4)),e("div",{class:w(["model-selector",T.value])},[e("div",O,[e("div",I,[a[2]||(a[2]=e("label",{for:"ram-select"},"RAM (GB)",-1)),M(e("select",{id:"ram-select","onUpdate:modelValue":a[0]||(a[0]=t=>s.value=t)},[(l(),i(S,null,y(P,t=>e("option",{key:t,value:t},v(t),9,A)),64))],512),[[B,s.value,void 0,{number:!0}]])]),e("div",C,[a[3]||(a[3]=e("label",{for:"vram-select"},"VRAM (GB)",-1)),M(e("select",{id:"vram-select","onUpdate:modelValue":a[1]||(a[1]=t=>d.value=t)},[(l(),i(S,null,y(k,t=>e("option",{key:t,value:t},v(t),9,q)),64))],512),[[B,d.value,void 0,{number:!0}]])])]),e("div",z,[a[4]||(a[4]=e("strong",null,"Recommended model:",-1)),e("span",{class:w(["model-name",x.value]),style:Q({backgroundColor:n.value.bg,color:n.value.color})},v(n.value.model),7)])],2),a[6]||(a[6]=f('<blockquote data-v-adcbc991><p data-v-adcbc991><strong data-v-adcbc991>‚ÄúNot recommended‚Äù means poor conversational memory and unreliable personalization</strong> On under-resourced systems, models may forget prior context within a few exchanges or fail to maintain consistent user preferences‚Äîmaking them ineffective as true personal assistants.</p></blockquote><hr data-v-adcbc991><h2 id="how-to-use-instruct-tuned-models-as-a-personal-assistant" tabindex="-1" data-v-adcbc991>How to Use Instruct-Tuned Models as a Personal Assistant <a class="header-anchor" href="#how-to-use-instruct-tuned-models-as-a-personal-assistant" aria-label="Permalink to ‚ÄúHow to Use Instruct-Tuned Models as a Personal Assistant‚Äù" data-v-adcbc991>‚Äã</a></h2><p data-v-adcbc991>Personal assistant models thrive on <strong data-v-adcbc991>long-term context awareness</strong>, <strong data-v-adcbc991>empathetic tone</strong>, and <strong data-v-adcbc991>user-specific adaptation</strong>. These prioritize natural dialogue, recall simulation, and task coordination.</p><h3 id="_1-choose-instruct-tuned-models" tabindex="-1" data-v-adcbc991>1. <strong data-v-adcbc991>Choose Instruct-Tuned Models</strong> <a class="header-anchor" href="#_1-choose-instruct-tuned-models" aria-label="Permalink to ‚Äú1. Choose Instruct-Tuned Models‚Äù" data-v-adcbc991>‚Äã</a></h3><ul data-v-adcbc991><li data-v-adcbc991>Use <strong data-v-adcbc991><code data-v-adcbc991>instruct</code></strong> variants (e.g., <code data-v-adcbc991>qwen3 4b instruct</code>, <code data-v-adcbc991>gpt-oss 20b</code>)‚Äîthey&#39;re fine-tuned for: <ul data-v-adcbc991><li data-v-adcbc991>Following multi-turn instructions</li><li data-v-adcbc991>Remembering stated preferences (‚ÄúI prefer morning summaries‚Äù)</li><li data-v-adcbc991>Managing to-do lists, reminders, or journaling prompts</li><li data-v-adcbc991>Adapting tone (casual, professional, supportive)</li></ul></li><li data-v-adcbc991>Avoid <code data-v-adcbc991>thinking</code> models‚Äîthey may over-analyze simple requests or ignore emotional nuance.</li></ul><h3 id="_2-prioritize-context-length-quantization" tabindex="-1" data-v-adcbc991>2. <strong data-v-adcbc991>Prioritize Context Length &amp; Quantization</strong> <a class="header-anchor" href="#_2-prioritize-context-length-quantization" aria-label="Permalink to ‚Äú2. Prioritize Context Length &amp; Quantization‚Äù" data-v-adcbc991>‚Äã</a></h3><table tabindex="0" data-v-adcbc991><thead data-v-adcbc991><tr data-v-adcbc991><th data-v-adcbc991>Quant</th><th data-v-adcbc991>Assistant Impact</th></tr></thead><tbody data-v-adcbc991><tr data-v-adcbc991><td data-v-adcbc991><code data-v-adcbc991>bf16</code> / <code data-v-adcbc991>f16</code></td><td data-v-adcbc991>Best for full personality retention over long chats; ideal if you use 32K+ context</td></tr><tr data-v-adcbc991><td data-v-adcbc991><code data-v-adcbc991>q8</code></td><td data-v-adcbc991>Excellent balance‚Äîretains nuance while fitting in moderate VRAM</td></tr><tr data-v-adcbc991><td data-v-adcbc991><code data-v-adcbc991>q6</code> / <code data-v-adcbc991>q4</code></td><td data-v-adcbc991>Usable for basic tasks, but may ‚Äúforget‚Äù early conversation details in long sessions</td></tr></tbody></table><blockquote data-v-adcbc991><p data-v-adcbc991>üîπ <strong data-v-adcbc991>Tip</strong>: For personal assistants, <strong data-v-adcbc991>context length matters more than raw parameter count</strong>. A well-quantized 20B model with 32K context often outperforms a 30B model limited to 4K.</p></blockquote><h3 id="_3-enable-full-gpu-offload" tabindex="-1" data-v-adcbc991>3. <strong data-v-adcbc991>Enable Full GPU Offload</strong> <a class="header-anchor" href="#_3-enable-full-gpu-offload" aria-label="Permalink to ‚Äú3. Enable Full GPU Offload‚Äù" data-v-adcbc991>‚Äã</a></h3><ul data-v-adcbc991><li data-v-adcbc991>Always use <strong data-v-adcbc991>full GPU offload</strong> (e.g., 48/48 layers) in LM Studio to keep conversation fast and responsive.</li><li data-v-adcbc991>If LM Studio pre-selects offload settings for your model, <strong data-v-adcbc991>do not override them</strong>.</li></ul><h3 id="_4-simulate-memory-with-prompt-engineering" tabindex="-1" data-v-adcbc991>4. <strong data-v-adcbc991>Simulate Memory with Prompt Engineering</strong> <a class="header-anchor" href="#_4-simulate-memory-with-prompt-engineering" aria-label="Permalink to ‚Äú4. Simulate Memory with Prompt Engineering‚Äù" data-v-adcbc991>‚Äã</a></h3><p data-v-adcbc991>Since local models lack true persistent memory:</p><ul data-v-adcbc991><li data-v-adcbc991><strong data-v-adcbc991>Seed your prompt</strong> with key facts:<div class="language-text" data-v-adcbc991><button title="Copy Code" class="copy" data-v-adcbc991></button><span class="lang" data-v-adcbc991>text</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr" data-v-adcbc991><code data-v-adcbc991><span class="line" data-v-adcbc991><span data-v-adcbc991>You are my personal assistant. I&#39;m a developer who enjoys Blender 3D. I eat meals regularly and track my learning goals. Today is Thursday, October 23, 2025.</span></span></code></pre></div></li><li data-v-adcbc991>Use <strong data-v-adcbc991>structured recall cues</strong>: <ul data-v-adcbc991><li data-v-adcbc991>‚ÄúBased on our last conversation about Blender shaders‚Ä¶‚Äù</li><li data-v-adcbc991>‚ÄúRemind me of my food preferences before suggesting dinner ideas.‚Äù</li></ul></li></ul><h3 id="_5-avoid-none-configurations" tabindex="-1" data-v-adcbc991>5. <strong data-v-adcbc991>Avoid ‚ÄúNone‚Äù Configurations</strong> <a class="header-anchor" href="#_5-avoid-none-configurations" aria-label="Permalink to ‚Äú5. Avoid ‚ÄúNone‚Äù Configurations‚Äù" data-v-adcbc991>‚Äã</a></h3><ul data-v-adcbc991><li data-v-adcbc991>Systems returning ‚Äúnone‚Äù lack the capacity to maintain even short-term conversational state.</li><li data-v-adcbc991>Forcing a load via CPU offload leads to: <ul data-v-adcbc991><li data-v-adcbc991>Slow responses that break conversational flow</li></ul></li><li data-v-adcbc991><strong data-v-adcbc991>Minimum viable setup</strong>: ‚â•6 GB VRAM + <code data-v-adcbc991>qwen3 4b instruct q4</code> for basic assistant duties.</li></ul><h3 id="_6-combine-with-external-memory-advanced" tabindex="-1" data-v-adcbc991>6. <strong data-v-adcbc991>Combine with External Memory (Advanced)</strong> <a class="header-anchor" href="#_6-combine-with-external-memory-advanced" aria-label="Permalink to ‚Äú6. Combine with External Memory (Advanced)‚Äù" data-v-adcbc991>‚Äã</a></h3><p data-v-adcbc991>For true long-term memory:</p><ul data-v-adcbc991><li data-v-adcbc991>Log key interactions to a local file or database</li><li data-v-adcbc991>Inject summarized memory into each new session:<div class="language-text" data-v-adcbc991><button title="Copy Code" class="copy" data-v-adcbc991></button><span class="lang" data-v-adcbc991>text</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr" data-v-adcbc991><code data-v-adcbc991><span class="line" data-v-adcbc991><span data-v-adcbc991>[Memory Summary: User is learning Blender geometry nodes. Last discussed procedural terrain generation on Oct 20. Prefers vegetarian meal suggestions.]</span></span></code></pre></div></li><li data-v-adcbc991>This turns even a 4B model into a surprisingly capable companion.</li></ul><h3 id="_7-monitor-resource-usage" tabindex="-1" data-v-adcbc991>7. <strong data-v-adcbc991>Monitor Resource Usage</strong> <a class="header-anchor" href="#_7-monitor-resource-usage" aria-label="Permalink to ‚Äú7. Monitor Resource Usage‚Äù" data-v-adcbc991>‚Äã</a></h3><ul data-v-adcbc991><li data-v-adcbc991>Keep VRAM usage <strong data-v-adcbc991>below 90%</strong> to avoid swapping, which destroys real-time responsiveness.</li><li data-v-adcbc991>On Windows, disable background apps (Discord, browsers) to free up the extra 1-2 GB needed for smooth 16K context.</li></ul><hr data-v-adcbc991><p data-v-adcbc991>By selecting the right instruct-tuned model for your hardware and structuring interactions to simulate memory, you can create a <strong data-v-adcbc991>responsive, personalized, and helpful local AI assistant</strong>‚Äîwithout relying on cloud services or sacrificing privacy.</p>',23))]))}}),E=G(U,[["__scopeId","data-v-adcbc991"]]);export{V as __pageData,E as default};
