import{_ as I,x as f,h as o,c as l,o as c,af as b,j as e,ag as y,ah as _,G as w,H as P,t as v,O as T,n as x}from"./chunks/framework.t7omRjYa.js";const q={class:"controls"},R={class:"control-group"},B=["value"],z={class:"control-group"},V=["value"],Q={class:"result"},G=JSON.parse('{"title":"Coding","description":"","frontmatter":{"title":"Coding"},"headers":[],"relativePath":"recommendations/coding/index.md","filePath":"recommendations/coding/index.md"}'),U={name:"recommendations/coding/index.md"},N=Object.assign(U,{setup(W){const i=f(16),s=f(8),M=[16,32,64,128],k=[0,4,6,8,12,16,24,32],A=[{ramMin:64,vramMin:16,model:"Qwen3 Coder 30B A3B Instruct bf16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:32,vramMin:16,model:"Qwen3 Coder 30B A3B Instruct Q8",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:32,vramMin:6,model:"Qwen3 Coder 30B A3B Instruct Q6",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:16,vramMin:4,model:"Qwen3 4B Instruct 2507 BF16",color:"var(--vp-c-purple-2)",bg:"var(--vp-c-purple-soft)"}],n=o(()=>{const d=A.find(a=>i.value>=a.ramMin&&s.value>=a.vramMin);return d?{model:d.model,color:d.color,bg:d.bg}:{model:"Not recommended",color:"var(--vp-c-text-3)",bg:"transparent"}}),u=o(()=>n.value.model!=="Not recommended"),r=o(()=>n.value.model.toLowerCase()),m=o(()=>r.value.includes("bf16")||r.value.includes("gpt oss")),g=o(()=>r.value.includes("q6")||r.value.includes("q8")),h=o(()=>r.value.includes("q4")),p=o(()=>r.value.includes("4b")),C=o(()=>u.value?p.value?{"recommended-4b":!0}:m.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:h.value?{"recommended-warning":!0}:{}:{"not-recommended":!0}),S=o(()=>u.value?p.value?{"recommended-4b":!0}:m.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:h.value?{"recommended-warning":!0}:{}:{"not-recommended":!0});return(d,a)=>(c(),l("div",null,[a[5]||(a[5]=b('<h1 id="coding-recommendations" tabindex="-1" data-v-7970924a>Coding Recommendations <a class="header-anchor" href="#coding-recommendations" aria-label="Permalink to “Coding Recommendations”" data-v-7970924a>​</a></h1><p data-v-7970924a><strong data-v-7970924a>Prioritizing Very High Precision in Code Generation, Completion, and Debugging</strong></p><p data-v-7970924a>When working with AI-assisted coding, <strong data-v-7970924a>precision is non-negotiable</strong>. Even minor hallucinations, syntactic errors, or incorrect logic can introduce bugs, security vulnerabilities, or maintenance debt. These recommendations are tailored for developers who require <strong data-v-7970924a>extremely high-fidelity outputs</strong>—not just plausible-looking code, but <strong data-v-7970924a>correct, production-ready, and logically sound</strong> implementations.</p><p data-v-7970924a>Use the selector below to identify the best model <strong data-v-7970924a>that balances your hardware constraints with the highest achievable precision</strong>:</p>',4)),e("div",{class:x(["model-selector",C.value])},[e("div",q,[e("div",R,[a[2]||(a[2]=e("label",{for:"ram-select"},"RAM (GB)",-1)),y(e("select",{id:"ram-select","onUpdate:modelValue":a[0]||(a[0]=t=>i.value=t)},[(c(),l(w,null,P(M,t=>e("option",{key:t,value:t},v(t),9,B)),64))],512),[[_,i.value,void 0,{number:!0}]])]),e("div",z,[a[3]||(a[3]=e("label",{for:"vram-select"},"VRAM (GB)",-1)),y(e("select",{id:"vram-select","onUpdate:modelValue":a[1]||(a[1]=t=>s.value=t)},[(c(),l(w,null,P(k,t=>e("option",{key:t,value:t},v(t),9,V)),64))],512),[[_,s.value,void 0,{number:!0}]])])]),e("div",Q,[a[4]||(a[4]=e("strong",null,"Recommended model:",-1)),e("span",{class:x(["model-name",S.value]),style:T({backgroundColor:n.value.bg,color:n.value.color})},v(n.value.model),7)])],2),a[6]||(a[6]=b('<h2 id="why-precision-matters-in-coding" tabindex="-1" data-v-7970924a>Why Precision Matters in Coding <a class="header-anchor" href="#why-precision-matters-in-coding" aria-label="Permalink to “Why Precision Matters in Coding”" data-v-7970924a>​</a></h2><p data-v-7970924a>Unlike creative or conversational tasks, <strong data-v-7970924a>code must be functionally correct</strong>. A model that “sounds right” but produces broken logic, incorrect APIs, or unsafe patterns is worse than no model at all. Therefore, <strong data-v-7970924a>favor models and configurations that maximize precision—even at the cost of speed or resource usage</strong>.</p><h3 id="_1-use-full-gpu-offload-when-possible" tabindex="-1" data-v-7970924a>1. <strong data-v-7970924a>Use Full GPU Offload (When Possible)</strong> <a class="header-anchor" href="#_1-use-full-gpu-offload-when-possible" aria-label="Permalink to “1. Use Full GPU Offload (When Possible)”" data-v-7970924a>​</a></h3><ul data-v-7970924a><li data-v-7970924a>In <strong data-v-7970924a>LM Studio</strong>, always enable <strong data-v-7970924a>full GPU offload</strong> (e.g., <code data-v-7970924a>48/48</code> layers for Qwen3 models).</li></ul><h3 id="_2-prefer-instruct-tuned-code-specialized-models" tabindex="-1" data-v-7970924a>2. <strong data-v-7970924a>Prefer Instruct-Tuned, Code-Specialized Models</strong> <a class="header-anchor" href="#_2-prefer-instruct-tuned-code-specialized-models" aria-label="Permalink to “2. Prefer Instruct-Tuned, Code-Specialized Models”" data-v-7970924a>​</a></h3><ul data-v-7970924a><li data-v-7970924a>Only use <strong data-v-7970924a>code-instruct variants</strong> like <code data-v-7970924a>qwen3-coder-30b-instruct</code>. These are fine-tuned on millions of correct code examples and aligned for <strong data-v-7970924a>semantic and syntactic accuracy</strong>.</li><li data-v-7970924a>Avoid base models or “thinking” variants—they lack the precision tuning needed for reliable code output.</li></ul><h3 id="_3-context-window-memory-stability" tabindex="-1" data-v-7970924a>3. <strong data-v-7970924a>Context Window &amp; Memory Stability</strong> <a class="header-anchor" href="#_3-context-window-memory-stability" aria-label="Permalink to “3. Context Window &amp; Memory Stability”" data-v-7970924a>​</a></h3><ul data-v-7970924a><li data-v-7970924a>All recommendations assume a <strong data-v-7970924a>stable ~16K context window</strong>.</li></ul><h3 id="_4-quantization-precision-vs-practicality" tabindex="-1" data-v-7970924a>4. <strong data-v-7970924a>Quantization: Precision vs. Practicality</strong> <a class="header-anchor" href="#_4-quantization-precision-vs-practicality" aria-label="Permalink to “4. Quantization: Precision vs. Practicality”" data-v-7970924a>​</a></h3><table tabindex="0" data-v-7970924a><thead data-v-7970924a><tr data-v-7970924a><th data-v-7970924a>Quant</th><th data-v-7970924a>Speed</th><th data-v-7970924a><strong data-v-7970924a>Precision</strong></th><th data-v-7970924a>VRAM Use</th></tr></thead><tbody data-v-7970924a><tr data-v-7970924a><td data-v-7970924a><code data-v-7970924a>bf16</code> / <code data-v-7970924a>f16</code></td><td data-v-7970924a>★☆☆☆☆</td><td data-v-7970924a>★★★★★ (<strong data-v-7970924a>Highest</strong>)</td><td data-v-7970924a>Highest</td></tr><tr data-v-7970924a><td data-v-7970924a><code data-v-7970924a>q8</code></td><td data-v-7970924a>★★★☆☆</td><td data-v-7970924a>★★★★☆ (<strong data-v-7970924a>High</strong>)</td><td data-v-7970924a>Medium-High</td></tr><tr data-v-7970924a><td data-v-7970924a><code data-v-7970924a>q6</code></td><td data-v-7970924a>★★★★☆</td><td data-v-7970924a>★★★☆☆ (<strong data-v-7970924a>Moderate</strong>)</td><td data-v-7970924a>Medium</td></tr><tr data-v-7970924a><td data-v-7970924a><code data-v-7970924a>q4</code></td><td data-v-7970924a>★★★★★</td><td data-v-7970924a>★☆☆☆☆ (<strong data-v-7970924a>Low - Not Recommended for Precision Work</strong>)</td><td data-v-7970924a>Lowest</td></tr></tbody></table><blockquote data-v-7970924a><p data-v-7970924a>💡 <strong data-v-7970924a>For high-stakes coding (e.g., production systems, security-sensitive logic, or complex algorithms), always prefer <code data-v-7970924a>bf16</code>, <code data-v-7970924a>f16</code>, or at minimum <code data-v-7970924a>q8</code>.</strong> Avoid <code data-v-7970924a>q4</code> unless absolutely necessary—it sacrifices too much precision.</p></blockquote><h3 id="_5-prompt-engineering-for-correctness" tabindex="-1" data-v-7970924a>5. <strong data-v-7970924a>Prompt Engineering for Correctness</strong> <a class="header-anchor" href="#_5-prompt-engineering-for-correctness" aria-label="Permalink to “5. Prompt Engineering for Correctness”" data-v-7970924a>​</a></h3><ul data-v-7970924a><li data-v-7970924a><strong data-v-7970924a>Be explicit and constrained</strong>:<br data-v-7970924a> ❌ <em data-v-7970924a>“Write a login function.”</em><br data-v-7970924a> ✅ <em data-v-7970924a>“Write a secure Python FastAPI login endpoint using OAuth2 password flow, with bcrypt hashing, rate limiting, and proper error responses.”</em></li><li data-v-7970924a><strong data-v-7970924a>Request verification steps</strong>: Ask the model to “explain why this implementation is safe” or “list potential edge cases.”</li><li data-v-7970924a><strong data-v-7970924a>Include error context</strong>: Paste stack traces or test failures—this helps the model reason precisely about the failure mode.</li></ul><h3 id="_6-avoid-none-or-underpowered-setups" tabindex="-1" data-v-7970924a>6. <strong data-v-7970924a>Avoid “None” or Underpowered Setups</strong> <a class="header-anchor" href="#_6-avoid-none-or-underpowered-setups" aria-label="Permalink to “6. Avoid “None” or Underpowered Setups”" data-v-7970924a>​</a></h3><ul data-v-7970924a><li data-v-7970924a>If the matrix returns <strong data-v-7970924a>“none”</strong>, <strong data-v-7970924a>do not force execution</strong> via heavy RAM offloading.</li><li data-v-7970924a>Under-resourced inference <strong data-v-7970924a>increases hallucination rates</strong> and reduces logical consistency—<strong data-v-7970924a>unacceptable for precision-critical workflows</strong>.</li><li data-v-7970924a>Consider cloud inference (e.g., with <code data-v-7970924a>bf16</code> models) if local hardware is insufficient.</li></ul><h3 id="_7-validate-and-monitor" tabindex="-1" data-v-7970924a>7. <strong data-v-7970924a>Validate and Monitor</strong> <a class="header-anchor" href="#_7-validate-and-monitor" aria-label="Permalink to “7. Validate and Monitor”" data-v-7970924a>​</a></h3><ul data-v-7970924a><li data-v-7970924a><strong data-v-7970924a>Never trust output blindly</strong>. Always: <ul data-v-7970924a><li data-v-7970924a>Run static analysis (e.g., <code data-v-7970924a>mypy</code>, <code data-v-7970924a>eslint</code>, <code data-v-7970924a>bandit</code>)</li><li data-v-7970924a>Execute unit tests</li><li data-v-7970924a>Review for logic correctness</li></ul></li><li data-v-7970924a>Monitor VRAM usage: sustained &gt;95% utilization can cause <strong data-v-7970924a>numerical errors</strong> that silently degrade output quality.</li></ul><hr data-v-7970924a><p data-v-7970924a>By aligning your tooling with the <strong data-v-7970924a>highest-precision models your hardware can support</strong>, you ensure that AI assistance enhances—rather than undermines—code quality, security, and maintainability. <strong data-v-7970924a>When correctness is paramount, precision isn&#39;t optional—it&#39;s essential.</strong></p>',19))]))}}),E=I(N,[["__scopeId","data-v-7970924a"]]);export{G as __pageData,E as default};
