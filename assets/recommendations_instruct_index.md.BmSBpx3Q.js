import{_ as S,x as y,h as r,c as i,o as c,j as e,af as R,a as v,ag as b,ah as w,G as M,H as I,t as u,O as A,n as Q}from"./chunks/framework.t7omRjYa.js";const q={class:"controls"},T={class:"control-group"},U=["value"],V={class:"control-group"},F=["value"],N={class:"result"},Y=JSON.parse('{"title":"Instruct Models","description":"","frontmatter":{"title":"Instruct Models"},"headers":[],"relativePath":"recommendations/instruct/index.md","filePath":"recommendations/instruct/index.md"}'),C={name:"recommendations/instruct/index.md"},G=Object.assign(C,{setup(L){const s=y(16),d=y(8),_=[16,32,64,128],B=[0,4,6,8,12,16,24,32],x=[{ramMin:128,vramMin:32,model:"Mistral Small Q8 or Qwen3 30B Instruct 2507 BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:24,model:"Mistral Small Q6 or Qwen3 30B Instruct 2507 BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:0,model:"Qwen3 30B Instruct 2507 BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:64,vramMin:32,model:"Mistral Small Q8 or Qwen3 30B Instruct 2507 BF16",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:24,model:"Mistral Small Q6 or Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:16,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:64,vramMin:12,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:64,vramMin:8,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:64,vramMin:6,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:64,vramMin:4,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:64,vramMin:0,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:32,vramMin:32,model:"Mistral Small Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:24,model:"Mistral Small Q6 or Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:16,model:"Qwen3 30B Instruct 2507 Q8",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:32,vramMin:8,model:"Qwen3 30B Instruct 2507 Q6",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:32,vramMin:4,model:"Qwen3 4B Instruct 2507 BF16",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:16,vramMin:32,model:"Mistral Small Q8",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:24,model:"Mistral Small Q6",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:12,model:"Qwen3 4B Instruct 2507 BF16",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:4,model:"Qwen3 4B Instruct 2507 Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"}],n=r(()=>{const l=x.find(a=>s.value>=a.ramMin&&d.value>=a.vramMin);return l?{model:l.model,color:l.color,bg:l.bg}:{model:"Not recommended",color:"var(--vp-c-text-3)",bg:"transparent"}}),m=r(()=>n.value.model!=="Not recommended"),o=r(()=>n.value.model.toLowerCase()),f=r(()=>o.value.includes("bf16")||o.value.includes("gpt oss")),g=r(()=>o.value.includes("q6")||o.value.includes("q8")),p=r(()=>o.value.includes("q4")),h=r(()=>o.value.includes("4b")),k=r(()=>m.value?h.value?{"recommended-4b":!0}:f.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:p.value?{"recommended-warning":!0}:{}:{"not-recommended":!0}),P=r(()=>m.value?h.value?{"recommended-4b":!0}:f.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:p.value?{"recommended-warning":!0}:{}:{"not-recommended":!0});return(l,a)=>(c(),i("div",null,[a[5]||(a[5]=e("h1",{id:"instruct-tuned-models",tabindex:"-1"},[v("Instruct-Tuned Models "),e("a",{class:"header-anchor",href:"#instruct-tuned-models","aria-label":"Permalink to “Instruct-Tuned Models”"},"​")],-1)),a[6]||(a[6]=e("p",null,"Instruct-tuned models are AI assistants trained specifically to understand and follow your directions—whether you're asking for help writing an email, summarizing a document, planning a trip, or explaining a complex idea. Unlike raw base models, these are fine-tuned to respond helpfully, clearly, and on-topic.",-1)),a[7]||(a[7]=e("p",null,[v("Use the selector below to find the best "),e("strong",null,"instruct-tuned"),v(" model that matches your computer’s capabilities:")],-1)),e("div",{class:Q(["model-selector",k.value])},[e("div",q,[e("div",T,[a[2]||(a[2]=e("label",{for:"ram-select"},"RAM (GB)",-1)),b(e("select",{id:"ram-select","onUpdate:modelValue":a[0]||(a[0]=t=>s.value=t)},[(c(),i(M,null,I(_,t=>e("option",{key:t,value:t},u(t),9,U)),64))],512),[[w,s.value,void 0,{number:!0}]])]),e("div",V,[a[3]||(a[3]=e("label",{for:"vram-select"},"VRAM (GB)",-1)),b(e("select",{id:"vram-select","onUpdate:modelValue":a[1]||(a[1]=t=>d.value=t)},[(c(),i(M,null,I(B,t=>e("option",{key:t,value:t},u(t),9,F)),64))],512),[[w,d.value,void 0,{number:!0}]])])]),e("div",N,[a[4]||(a[4]=e("strong",null,"Recommended model:",-1)),e("span",{class:Q(["model-name",P.value]),style:A({backgroundColor:n.value.bg,color:n.value.color})},u(n.value.model),7)])],2),a[8]||(a[8]=R('<h2 id="how-to-use-instruct-models-effectively" tabindex="-1" data-v-75a8f46a>How to Use Instruct Models Effectively <a class="header-anchor" href="#how-to-use-instruct-models-effectively" aria-label="Permalink to “How to Use Instruct Models Effectively”" data-v-75a8f46a>​</a></h2><p data-v-75a8f46a>These models shine when given clear, thoughtful instructions. Follow these tips to get the most helpful, accurate, and reliable responses—whether you&#39;re drafting messages, researching topics, or organizing your day.</p><h3 id="_1-always-choose-an-instruct-model" tabindex="-1" data-v-75a8f46a>1. <strong data-v-75a8f46a>Always Choose an “Instruct” Model</strong> <a class="header-anchor" href="#_1-always-choose-an-instruct-model" aria-label="Permalink to “1. Always Choose an “Instruct” Model”" data-v-75a8f46a>​</a></h3><ul data-v-75a8f46a><li data-v-75a8f46a>Look for <strong data-v-75a8f46a><code data-v-75a8f46a>instruct</code></strong> in the model name (e.g., <code data-v-75a8f46a>qwen3 30b instruct</code>, <code data-v-75a8f46a>mistral small</code>). These are specially trained to follow directions.</li><li data-v-75a8f46a>Non-instruct models may ignore your request or give generic, off-topic replies.</li></ul><h3 id="_2-pick-the-right-version-for-your-hardware" tabindex="-1" data-v-75a8f46a>2. <strong data-v-75a8f46a>Pick the Right Version for Your Hardware</strong> <a class="header-anchor" href="#_2-pick-the-right-version-for-your-hardware" aria-label="Permalink to “2. Pick the Right Version for Your Hardware”" data-v-75a8f46a>​</a></h3><table tabindex="0" data-v-75a8f46a><thead data-v-75a8f46a><tr data-v-75a8f46a><th data-v-75a8f46a>Version</th><th data-v-75a8f46a>Best For</th><th data-v-75a8f46a>Notes</th></tr></thead><tbody data-v-75a8f46a><tr data-v-75a8f46a><td data-v-75a8f46a><code data-v-75a8f46a>bf16</code> / <code data-v-75a8f46a>f16</code></td><td data-v-75a8f46a>Best quality, powerful GPUs</td><td data-v-75a8f46a>Highest accuracy; needs lots of VRAM</td></tr><tr data-v-75a8f46a><td data-v-75a8f46a><code data-v-75a8f46a>q6</code> / <code data-v-75a8f46a>q8</code></td><td data-v-75a8f46a>Great balance of speed and clarity</td><td data-v-75a8f46a>Works well on most modern laptops or desktops</td></tr><tr data-v-75a8f46a><td data-v-75a8f46a><code data-v-75a8f46a>q4</code></td><td data-v-75a8f46a>Limited VRAM (e.g., older or entry-level GPUs)</td><td data-v-75a8f46a>Faster but may miss finer details or nuance</td></tr></tbody></table><h3 id="_3-use-your-gpu-fully-if-available" tabindex="-1" data-v-75a8f46a>3. <strong data-v-75a8f46a>Use Your GPU Fully (If Available)</strong> <a class="header-anchor" href="#_3-use-your-gpu-fully-if-available" aria-label="Permalink to “3. Use Your GPU Fully (If Available)”" data-v-75a8f46a>​</a></h3><ul data-v-75a8f46a><li data-v-75a8f46a>In apps like <strong data-v-75a8f46a>LM Studio</strong>, enable full GPU offloading to keep things fast and smooth.</li><li data-v-75a8f46a>Example: For <code data-v-75a8f46a>qwen3 30b instruct</code>, try loading <strong data-v-75a8f46a>all layers</strong> onto the GPU if you have enough VRAM.</li><li data-v-75a8f46a>Partial use of the GPU can cause slowdowns as the system swaps data between memory and graphics card.</li></ul><h3 id="_4-mind-the-context-length" tabindex="-1" data-v-75a8f46a>4. <strong data-v-75a8f46a>Mind the Context Length</strong> <a class="header-anchor" href="#_4-mind-the-context-length" aria-label="Permalink to “4. Mind the Context Length”" data-v-75a8f46a>​</a></h3><ul data-v-75a8f46a><li data-v-75a8f46a>All recommended models support <strong data-v-75a8f46a>16,000+ tokens</strong> of context—enough for long emails, reports, or multi-turn conversations.</li><li data-v-75a8f46a>Very long inputs use more memory; if your system feels sluggish, shorten your prompt or reduce the model’s quantization.</li></ul><h3 id="_5-be-clear-and-specific-in-your-requests" tabindex="-1" data-v-75a8f46a>5. <strong data-v-75a8f46a>Be Clear and Specific in Your Requests</strong> <a class="header-anchor" href="#_5-be-clear-and-specific-in-your-requests" aria-label="Permalink to “5. Be Clear and Specific in Your Requests”" data-v-75a8f46a>​</a></h3><ul data-v-75a8f46a><li data-v-75a8f46a>❌ <em data-v-75a8f46a>“Tell me about climate change.”</em></li><li data-v-75a8f46a>✅ <em data-v-75a8f46a>“Explain the main causes of climate change in simple terms, as if I’m 15 years old.”</em></li><li data-v-75a8f46a>The more precise your instruction—goal, audience, format, or examples—the better the response.</li></ul><h3 id="_6-avoid-not-recommended-setups" tabindex="-1" data-v-75a8f46a>6. <strong data-v-75a8f46a>Avoid “Not Recommended” Setups</strong> <a class="header-anchor" href="#_6-avoid-not-recommended-setups" aria-label="Permalink to “6. Avoid “Not Recommended” Setups”" data-v-75a8f46a>​</a></h3><ul data-v-75a8f46a><li data-v-75a8f46a>If the tool says <strong data-v-75a8f46a>“Not recommended,”</strong> your device likely can’t run even the smallest instruct model well.</li><li data-v-75a8f46a>Trying to force it may result in <strong data-v-75a8f46a>slow replies, crashes, or unreliable answers</strong>.</li><li data-v-75a8f46a>In that case, consider using a cloud-based service (like OpenRouter or Together.ai) or upgrading your hardware.</li></ul><h3 id="_7-watch-your-system-resources" tabindex="-1" data-v-75a8f46a>7. <strong data-v-75a8f46a>Watch Your System Resources</strong> <a class="header-anchor" href="#_7-watch-your-system-resources" aria-label="Permalink to “7. Watch Your System Resources”" data-v-75a8f46a>​</a></h3><ul data-v-75a8f46a><li data-v-75a8f46a><strong data-v-75a8f46a>Windows</strong>: Open Task Manager → Performance → GPU</li><li data-v-75a8f46a><strong data-v-75a8f46a>Linux</strong>: Use <code data-v-75a8f46a>nvidia-smi</code> (NVIDIA) or <code data-v-75a8f46a>radeontop</code> (AMD)</li><li data-v-75a8f46a>If VRAM usage goes above <strong data-v-75a8f46a>90%</strong>, try a lighter model version (e.g., switch from <code data-v-75a8f46a>q8</code> to <code data-v-75a8f46a>q6</code>) or shorten your input.</li></ul><p data-v-75a8f46a>By matching the right model to your hardware and giving clear, purposeful instructions, you’ll enjoy a smooth, intelligent assistant experience—perfect for everyday tasks, learning, planning, and more.</p>',17))]))}}),z=S(G,[["__scopeId","data-v-75a8f46a"]]);export{Y as __pageData,z as default};
