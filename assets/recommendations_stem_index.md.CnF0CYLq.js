import{M as t}from"./chunks/ModelSelector.VrCpm0KP.js";import{c as a,o,af as n,J as s}from"./chunks/framework.CuQK53I8.js";const u=JSON.parse('{"title":"STEM","description":"","frontmatter":{"title":"STEM"},"headers":[],"relativePath":"recommendations/stem/index.md","filePath":"recommendations/stem/index.md"}'),r={name:"recommendations/stem/index.md"},g=Object.assign(r,{setup(l){const i=[{ramMin:128,vramMin:0,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"BF16"}}],usefulness:1},{ramMin:64,vramMin:32,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"BF16"}}],usefulness:.8},{ramMin:64,vramMin:12,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.6},{ramMin:64,vramMin:0,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"Q6_K_XL"}}],usefulness:.5},{ramMin:32,vramMin:32,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.7},{ramMin:32,vramMin:24,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.6},{ramMin:32,vramMin:12,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"BF16"}}],usefulness:.5},{ramMin:32,vramMin:8,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"Q8_K_XL"}}],usefulness:.4},{ramMin:32,vramMin:0,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"Q6_K_XL"}}],usefulness:.3},{ramMin:16,vramMin:32,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"BF16"}},{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"Q6_K_XL"}}],usefulness:.5},{ramMin:16,vramMin:12,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"BF16"}}],usefulness:.4},{ramMin:16,vramMin:8,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"Q8_K_XL"}}],usefulness:.3},{ramMin:16,vramMin:6,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"Q6_K_XL"}}],usefulness:.25},{ramMin:16,vramMin:4,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"Q4_K_XL"}}],usefulness:.2}];return(d,e)=>(o(),a("div",null,[e[0]||(e[0]=n('<h1 id="stem-science-technology-engineering-mathematics" tabindex="-1">STEM (Science, Technology, Engineering, Mathematics) <a class="header-anchor" href="#stem-science-technology-engineering-mathematics" aria-label="Permalink to â€œSTEM (Science, Technology, Engineering, Mathematics)â€">â€‹</a></h1><p><strong>Thinking-tuned models</strong> optimized for technical reasoning, mathematical precision, scientific problem-solving, and code-heavy workflows. These models excel at tasks like deriving equations, debugging complex systems, simulating experiments, and explaining STEM concepts with rigor.</p><blockquote><p>ğŸ’¡ <strong>Note</strong>: All models listed below are <em>thinking-tuned variants</em> (e.g., <code>qwen3 4b thinking</code>), which are specifically fine-tuned for analytical depthâ€”not general-purpose chat. They outperform standard instruct models on logic-heavy, multi-step STEM problems.</p></blockquote><p>Use the selector below to find the best <strong>thinking-tuned</strong> model for your hardware:</p>',4)),s(t,{modelDefinitions:i}),e[1]||(e[1]=n('<blockquote><p><strong>â€œNot recommendedâ€ means unreliable STEM output</strong> If the selector returns â€œNot recommended,â€ your system likely lacks the resources to run even the smallest thinking model effectively. In such cases, output quality degrades severelyâ€”often worse than manually reasoning through the problem for 5+ minutes.</p></blockquote><hr><h2 id="how-to-use-thinking-tuned-models-for-stem-work" tabindex="-1">How to Use Thinking-Tuned Models for STEM Work <a class="header-anchor" href="#how-to-use-thinking-tuned-models-for-stem-work" aria-label="Permalink to â€œHow to Use Thinking-Tuned Models for STEM Workâ€">â€‹</a></h2><p>These models are designed for <strong>deep technical reasoning</strong>, not casual conversation. Follow these guidelines to maximize accuracy and performance in scientific, engineering, or mathematical contexts.</p><h3 id="_1-use-only-thinking-tuned-variants" tabindex="-1">1. <strong>Use Only Thinking-Tuned Variants</strong> <a class="header-anchor" href="#_1-use-only-thinking-tuned-variants" aria-label="Permalink to â€œ1. Use Only Thinking-Tuned Variantsâ€">â€‹</a></h3><ul><li>Only models labeled <strong><code>thinking</code></strong> (e.g., <code>qwen3 4b thinking</code>, <code>qwen3 30b thinking</code>) are fine-tuned for analytical tasks like: <ul><li>Solving differential equations</li><li>Deriving physics formulas</li><li>Debugging numerical simulations</li><li>Explaining quantum mechanics or circuit design</li></ul></li><li>Standard <code>instruct</code> or base models often <strong>fail on multi-step logic</strong> or produce plausible but incorrect STEM conclusions.</li></ul><h3 id="_2-understand-quantization-trade-offs" tabindex="-1">2. <strong>Understand Quantization Trade-offs</strong> <a class="header-anchor" href="#_2-understand-quantization-trade-offs" aria-label="Permalink to â€œ2. Understand Quantization Trade-offsâ€">â€‹</a></h3><table tabindex="0"><thead><tr><th>Quant</th><th>Use Case</th><th>STEM Impact</th></tr></thead><tbody><tr><td><code>bf16</code> / <code>f16</code></td><td>High-precision math, symbolic reasoning</td><td>Best for accuracy in calculus, linear algebra, or physics derivations</td></tr><tr><td><code>q8</code></td><td>Balanced performance</td><td>Suitable for most coding + math tasks (e.g., Python + NumPy workflows)</td></tr><tr><td><code>q6</code> / <code>q4</code></td><td>Low-resource systems</td><td>May skip steps in proofs or mis-evaluate edge casesâ€”use only when necessary</td></tr></tbody></table><h3 id="_3-full-gpu-offload-is-critical" tabindex="-1">3. <strong>Full GPU Offload Is Critical</strong> <a class="header-anchor" href="#_3-full-gpu-offload-is-critical" aria-label="Permalink to â€œ3. Full GPU Offload Is Criticalâ€">â€‹</a></h3><ul><li><strong>Context lives in VRAM</strong>â€”always aim for <strong>full GPU offload</strong> (e.g., 48/48 layers in LM Studio).</li><li>If LM Studio suggests default offload settings for a model, <strong>keep them</strong>â€”they&#39;re tuned for stability.</li></ul><h3 id="_4-ram-vram-combo-rule-for-cpu-offload" tabindex="-1">4. <strong>RAM+VRAM Combo Rule (For CPU Offload)</strong> <a class="header-anchor" href="#_4-ram-vram-combo-rule-for-cpu-offload" aria-label="Permalink to â€œ4. RAM+VRAM Combo Rule (For CPU Offload)â€">â€‹</a></h3><p>If using partial CPU offload:</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span>Total Free RAM + VRAM â‰¥ Model Size + 4 GB (context + overhead)</span></span></code></pre></div><ul><li>Example: <code>qwen3-30b-thinking-q8</code> â‰ˆ 36 GB â†’ requires <strong>â‰¥40 GB combined free memory</strong>.</li><li>This setup works but is <strong>not ideal for real-time STEM exploration</strong>â€”expect latency.</li></ul><h3 id="_5-prompt-with-precision" tabindex="-1">5. <strong>Prompt with Precision</strong> <a class="header-anchor" href="#_5-prompt-with-precision" aria-label="Permalink to â€œ5. Prompt with Precisionâ€">â€‹</a></h3><p>STEM models thrive on <strong>structured, explicit prompts</strong>:</p><ul><li>âŒ <em>â€œHow do I solve this?â€</em></li><li>âœ… <em>â€œGiven a 2D heat equation âˆ‚u/âˆ‚t = Î±âˆ‡Â²u on [0,1]Ã—[0,1] with Dirichlet BCs, derive the finite difference scheme using central differences and Î”x=Î”y=0.1.â€</em></li><li>Include: <ul><li>Known variables &amp; constraints</li><li>Desired output format (e.g., â€œshow all steps,â€ â€œreturn Python codeâ€)</li><li>Relevant domain (e.g., â€œin classical electromagnetismâ€¦â€)</li></ul></li></ul><h3 id="_6-avoid-none-configurations" tabindex="-1">6. <strong>Avoid â€œNoneâ€ Configurations</strong> <a class="header-anchor" href="#_6-avoid-none-configurations" aria-label="Permalink to â€œ6. Avoid â€œNoneâ€ Configurationsâ€">â€‹</a></h3><ul><li>If your hardware yields <strong>â€œnoneâ€</strong>, do <strong>not</strong> force a model load via heavy CPU offload.</li><li>You&#39;ll get <strong>hallucinated derivations</strong>, incorrect unit conversions, or broken logicâ€”worse than no model at all.</li><li>Alternatives: <ul><li>Use cloud inference (e.g., Together.ai with <code>qwen3-30b-thinking</code>)</li><li>Upgrade to â‰¥8 GB VRAM for basic STEM tasks</li></ul></li></ul><h3 id="_7-monitor-real-time-usage" tabindex="-1">7. <strong>Monitor Real-Time Usage</strong> <a class="header-anchor" href="#_7-monitor-real-time-usage" aria-label="Permalink to â€œ7. Monitor Real-Time Usageâ€">â€‹</a></h3><ul><li><strong>Windows</strong>: Task Manager â†’ Performance â†’ GPU &amp; Memory</li><li><strong>Linux</strong>: <code>nvidia-smi</code> (NVIDIA) or <code>radeontop</code> (AMD)</li><li>If VRAM &gt;90% usage, reduce context length or switch to a lower quant (e.g., <code>bf16</code> â†’ <code>q8</code>).</li></ul><hr><p>By aligning your hardware capabilities, quantization choice, and prompt engineering with these principles, you&#39;ll unlock <strong>reliable, step-by-step technical reasoning</strong>â€”whether you&#39;re proving theorems, simulating systems, or designing experiments.</p>',23))]))}});export{u as __pageData,g as default};
