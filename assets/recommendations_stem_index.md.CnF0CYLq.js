import{M as t}from"./chunks/ModelSelector.VrCpm0KP.js";import{c as a,o,af as n,J as s}from"./chunks/framework.CuQK53I8.js";const u=JSON.parse('{"title":"STEM","description":"","frontmatter":{"title":"STEM"},"headers":[],"relativePath":"recommendations/stem/index.md","filePath":"recommendations/stem/index.md"}'),r={name:"recommendations/stem/index.md"},g=Object.assign(r,{setup(l){const i=[{ramMin:128,vramMin:0,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"BF16"}}],usefulness:1},{ramMin:64,vramMin:32,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"BF16"}}],usefulness:.8},{ramMin:64,vramMin:12,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.6},{ramMin:64,vramMin:0,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"Q6_K_XL"}}],usefulness:.5},{ramMin:32,vramMin:32,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.7},{ramMin:32,vramMin:24,models:[{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.6},{ramMin:32,vramMin:12,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"BF16"}}],usefulness:.5},{ramMin:32,vramMin:8,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"Q8_K_XL"}}],usefulness:.4},{ramMin:32,vramMin:0,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"Q6_K_XL"}}],usefulness:.3},{ramMin:16,vramMin:32,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"BF16"}},{"Qwen3 30B Thinking 2507":{parameters:30,quantization:"Q6_K_XL"}}],usefulness:.5},{ramMin:16,vramMin:12,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"BF16"}}],usefulness:.4},{ramMin:16,vramMin:8,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"Q8_K_XL"}}],usefulness:.3},{ramMin:16,vramMin:6,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"Q6_K_XL"}}],usefulness:.25},{ramMin:16,vramMin:4,models:[{"Qwen3 4B Thinking 2507":{parameters:4,quantization:"Q4_K_XL"}}],usefulness:.2}];return(d,e)=>(o(),a("div",null,[e[0]||(e[0]=n('<h1 id="stem-science-technology-engineering-mathematics" tabindex="-1">STEM (Science, Technology, Engineering, Mathematics) <a class="header-anchor" href="#stem-science-technology-engineering-mathematics" aria-label="Permalink to “STEM (Science, Technology, Engineering, Mathematics)”">​</a></h1><p><strong>Thinking-tuned models</strong> optimized for technical reasoning, mathematical precision, scientific problem-solving, and code-heavy workflows. These models excel at tasks like deriving equations, debugging complex systems, simulating experiments, and explaining STEM concepts with rigor.</p><blockquote><p>💡 <strong>Note</strong>: All models listed below are <em>thinking-tuned variants</em> (e.g., <code>qwen3 4b thinking</code>), which are specifically fine-tuned for analytical depth—not general-purpose chat. They outperform standard instruct models on logic-heavy, multi-step STEM problems.</p></blockquote><p>Use the selector below to find the best <strong>thinking-tuned</strong> model for your hardware:</p>',4)),s(t,{modelDefinitions:i}),e[1]||(e[1]=n('<blockquote><p><strong>“Not recommended” means unreliable STEM output</strong> If the selector returns “Not recommended,” your system likely lacks the resources to run even the smallest thinking model effectively. In such cases, output quality degrades severely—often worse than manually reasoning through the problem for 5+ minutes.</p></blockquote><hr><h2 id="how-to-use-thinking-tuned-models-for-stem-work" tabindex="-1">How to Use Thinking-Tuned Models for STEM Work <a class="header-anchor" href="#how-to-use-thinking-tuned-models-for-stem-work" aria-label="Permalink to “How to Use Thinking-Tuned Models for STEM Work”">​</a></h2><p>These models are designed for <strong>deep technical reasoning</strong>, not casual conversation. Follow these guidelines to maximize accuracy and performance in scientific, engineering, or mathematical contexts.</p><h3 id="_1-use-only-thinking-tuned-variants" tabindex="-1">1. <strong>Use Only Thinking-Tuned Variants</strong> <a class="header-anchor" href="#_1-use-only-thinking-tuned-variants" aria-label="Permalink to “1. Use Only Thinking-Tuned Variants”">​</a></h3><ul><li>Only models labeled <strong><code>thinking</code></strong> (e.g., <code>qwen3 4b thinking</code>, <code>qwen3 30b thinking</code>) are fine-tuned for analytical tasks like: <ul><li>Solving differential equations</li><li>Deriving physics formulas</li><li>Debugging numerical simulations</li><li>Explaining quantum mechanics or circuit design</li></ul></li><li>Standard <code>instruct</code> or base models often <strong>fail on multi-step logic</strong> or produce plausible but incorrect STEM conclusions.</li></ul><h3 id="_2-understand-quantization-trade-offs" tabindex="-1">2. <strong>Understand Quantization Trade-offs</strong> <a class="header-anchor" href="#_2-understand-quantization-trade-offs" aria-label="Permalink to “2. Understand Quantization Trade-offs”">​</a></h3><table tabindex="0"><thead><tr><th>Quant</th><th>Use Case</th><th>STEM Impact</th></tr></thead><tbody><tr><td><code>bf16</code> / <code>f16</code></td><td>High-precision math, symbolic reasoning</td><td>Best for accuracy in calculus, linear algebra, or physics derivations</td></tr><tr><td><code>q8</code></td><td>Balanced performance</td><td>Suitable for most coding + math tasks (e.g., Python + NumPy workflows)</td></tr><tr><td><code>q6</code> / <code>q4</code></td><td>Low-resource systems</td><td>May skip steps in proofs or mis-evaluate edge cases—use only when necessary</td></tr></tbody></table><h3 id="_3-full-gpu-offload-is-critical" tabindex="-1">3. <strong>Full GPU Offload Is Critical</strong> <a class="header-anchor" href="#_3-full-gpu-offload-is-critical" aria-label="Permalink to “3. Full GPU Offload Is Critical”">​</a></h3><ul><li><strong>Context lives in VRAM</strong>—always aim for <strong>full GPU offload</strong> (e.g., 48/48 layers in LM Studio).</li><li>If LM Studio suggests default offload settings for a model, <strong>keep them</strong>—they&#39;re tuned for stability.</li></ul><h3 id="_4-ram-vram-combo-rule-for-cpu-offload" tabindex="-1">4. <strong>RAM+VRAM Combo Rule (For CPU Offload)</strong> <a class="header-anchor" href="#_4-ram-vram-combo-rule-for-cpu-offload" aria-label="Permalink to “4. RAM+VRAM Combo Rule (For CPU Offload)”">​</a></h3><p>If using partial CPU offload:</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span>Total Free RAM + VRAM ≥ Model Size + 4 GB (context + overhead)</span></span></code></pre></div><ul><li>Example: <code>qwen3-30b-thinking-q8</code> ≈ 36 GB → requires <strong>≥40 GB combined free memory</strong>.</li><li>This setup works but is <strong>not ideal for real-time STEM exploration</strong>—expect latency.</li></ul><h3 id="_5-prompt-with-precision" tabindex="-1">5. <strong>Prompt with Precision</strong> <a class="header-anchor" href="#_5-prompt-with-precision" aria-label="Permalink to “5. Prompt with Precision”">​</a></h3><p>STEM models thrive on <strong>structured, explicit prompts</strong>:</p><ul><li>❌ <em>“How do I solve this?”</em></li><li>✅ <em>“Given a 2D heat equation ∂u/∂t = α∇²u on [0,1]×[0,1] with Dirichlet BCs, derive the finite difference scheme using central differences and Δx=Δy=0.1.”</em></li><li>Include: <ul><li>Known variables &amp; constraints</li><li>Desired output format (e.g., “show all steps,” “return Python code”)</li><li>Relevant domain (e.g., “in classical electromagnetism…”)</li></ul></li></ul><h3 id="_6-avoid-none-configurations" tabindex="-1">6. <strong>Avoid “None” Configurations</strong> <a class="header-anchor" href="#_6-avoid-none-configurations" aria-label="Permalink to “6. Avoid “None” Configurations”">​</a></h3><ul><li>If your hardware yields <strong>“none”</strong>, do <strong>not</strong> force a model load via heavy CPU offload.</li><li>You&#39;ll get <strong>hallucinated derivations</strong>, incorrect unit conversions, or broken logic—worse than no model at all.</li><li>Alternatives: <ul><li>Use cloud inference (e.g., Together.ai with <code>qwen3-30b-thinking</code>)</li><li>Upgrade to ≥8 GB VRAM for basic STEM tasks</li></ul></li></ul><h3 id="_7-monitor-real-time-usage" tabindex="-1">7. <strong>Monitor Real-Time Usage</strong> <a class="header-anchor" href="#_7-monitor-real-time-usage" aria-label="Permalink to “7. Monitor Real-Time Usage”">​</a></h3><ul><li><strong>Windows</strong>: Task Manager → Performance → GPU &amp; Memory</li><li><strong>Linux</strong>: <code>nvidia-smi</code> (NVIDIA) or <code>radeontop</code> (AMD)</li><li>If VRAM &gt;90% usage, reduce context length or switch to a lower quant (e.g., <code>bf16</code> → <code>q8</code>).</li></ul><hr><p>By aligning your hardware capabilities, quantization choice, and prompt engineering with these principles, you&#39;ll unlock <strong>reliable, step-by-step technical reasoning</strong>—whether you&#39;re proving theorems, simulating systems, or designing experiments.</p>',23))]))}});export{u as __pageData,g as default};
