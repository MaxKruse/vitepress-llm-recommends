import{M as s}from"./chunks/ModelSelector.DQGW46EL.js";import{c as n,o,j as t,J as i,af as l,a}from"./chunks/framework.CuQK53I8.js";const p=JSON.parse('{"title":"Instruct Models","description":"","frontmatter":{"title":"Instruct Models"},"headers":[],"relativePath":"recommendations/instruct/index.md","filePath":"recommendations/instruct/index.md"}'),u={name:"recommendations/instruct/index.md"},f=Object.assign(u,{setup(d){const r=[{ramMin:128,vramMin:32,models:[{"Mistral Small 3.2":{parameters:24,quantization:"Q8_K_XL"}},{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"BF16"}}],usefulness:1},{ramMin:128,vramMin:24,models:[{"Mistral Small 3.2":{parameters:24,quantization:"Q6_K_XL"}},{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"BF16"}}],usefulness:.9},{ramMin:128,vramMin:0,models:[{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"BF16"}}],usefulness:.8},{ramMin:64,vramMin:32,models:[{"Mistral Small 3.2":{parameters:24,quantization:"Q8_K_XL"}},{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"BF16"}}],usefulness:.9},{ramMin:64,vramMin:24,models:[{"Mistral Small 3.2":{parameters:24,quantization:"Q6_K_XL"}},{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.8},{ramMin:64,vramMin:16,models:[{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.7},{ramMin:64,vramMin:12,models:[{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.6},{ramMin:64,vramMin:8,models:[{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.5},{ramMin:64,vramMin:6,models:[{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.4},{ramMin:64,vramMin:4,models:[{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.3},{ramMin:64,vramMin:0,models:[{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.2},{ramMin:32,vramMin:32,models:[{"Mistral Small 3.2":{parameters:24,quantization:"Q8_K_XL"}}],usefulness:.8},{ramMin:32,vramMin:24,models:[{"Mistral Small 3.2":{parameters:24,quantization:"Q6_K_XL"}},{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.7},{ramMin:32,vramMin:16,models:[{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q8_K_XL"}}],usefulness:.6},{ramMin:32,vramMin:8,models:[{"Qwen3 30B Instruct 2507":{parameters:30,quantization:"Q6_K_XL"}}],usefulness:.4},{ramMin:32,vramMin:4,models:[{"Qwen3 4B Instruct 2507":{parameters:4,quantization:"BF16"}}],usefulness:.3},{ramMin:16,vramMin:32,models:[{"Mistral Small 3.2":{parameters:24,quantization:"Q8_K_XL"}}],usefulness:.6},{ramMin:16,vramMin:24,models:[{"Mistral Small 3.2":{parameters:24,quantization:"Q6_K_XL"}}],usefulness:.5},{ramMin:16,vramMin:12,models:[{"Qwen3 4B Instruct 2507":{parameters:4,quantization:"BF16"}}],usefulness:.3},{ramMin:16,vramMin:4,models:[{"Qwen3 4B Instruct 2507":{parameters:4,quantization:"Q4_K_XL"}}],usefulness:.2}];return(m,e)=>(o(),n("div",null,[e[0]||(e[0]=t("h1",{id:"instruct-tuned-models",tabindex:"-1"},[a("Instruct-Tuned Models "),t("a",{class:"header-anchor",href:"#instruct-tuned-models","aria-label":"Permalink to “Instruct-Tuned Models”"},"​")],-1)),e[1]||(e[1]=t("p",null,"Instruct-tuned models are AI assistants trained specifically to understand and follow your directions—whether you're asking for help writing an email, summarizing a document, planning a trip, or explaining a complex idea. Unlike raw base models, these are fine-tuned to respond helpfully, clearly, and on-topic.",-1)),e[2]||(e[2]=t("p",null,[a("Use the selector below to find the best "),t("strong",null,"instruct-tuned"),a(" model that matches your computer’s capabilities:")],-1)),i(s,{modelDefinitions:r}),e[3]||(e[3]=l('<h2 id="how-to-use-instruct-models-effectively" tabindex="-1">How to Use Instruct Models Effectively <a class="header-anchor" href="#how-to-use-instruct-models-effectively" aria-label="Permalink to “How to Use Instruct Models Effectively”">​</a></h2><p>These models shine when given clear, thoughtful instructions. Follow these tips to get the most helpful, accurate, and reliable responses—whether you&#39;re drafting messages, researching topics, or organizing your day.</p><h3 id="_1-always-choose-an-instruct-model" tabindex="-1">1. <strong>Always Choose an “Instruct” Model</strong> <a class="header-anchor" href="#_1-always-choose-an-instruct-model" aria-label="Permalink to “1. Always Choose an “Instruct” Model”">​</a></h3><ul><li>Look for <strong><code>instruct</code></strong> in the model name (e.g., <code>qwen3 30b instruct</code>, <code>mistral small</code>). These are specially trained to follow directions.</li><li>Non-instruct models may ignore your request or give generic, off-topic replies.</li></ul><h3 id="_2-pick-the-right-version-for-your-hardware" tabindex="-1">2. <strong>Pick the Right Version for Your Hardware</strong> <a class="header-anchor" href="#_2-pick-the-right-version-for-your-hardware" aria-label="Permalink to “2. Pick the Right Version for Your Hardware”">​</a></h3><table tabindex="0"><thead><tr><th>Version</th><th>Best For</th><th>Notes</th></tr></thead><tbody><tr><td><code>bf16</code> / <code>f16</code></td><td>Best quality, powerful GPUs</td><td>Highest accuracy; needs lots of VRAM</td></tr><tr><td><code>q6</code> / <code>q8</code></td><td>Great balance of speed and clarity</td><td>Works well on most modern laptops or desktops</td></tr><tr><td><code>q4</code></td><td>Limited VRAM (e.g., older or entry-level GPUs)</td><td>Faster but may miss finer details or nuance</td></tr></tbody></table><h3 id="_3-use-your-gpu-fully-if-available" tabindex="-1">3. <strong>Use Your GPU Fully (If Available)</strong> <a class="header-anchor" href="#_3-use-your-gpu-fully-if-available" aria-label="Permalink to “3. Use Your GPU Fully (If Available)”">​</a></h3><ul><li>In apps like <strong>LM Studio</strong>, enable full GPU offloading to keep things fast and smooth.</li><li>Example: For <code>qwen3 30b instruct</code>, try loading <strong>all layers</strong> onto the GPU if you have enough VRAM.</li><li>Partial use of the GPU can cause slowdowns as the system swaps data between memory and graphics card.</li></ul><h3 id="_4-mind-the-context-length" tabindex="-1">4. <strong>Mind the Context Length</strong> <a class="header-anchor" href="#_4-mind-the-context-length" aria-label="Permalink to “4. Mind the Context Length”">​</a></h3><ul><li>All recommended models support <strong>16,000+ tokens</strong> of context—enough for long emails, reports, or multi-turn conversations.</li><li>Very long inputs use more memory; if your system feels sluggish, shorten your prompt or reduce the model’s quantization.</li></ul><h3 id="_5-be-clear-and-specific-in-your-requests" tabindex="-1">5. <strong>Be Clear and Specific in Your Requests</strong> <a class="header-anchor" href="#_5-be-clear-and-specific-in-your-requests" aria-label="Permalink to “5. Be Clear and Specific in Your Requests”">​</a></h3><ul><li>❌ <em>“Tell me about climate change.”</em></li><li>✅ <em>“Explain the main causes of climate change in simple terms, as if I’m 15 years old.”</em></li><li>The more precise your instruction—goal, audience, format, or examples—the better the response.</li></ul><h3 id="_6-avoid-not-recommended-setups" tabindex="-1">6. <strong>Avoid “Not Recommended” Setups</strong> <a class="header-anchor" href="#_6-avoid-not-recommended-setups" aria-label="Permalink to “6. Avoid “Not Recommended” Setups”">​</a></h3><ul><li>If the tool says <strong>“Not recommended,”</strong> your device likely can’t run even the smallest instruct model well.</li><li>Trying to force it may result in <strong>slow replies, crashes, or unreliable answers</strong>.</li><li>In that case, consider using a cloud-based service (like OpenRouter or Together.ai) or upgrading your hardware.</li></ul><h3 id="_7-watch-your-system-resources" tabindex="-1">7. <strong>Watch Your System Resources</strong> <a class="header-anchor" href="#_7-watch-your-system-resources" aria-label="Permalink to “7. Watch Your System Resources”">​</a></h3><ul><li><strong>Windows</strong>: Open Task Manager → Performance → GPU</li><li><strong>Linux</strong>: Use <code>nvidia-smi</code> (NVIDIA) or <code>radeontop</code> (AMD)</li><li>If VRAM usage goes above <strong>90%</strong>, try a lighter model version (e.g., switch from <code>q8</code> to <code>q6</code>) or shorten your input.</li></ul><p>By matching the right model to your hardware and giving clear, purposeful instructions, you’ll enjoy a smooth, intelligent assistant experience—perfect for everyday tasks, learning, planning, and more.</p>',17))]))}});export{p as __pageData,f as default};
