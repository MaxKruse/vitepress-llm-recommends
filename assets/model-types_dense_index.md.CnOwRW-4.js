import{_ as t,c as r,o as a,af as o}from"./chunks/framework.t7omRjYa.js";const c=JSON.parse('{"title":"Dense Models","description":"","frontmatter":{"title":"Dense Models"},"headers":[],"relativePath":"model-types/dense/index.md","filePath":"model-types/dense/index.md"}'),n={name:"model-types/dense/index.md"};function s(i,e,l,d,m,h){return a(),r("div",null,[...e[0]||(e[0]=[o('<h1 id="dense-models" tabindex="-1">Dense Models <a class="header-anchor" href="#dense-models" aria-label="Permalink to “Dense Models”">​</a></h1><p>Dense models activate <strong>all parameters</strong> for every input. They are simpler to train, but have lower inference throughput (Tokens/s) than Mixture-of-Experts (MoE) models of a similar <em>total</em> parameter count (due to the Mixture-of-Experts model only activating a fraction of the parameters per token).</p><h2 id="how-dense-models-work" tabindex="-1">How Dense Models Work <a class="header-anchor" href="#how-dense-models-work" aria-label="Permalink to “How Dense Models Work”">​</a></h2><p>In a dense Large Language Model (LLM), every layer processes the entire input using <strong>all of its parameters</strong>, regardless of the specific tokens or context. This means:</p><ul><li><strong>Full parameter utilization</strong>: Every weight in the model contributes to the output for any given input.</li><li><strong>Straightforward architecture</strong>: Dense models follow standard transformer architectures without dynamic routing or conditional computation.</li><li><strong>Easier to train</strong>: Due to their simpler architecture, dense models are generally more straightforward to train (although training any large model is still extremely difficult).</li></ul><h2 id="tradeoffs" tabindex="-1">Tradeoffs <a class="header-anchor" href="#tradeoffs" aria-label="Permalink to “Tradeoffs”">​</a></h2><p>Given that a dense model has lower inference throughput than an MoE model of the same <em>total</em> parameter count, one might question why one would even use them.</p><p>One part of this answer is the relative <strong>ease of training and iteration</strong>. The simpler architecture is easier to debug and allows for faster experimentation with changes, like new activation functions.</p><p>Another, more important difference, is <strong>parameter efficiency</strong>. In an MoE, different experts (which are themselves small dense models) might learn to store similar information, leading to potential <em>redundancy</em>. In a dense model, all parameters are interconnected and used for every input. This means a 24B dense model may store knowledge with less redundancy than a 24B MoE model.</p><p>Therefore, if <strong>knowledge density</strong> (the amount of unique information stored per parameter) is the main priority, a dense model can be more parameter-efficient, though this comes at the significant cost of slower inference.</p>',10)])])}const p=t(n,[["render",s]]);export{c as __pageData,p as default};
