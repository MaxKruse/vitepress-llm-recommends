import{_ as I,x as f,h as d,c as s,o as l,af as b,j as e,ag as y,ah as _,G as w,H as P,t as v,O as T,n as x}from"./chunks/framework.t7omRjYa.js";const q={class:"controls"},R={class:"control-group"},B=["value"],z={class:"control-group"},V=["value"],Q={class:"result"},G=JSON.parse('{"title":"Coding","description":"","frontmatter":{"title":"Coding"},"headers":[],"relativePath":"recommendations/coding/index.md","filePath":"recommendations/coding/index.md"}'),U={name:"recommendations/coding/index.md"},N=Object.assign(U,{setup(W){const n=f(16),i=f(8),M=[16,32,64,128],k=[0,4,6,8,12,16,24,32],A=[{ramMin:64,vramMin:16,model:"Qwen3 Coder 30B A3B Instruct bf16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:32,vramMin:16,model:"Qwen3 Coder 30B A3B Instruct Q8",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:32,vramMin:6,model:"Qwen3 Coder 30B A3B Instruct Q6",color:"var(--vp-c-yellow-2)",bg:"var(--vp-c-yellow-soft)"},{ramMin:16,vramMin:4,model:"Qwen3 4B Instruct 2507 BF16",color:"var(--vp-c-purple-2)",bg:"var(--vp-c-purple-soft)"}],o=d(()=>{const r=A.find(a=>n.value>=a.ramMin&&i.value>=a.vramMin);return r?{model:r.model,color:r.color,bg:r.bg}:{model:"Not recommended",color:"var(--vp-c-text-3)",bg:"transparent"}}),u=d(()=>o.value.model!=="Not recommended"),c=d(()=>o.value.model.toLowerCase()),m=d(()=>c.value.includes("bf16")||c.value.includes("gpt oss")),g=d(()=>c.value.includes("q6")||c.value.includes("q8")),h=d(()=>c.value.includes("q4")),p=d(()=>c.value.includes("4b")),C=d(()=>u.value?p.value?{"recommended-4b":!0}:m.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:h.value?{"recommended-warning":!0}:{}:{"not-recommended":!0}),S=d(()=>u.value?p.value?{"recommended-4b":!0}:m.value?{"recommended-success":!0}:g.value?{"recommended-caution":!0}:h.value?{"recommended-warning":!0}:{}:{"not-recommended":!0});return(r,a)=>(l(),s("div",null,[a[5]||(a[5]=b('<h1 id="coding-recommendations" tabindex="-1" data-v-4c25dca6>Coding Recommendations <a class="header-anchor" href="#coding-recommendations" aria-label="Permalink to “Coding Recommendations”" data-v-4c25dca6>​</a></h1><p data-v-4c25dca6><strong data-v-4c25dca6>Prioritizing Very High Precision in Code Generation, Completion, and Debugging</strong></p><p data-v-4c25dca6>When working with AI-assisted coding, <strong data-v-4c25dca6>precision is non-negotiable</strong>. Even minor hallucinations, syntactic errors, or incorrect logic can introduce bugs, security vulnerabilities, or maintenance debt. These recommendations are tailored for developers who require <strong data-v-4c25dca6>extremely high-fidelity outputs</strong>—not just plausible-looking code, but <strong data-v-4c25dca6>correct, production-ready, and logically sound</strong> implementations.</p><p data-v-4c25dca6>Use the selector below to identify the best model <strong data-v-4c25dca6>that balances your hardware constraints with the highest achievable precision</strong>:</p>',4)),e("div",{class:x(["model-selector",C.value])},[e("div",q,[e("div",R,[a[2]||(a[2]=e("label",{for:"ram-select"},"RAM (GB)",-1)),y(e("select",{id:"ram-select","onUpdate:modelValue":a[0]||(a[0]=t=>n.value=t)},[(l(),s(w,null,P(M,t=>e("option",{key:t,value:t},v(t),9,B)),64))],512),[[_,n.value,void 0,{number:!0}]])]),e("div",z,[a[3]||(a[3]=e("label",{for:"vram-select"},"VRAM (GB)",-1)),y(e("select",{id:"vram-select","onUpdate:modelValue":a[1]||(a[1]=t=>i.value=t)},[(l(),s(w,null,P(k,t=>e("option",{key:t,value:t},v(t),9,V)),64))],512),[[_,i.value,void 0,{number:!0}]])])]),e("div",Q,[a[4]||(a[4]=e("strong",null,"Recommended model:",-1)),e("span",{class:x(["model-name",S.value]),style:T({backgroundColor:o.value.bg,color:o.value.color})},v(o.value.model),7)])],2),a[6]||(a[6]=b('<h2 id="why-precision-matters-in-coding" tabindex="-1" data-v-4c25dca6>Why Precision Matters in Coding <a class="header-anchor" href="#why-precision-matters-in-coding" aria-label="Permalink to “Why Precision Matters in Coding”" data-v-4c25dca6>​</a></h2><p data-v-4c25dca6>Unlike creative or conversational tasks, <strong data-v-4c25dca6>code must be functionally correct</strong>. A model that “sounds right” but produces broken logic, incorrect APIs, or unsafe patterns is worse than no model at all. Therefore, <strong data-v-4c25dca6>favor models and configurations that maximize precision—even at the cost of speed or resource usage</strong>.</p><h3 id="_1-use-full-gpu-offload-when-possible" tabindex="-1" data-v-4c25dca6>1. <strong data-v-4c25dca6>Use Full GPU Offload (When Possible)</strong> <a class="header-anchor" href="#_1-use-full-gpu-offload-when-possible" aria-label="Permalink to “1. Use Full GPU Offload (When Possible)”" data-v-4c25dca6>​</a></h3><ul data-v-4c25dca6><li data-v-4c25dca6>In <strong data-v-4c25dca6>LM Studio</strong>, always enable <strong data-v-4c25dca6>full GPU offload</strong> (e.g., <code data-v-4c25dca6>48/48</code> layers for Qwen3 models).</li></ul><h3 id="_2-prefer-instruct-tuned-code-specialized-models" tabindex="-1" data-v-4c25dca6>2. <strong data-v-4c25dca6>Prefer Instruct-Tuned, Code-Specialized Models</strong> <a class="header-anchor" href="#_2-prefer-instruct-tuned-code-specialized-models" aria-label="Permalink to “2. Prefer Instruct-Tuned, Code-Specialized Models”" data-v-4c25dca6>​</a></h3><ul data-v-4c25dca6><li data-v-4c25dca6>Only use <strong data-v-4c25dca6>code-instruct variants</strong> like <code data-v-4c25dca6>qwen3-coder-30b-instruct</code>. These are fine-tuned on millions of correct code examples and aligned for <strong data-v-4c25dca6>semantic and syntactic accuracy</strong>.</li><li data-v-4c25dca6>Avoid base models or “thinking” variants—they lack the precision tuning needed for reliable code output.</li></ul><h3 id="_3-context-window-memory-stability" tabindex="-1" data-v-4c25dca6>3. <strong data-v-4c25dca6>Context Window &amp; Memory Stability</strong> <a class="header-anchor" href="#_3-context-window-memory-stability" aria-label="Permalink to “3. Context Window &amp; Memory Stability”" data-v-4c25dca6>​</a></h3><ul data-v-4c25dca6><li data-v-4c25dca6>All recommendations assume a <strong data-v-4c25dca6>stable ~16K context window</strong>.</li></ul><h3 id="_4-quantization-precision-vs-practicality" tabindex="-1" data-v-4c25dca6>4. <strong data-v-4c25dca6>Quantization: Precision vs. Practicality</strong> <a class="header-anchor" href="#_4-quantization-precision-vs-practicality" aria-label="Permalink to “4. Quantization: Precision vs. Practicality”" data-v-4c25dca6>​</a></h3><table tabindex="0" data-v-4c25dca6><thead data-v-4c25dca6><tr data-v-4c25dca6><th data-v-4c25dca6>Quant</th><th data-v-4c25dca6>Speed</th><th data-v-4c25dca6><strong data-v-4c25dca6>Precision</strong></th><th data-v-4c25dca6>VRAM Use</th></tr></thead><tbody data-v-4c25dca6><tr data-v-4c25dca6><td data-v-4c25dca6><code data-v-4c25dca6>bf16</code> / <code data-v-4c25dca6>f16</code></td><td data-v-4c25dca6>★☆☆☆☆</td><td data-v-4c25dca6>★★★★★ (<strong data-v-4c25dca6>Highest</strong>)</td><td data-v-4c25dca6>Highest</td></tr><tr data-v-4c25dca6><td data-v-4c25dca6><code data-v-4c25dca6>q8</code></td><td data-v-4c25dca6>★★★☆☆</td><td data-v-4c25dca6>★★★★☆ (<strong data-v-4c25dca6>High</strong>)</td><td data-v-4c25dca6>Medium-High</td></tr><tr data-v-4c25dca6><td data-v-4c25dca6><code data-v-4c25dca6>q6</code></td><td data-v-4c25dca6>★★★★☆</td><td data-v-4c25dca6>★★★☆☆ (<strong data-v-4c25dca6>Moderate</strong>)</td><td data-v-4c25dca6>Medium</td></tr><tr data-v-4c25dca6><td data-v-4c25dca6><code data-v-4c25dca6>q4</code></td><td data-v-4c25dca6>★★★★★</td><td data-v-4c25dca6>★☆☆☆☆ (<strong data-v-4c25dca6>Low - Not Recommended for Precision Work</strong>)</td><td data-v-4c25dca6>Lowest</td></tr></tbody></table><blockquote data-v-4c25dca6><p data-v-4c25dca6>💡 <strong data-v-4c25dca6>For high-stakes coding (e.g., production systems, security-sensitive logic, or complex algorithms), always prefer <code data-v-4c25dca6>bf16</code>, <code data-v-4c25dca6>f16</code>, or at minimum <code data-v-4c25dca6>q8</code>.</strong> Avoid <code data-v-4c25dca6>q4</code> unless absolutely necessary—it sacrifices too much precision.</p></blockquote><h3 id="_5-prompt-engineering-for-correctness" tabindex="-1" data-v-4c25dca6>5. <strong data-v-4c25dca6>Prompt Engineering for Correctness</strong> <a class="header-anchor" href="#_5-prompt-engineering-for-correctness" aria-label="Permalink to “5. Prompt Engineering for Correctness”" data-v-4c25dca6>​</a></h3><ul data-v-4c25dca6><li data-v-4c25dca6><strong data-v-4c25dca6>Be explicit and constrained</strong>:<br data-v-4c25dca6> ❌ <em data-v-4c25dca6>“Write a login function.”</em><br data-v-4c25dca6> ✅ <em data-v-4c25dca6>“Write a secure Python FastAPI login endpoint using OAuth2 password flow, with bcrypt hashing, rate limiting, and proper error responses.”</em></li><li data-v-4c25dca6><strong data-v-4c25dca6>Request verification steps</strong>: Ask the model to “explain why this implementation is safe” or “list potential edge cases.”</li><li data-v-4c25dca6><strong data-v-4c25dca6>Include error context</strong>: Paste stack traces or test failures—this helps the model reason precisely about the failure mode.</li></ul><h3 id="_6-avoid-none-or-underpowered-setups" tabindex="-1" data-v-4c25dca6>6. <strong data-v-4c25dca6>Avoid “None” or Underpowered Setups</strong> <a class="header-anchor" href="#_6-avoid-none-or-underpowered-setups" aria-label="Permalink to “6. Avoid “None” or Underpowered Setups”" data-v-4c25dca6>​</a></h3><ul data-v-4c25dca6><li data-v-4c25dca6>If the matrix returns <strong data-v-4c25dca6>“none”</strong>, <strong data-v-4c25dca6>do not force execution</strong> via heavy RAM offloading.</li><li data-v-4c25dca6>Under-resourced inference <strong data-v-4c25dca6>increases hallucination rates</strong> and reduces logical consistency—<strong data-v-4c25dca6>unacceptable for precision-critical workflows</strong>.</li><li data-v-4c25dca6>Consider cloud inference (e.g., with <code data-v-4c25dca6>bf16</code> models) if local hardware is insufficient.</li></ul><h3 id="_7-validate-and-monitor" tabindex="-1" data-v-4c25dca6>7. <strong data-v-4c25dca6>Validate and Monitor</strong> <a class="header-anchor" href="#_7-validate-and-monitor" aria-label="Permalink to “7. Validate and Monitor”" data-v-4c25dca6>​</a></h3><ul data-v-4c25dca6><li data-v-4c25dca6><strong data-v-4c25dca6>Never trust output blindly</strong>. Always: <ul data-v-4c25dca6><li data-v-4c25dca6>Run static analysis (e.g., <code data-v-4c25dca6>mypy</code>, <code data-v-4c25dca6>eslint</code>, <code data-v-4c25dca6>bandit</code>)</li><li data-v-4c25dca6>Execute unit tests</li><li data-v-4c25dca6>Review for logic correctness</li></ul></li><li data-v-4c25dca6>Monitor VRAM usage: sustained &gt;95% utilization can cause <strong data-v-4c25dca6>numerical errors</strong> that silently degrade output quality.</li></ul><hr data-v-4c25dca6><p data-v-4c25dca6>By aligning your tooling with the <strong data-v-4c25dca6>highest-precision models your hardware can support</strong>, you ensure that AI assistance enhances—rather than undermines—code quality, security, and maintainability. <strong data-v-4c25dca6>When correctness is paramount, precision isn&#39;t optional—it&#39;s essential.</strong></p>',19))]))}}),E=I(N,[["__scopeId","data-v-4c25dca6"]]);export{G as __pageData,E as default};
