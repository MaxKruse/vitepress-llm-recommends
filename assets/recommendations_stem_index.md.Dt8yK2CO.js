import{_ as P,x as f,h as t,c as s,o as v,af as b,j as a,ag as M,ah as k,G as T,H as w,t as c,O as q,n as B}from"./chunks/framework.t7omRjYa.js";const F={class:"controls"},C={class:"control-group"},A=["value"],R={class:"control-group"},U=["value"],E={class:"result"},O=JSON.parse('{"title":"STEM","description":"","frontmatter":{"title":"STEM"},"headers":[],"relativePath":"recommendations/stem/index.md","filePath":"recommendations/stem/index.md"}'),I={name:"recommendations/stem/index.md"},V=Object.assign(I,{setup(N){const i=f(16),l=f(8),y=[16,32,64,128],Q=[0,4,6,8,12,16,24,32],_=[{ramMin:128,vramMin:32,model:"Qwen3-30B-Thinking BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:24,model:"Qwen3-30B-Thinking BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:16,model:"Qwen3-30B-Thinking BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:12,model:"Qwen3-30B-Thinking BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:8,model:"Qwen3-30B-Thinking BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:6,model:"Qwen3-30B-Thinking BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:4,model:"Qwen3-30B-Thinking BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:128,vramMin:0,model:"Qwen3-30B-Thinking BF16",color:"var(--vp-c-green-2)",bg:"var(--vp-c-green-soft)"},{ramMin:64,vramMin:32,model:"Qwen3-30B-Thinking BF16",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:24,model:"Qwen3-30B-Thinking Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:16,model:"Qwen3-30B-Thinking Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:12,model:"Qwen3-30B-Thinking Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:8,model:"Qwen3-30B-Thinking Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:6,model:"Qwen3-30B-Thinking Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:4,model:"Qwen3-30B-Thinking Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:64,vramMin:0,model:"Qwen3-30B-Thinking Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:32,model:"Qwen3-4B-Thinking BF16 or Qwen3-30B-Thinking Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:24,model:"Qwen3-4B-Thinking BF16",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:16,model:"Qwen3-4B-Thinking BF16",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:12,model:"Qwen3-4B-Thinking BF16",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:8,model:"Qwen3-4B-Thinking Q8",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:6,model:"Qwen3-4B-Thinking Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:4,model:"Qwen3-4B-Thinking Q4",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:32,vramMin:0,model:"Qwen3-4B-Thinking Q6",color:"var(--vp-c-blue-2)",bg:"var(--vp-c-blue-soft)"},{ramMin:16,vramMin:32,model:"Qwen3-4B-Thinking BF16 or Qwen3-30B-Thinking Q6",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:24,model:"Qwen3-4B-Thinking BF16",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:16,model:"Qwen3-4B-Thinking BF16",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:12,model:"Qwen3-4B-Thinking BF16",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:8,model:"Qwen3-4B-Thinking Q8",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:6,model:"Qwen3-4B-Thinking Q6",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"},{ramMin:16,vramMin:4,model:"Qwen3-4B-Thinking Q4",color:"var(--vp-c-orange-2)",bg:"var(--vp-c-orange-soft)"}],r=t(()=>{const d=_.find(e=>i.value>=e.ramMin&&l.value>=e.vramMin);return d?{model:d.model,color:d.color,bg:d.bg}:{model:"Not recommended",color:"var(--vp-c-text-3)",bg:"transparent"}}),m=t(()=>r.value.model!=="Not recommended"),o=t(()=>r.value.model.toLowerCase()),g=t(()=>o.value.includes("bf16")||o.value.includes("gpt oss")),u=t(()=>o.value.includes("q6")||o.value.includes("q8")),h=t(()=>o.value.includes("q4")),p=t(()=>o.value.includes("4b")),x=t(()=>m.value?p.value?{"recommended-4b":!0}:g.value?{"recommended-success":!0}:u.value?{"recommended-caution":!0}:h.value?{"recommended-warning":!0}:{}:{"not-recommended":!0}),S=t(()=>m.value?p.value?{"recommended-4b":!0}:g.value?{"recommended-success":!0}:u.value?{"recommended-caution":!0}:h.value?{"recommended-warning":!0}:{}:{"not-recommended":!0});return(d,e)=>(v(),s("div",null,[e[5]||(e[5]=b('<h1 id="stem-science-technology-engineering-mathematics" tabindex="-1" data-v-e289de89>STEM (Science, Technology, Engineering, Mathematics) <a class="header-anchor" href="#stem-science-technology-engineering-mathematics" aria-label="Permalink to ‚ÄúSTEM (Science, Technology, Engineering, Mathematics)‚Äù" data-v-e289de89>‚Äã</a></h1><p data-v-e289de89><strong data-v-e289de89>Thinking-tuned models</strong> optimized for technical reasoning, mathematical precision, scientific problem-solving, and code-heavy workflows. These models excel at tasks like deriving equations, debugging complex systems, simulating experiments, and explaining STEM concepts with rigor.</p><blockquote data-v-e289de89><p data-v-e289de89>üí° <strong data-v-e289de89>Note</strong>: All models listed below are <em data-v-e289de89>thinking-tuned variants</em> (e.g., <code data-v-e289de89>qwen3 4b thinking</code>), which are specifically fine-tuned for analytical depth‚Äînot general-purpose chat. They outperform standard instruct models on logic-heavy, multi-step STEM problems.</p></blockquote><p data-v-e289de89>Use the selector below to find the best <strong data-v-e289de89>thinking-tuned</strong> model for your hardware:</p>',4)),a("div",{class:B(["model-selector",x.value])},[a("div",F,[a("div",C,[e[2]||(e[2]=a("label",{for:"ram-select"},"RAM (GB)",-1)),M(a("select",{id:"ram-select","onUpdate:modelValue":e[0]||(e[0]=n=>i.value=n)},[(v(),s(T,null,w(y,n=>a("option",{key:n,value:n},c(n),9,A)),64))],512),[[k,i.value,void 0,{number:!0}]])]),a("div",R,[e[3]||(e[3]=a("label",{for:"vram-select"},"VRAM (GB)",-1)),M(a("select",{id:"vram-select","onUpdate:modelValue":e[1]||(e[1]=n=>l.value=n)},[(v(),s(T,null,w(Q,n=>a("option",{key:n,value:n},c(n),9,U)),64))],512),[[k,l.value,void 0,{number:!0}]])])]),a("div",E,[e[4]||(e[4]=a("strong",null,"Recommended model:",-1)),a("span",{class:B(["model-name",S.value]),style:q({backgroundColor:r.value.bg,color:r.value.color})},c(r.value.model),7)])],2),e[6]||(e[6]=b('<blockquote data-v-e289de89><p data-v-e289de89><strong data-v-e289de89>‚ÄúNot recommended‚Äù means unreliable STEM output</strong> If the selector returns ‚ÄúNot recommended,‚Äù your system likely lacks the resources to run even the smallest thinking model effectively. In such cases, output quality degrades severely‚Äîoften worse than manually reasoning through the problem for 5+ minutes.</p></blockquote><hr data-v-e289de89><h2 id="how-to-use-thinking-tuned-models-for-stem-work" tabindex="-1" data-v-e289de89>How to Use Thinking-Tuned Models for STEM Work <a class="header-anchor" href="#how-to-use-thinking-tuned-models-for-stem-work" aria-label="Permalink to ‚ÄúHow to Use Thinking-Tuned Models for STEM Work‚Äù" data-v-e289de89>‚Äã</a></h2><p data-v-e289de89>These models are designed for <strong data-v-e289de89>deep technical reasoning</strong>, not casual conversation. Follow these guidelines to maximize accuracy and performance in scientific, engineering, or mathematical contexts.</p><h3 id="_1-use-only-thinking-tuned-variants" tabindex="-1" data-v-e289de89>1. <strong data-v-e289de89>Use Only Thinking-Tuned Variants</strong> <a class="header-anchor" href="#_1-use-only-thinking-tuned-variants" aria-label="Permalink to ‚Äú1. Use Only Thinking-Tuned Variants‚Äù" data-v-e289de89>‚Äã</a></h3><ul data-v-e289de89><li data-v-e289de89>Only models labeled <strong data-v-e289de89><code data-v-e289de89>thinking</code></strong> (e.g., <code data-v-e289de89>qwen3 4b thinking</code>, <code data-v-e289de89>qwen3 30b thinking</code>) are fine-tuned for analytical tasks like: <ul data-v-e289de89><li data-v-e289de89>Solving differential equations</li><li data-v-e289de89>Deriving physics formulas</li><li data-v-e289de89>Debugging numerical simulations</li><li data-v-e289de89>Explaining quantum mechanics or circuit design</li></ul></li><li data-v-e289de89>Standard <code data-v-e289de89>instruct</code> or base models often <strong data-v-e289de89>fail on multi-step logic</strong> or produce plausible but incorrect STEM conclusions.</li></ul><h3 id="_2-understand-quantization-trade-offs" tabindex="-1" data-v-e289de89>2. <strong data-v-e289de89>Understand Quantization Trade-offs</strong> <a class="header-anchor" href="#_2-understand-quantization-trade-offs" aria-label="Permalink to ‚Äú2. Understand Quantization Trade-offs‚Äù" data-v-e289de89>‚Äã</a></h3><table tabindex="0" data-v-e289de89><thead data-v-e289de89><tr data-v-e289de89><th data-v-e289de89>Quant</th><th data-v-e289de89>Use Case</th><th data-v-e289de89>STEM Impact</th></tr></thead><tbody data-v-e289de89><tr data-v-e289de89><td data-v-e289de89><code data-v-e289de89>bf16</code> / <code data-v-e289de89>f16</code></td><td data-v-e289de89>High-precision math, symbolic reasoning</td><td data-v-e289de89>Best for accuracy in calculus, linear algebra, or physics derivations</td></tr><tr data-v-e289de89><td data-v-e289de89><code data-v-e289de89>q8</code></td><td data-v-e289de89>Balanced performance</td><td data-v-e289de89>Suitable for most coding + math tasks (e.g., Python + NumPy workflows)</td></tr><tr data-v-e289de89><td data-v-e289de89><code data-v-e289de89>q6</code> / <code data-v-e289de89>q4</code></td><td data-v-e289de89>Low-resource systems</td><td data-v-e289de89>May skip steps in proofs or mis-evaluate edge cases‚Äîuse only when necessary</td></tr></tbody></table><h3 id="_3-full-gpu-offload-is-critical" tabindex="-1" data-v-e289de89>3. <strong data-v-e289de89>Full GPU Offload Is Critical</strong> <a class="header-anchor" href="#_3-full-gpu-offload-is-critical" aria-label="Permalink to ‚Äú3. Full GPU Offload Is Critical‚Äù" data-v-e289de89>‚Äã</a></h3><ul data-v-e289de89><li data-v-e289de89><strong data-v-e289de89>Context lives in VRAM</strong>‚Äîalways aim for <strong data-v-e289de89>full GPU offload</strong> (e.g., 48/48 layers in LM Studio).</li><li data-v-e289de89>If LM Studio suggests default offload settings for a model, <strong data-v-e289de89>keep them</strong>‚Äîthey&#39;re tuned for stability.</li></ul><h3 id="_4-ram-vram-combo-rule-for-cpu-offload" tabindex="-1" data-v-e289de89>4. <strong data-v-e289de89>RAM+VRAM Combo Rule (For CPU Offload)</strong> <a class="header-anchor" href="#_4-ram-vram-combo-rule-for-cpu-offload" aria-label="Permalink to ‚Äú4. RAM+VRAM Combo Rule (For CPU Offload)‚Äù" data-v-e289de89>‚Äã</a></h3><p data-v-e289de89>If using partial CPU offload:</p><div class="language-" data-v-e289de89><button title="Copy Code" class="copy" data-v-e289de89></button><span class="lang" data-v-e289de89></span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr" data-v-e289de89><code data-v-e289de89><span class="line" data-v-e289de89><span data-v-e289de89>Total Free RAM + VRAM ‚â• Model Size + 4 GB (context + overhead)</span></span></code></pre></div><ul data-v-e289de89><li data-v-e289de89>Example: <code data-v-e289de89>qwen3-30b-thinking-q8</code> ‚âà 36 GB ‚Üí requires <strong data-v-e289de89>‚â•40 GB combined free memory</strong>.</li><li data-v-e289de89>This setup works but is <strong data-v-e289de89>not ideal for real-time STEM exploration</strong>‚Äîexpect latency.</li></ul><h3 id="_5-prompt-with-precision" tabindex="-1" data-v-e289de89>5. <strong data-v-e289de89>Prompt with Precision</strong> <a class="header-anchor" href="#_5-prompt-with-precision" aria-label="Permalink to ‚Äú5. Prompt with Precision‚Äù" data-v-e289de89>‚Äã</a></h3><p data-v-e289de89>STEM models thrive on <strong data-v-e289de89>structured, explicit prompts</strong>:</p><ul data-v-e289de89><li data-v-e289de89>‚ùå <em data-v-e289de89>‚ÄúHow do I solve this?‚Äù</em></li><li data-v-e289de89>‚úÖ <em data-v-e289de89>‚ÄúGiven a 2D heat equation ‚àÇu/‚àÇt = Œ±‚àá¬≤u on [0,1]√ó[0,1] with Dirichlet BCs, derive the finite difference scheme using central differences and Œîx=Œîy=0.1.‚Äù</em></li><li data-v-e289de89>Include: <ul data-v-e289de89><li data-v-e289de89>Known variables &amp; constraints</li><li data-v-e289de89>Desired output format (e.g., ‚Äúshow all steps,‚Äù ‚Äúreturn Python code‚Äù)</li><li data-v-e289de89>Relevant domain (e.g., ‚Äúin classical electromagnetism‚Ä¶‚Äù)</li></ul></li></ul><h3 id="_6-avoid-none-configurations" tabindex="-1" data-v-e289de89>6. <strong data-v-e289de89>Avoid ‚ÄúNone‚Äù Configurations</strong> <a class="header-anchor" href="#_6-avoid-none-configurations" aria-label="Permalink to ‚Äú6. Avoid ‚ÄúNone‚Äù Configurations‚Äù" data-v-e289de89>‚Äã</a></h3><ul data-v-e289de89><li data-v-e289de89>If your hardware yields <strong data-v-e289de89>‚Äúnone‚Äù</strong>, do <strong data-v-e289de89>not</strong> force a model load via heavy CPU offload.</li><li data-v-e289de89>You&#39;ll get <strong data-v-e289de89>hallucinated derivations</strong>, incorrect unit conversions, or broken logic‚Äîworse than no model at all.</li><li data-v-e289de89>Alternatives: <ul data-v-e289de89><li data-v-e289de89>Use cloud inference (e.g., Together.ai with <code data-v-e289de89>qwen3-30b-thinking</code>)</li><li data-v-e289de89>Upgrade to ‚â•8 GB VRAM for basic STEM tasks</li></ul></li></ul><h3 id="_7-monitor-real-time-usage" tabindex="-1" data-v-e289de89>7. <strong data-v-e289de89>Monitor Real-Time Usage</strong> <a class="header-anchor" href="#_7-monitor-real-time-usage" aria-label="Permalink to ‚Äú7. Monitor Real-Time Usage‚Äù" data-v-e289de89>‚Äã</a></h3><ul data-v-e289de89><li data-v-e289de89><strong data-v-e289de89>Windows</strong>: Task Manager ‚Üí Performance ‚Üí GPU &amp; Memory</li><li data-v-e289de89><strong data-v-e289de89>Linux</strong>: <code data-v-e289de89>nvidia-smi</code> (NVIDIA) or <code data-v-e289de89>radeontop</code> (AMD)</li><li data-v-e289de89>If VRAM &gt;90% usage, reduce context length or switch to a lower quant (e.g., <code data-v-e289de89>bf16</code> ‚Üí <code data-v-e289de89>q8</code>).</li></ul><hr data-v-e289de89><p data-v-e289de89>By aligning your hardware capabilities, quantization choice, and prompt engineering with these principles, you&#39;ll unlock <strong data-v-e289de89>reliable, step-by-step technical reasoning</strong>‚Äîwhether you&#39;re proving theorems, simulating systems, or designing experiments.</p>',23))]))}}),D=P(V,[["__scopeId","data-v-e289de89"]]);export{O as __pageData,D as default};
