<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Personal Assistant | AI Model Guide</title>
    <meta name="description" content="A practical guide to understanding and using modern AI models.">
    <meta name="generator" content="VitePress v2.0.0-alpha.12">
    <link rel="preload stylesheet" href="/vitepress-llm-recommends/assets/style.DQ7kH3JY.css" as="style">
    <link rel="preload stylesheet" href="/vitepress-llm-recommends/vp-icons.css" as="style">
    
    <script type="module" src="/vitepress-llm-recommends/assets/app.IP3h1B2O.js"></script>
    <link rel="preload" href="/vitepress-llm-recommends/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/vitepress-llm-recommends/assets/chunks/theme.BbyOaCFE.js">
    <link rel="modulepreload" href="/vitepress-llm-recommends/assets/chunks/framework.CuQK53I8.js">
    <link rel="modulepreload" href="/vitepress-llm-recommends/assets/chunks/ModelSelector.DQGW46EL.js">
    <link rel="modulepreload" href="/vitepress-llm-recommends/assets/recommendations_personal-assistant_index.md.B6d8-ReY.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-1df9f90f><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-1df9f90f data-v-9f75dce3><div class="VPNavBar" data-v-9f75dce3 data-v-2a96a3d0><div class="wrapper" data-v-2a96a3d0><div class="container" data-v-2a96a3d0><div class="title" data-v-2a96a3d0><div class="VPNavBarTitle has-sidebar" data-v-2a96a3d0 data-v-1e38c6bc><a class="title" href="/vitepress-llm-recommends/" data-v-1e38c6bc><!--[--><!--]--><!----><span data-v-1e38c6bc>AI Model Guide</span><!--[--><!--]--></a></div></div><div class="content" data-v-2a96a3d0><div class="content-body" data-v-2a96a3d0><!--[--><!--]--><div class="VPNavBarSearch search" data-v-2a96a3d0><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-2a96a3d0 data-v-39714824><span id="main-nav-aria-label" class="visually-hidden" data-v-39714824> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/vitepress-llm-recommends/" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/vitepress-llm-recommends/evaluation/" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Evaluation</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/vitepress-llm-recommends/inference/" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Inference</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/vitepress-llm-recommends/model-types/" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Model Types</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/vitepress-llm-recommends/recommendations/" tabindex="0" data-v-39714824 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Recommendations</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-2a96a3d0 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-2a96a3d0 data-v-0394ad82 data-v-d07f11e6><!--[--><a class="VPSocialLink no-icon" href="https://github.com/MaxKruse/vitepress-llm-recommends/" aria-label="github" target="_blank" rel="me noopener" data-v-d07f11e6 data-v-591a6b30><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-2a96a3d0 data-v-bb2aa2f0 data-v-42cb505d><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-42cb505d><span class="vpi-more-horizontal icon" data-v-42cb505d></span></button><div class="menu" data-v-42cb505d><div class="VPMenu" data-v-42cb505d data-v-25a6cce8><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-d07f11e6><!--[--><a class="VPSocialLink no-icon" href="https://github.com/MaxKruse/vitepress-llm-recommends/" aria-label="github" target="_blank" rel="me noopener" data-v-d07f11e6 data-v-591a6b30><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-2a96a3d0 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-2a96a3d0><div class="divider-line" data-v-2a96a3d0></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-1df9f90f data-v-8acdfeb5><div class="container" data-v-8acdfeb5><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-8acdfeb5><span class="vpi-align-left menu-icon" data-v-8acdfeb5></span><span class="menu-text" data-v-8acdfeb5>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-8acdfeb5 data-v-0bf0e06f><button data-v-0bf0e06f>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-1df9f90f data-v-e7c6e512><div class="curtain" data-v-e7c6e512></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-e7c6e512><span class="visually-hidden" id="sidebar-aria-label" data-v-e7c6e512> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-8d50c081><section class="VPSidebarItem level-0" data-v-8d50c081 data-v-d81de50c><div class="item" role="button" tabindex="0" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><h2 class="text" data-v-d81de50c>Introduction</h2><!----></div><div class="items" data-v-d81de50c><!--[--><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Home</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-8d50c081><section class="VPSidebarItem level-0" data-v-8d50c081 data-v-d81de50c><div class="item" role="button" tabindex="0" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><h2 class="text" data-v-d81de50c>Evaluation</h2><!----></div><div class="items" data-v-d81de50c><!--[--><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/evaluation/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Overview</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-8d50c081><section class="VPSidebarItem level-0" data-v-8d50c081 data-v-d81de50c><div class="item" role="button" tabindex="0" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><h2 class="text" data-v-d81de50c>Inference</h2><!----></div><div class="items" data-v-d81de50c><!--[--><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/inference/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Overview</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-8d50c081><section class="VPSidebarItem level-0 collapsible" data-v-8d50c081 data-v-d81de50c><div class="item" role="button" tabindex="0" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><h2 class="text" data-v-d81de50c>Model Types</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-d81de50c><span class="vpi-chevron-right caret-icon" data-v-d81de50c></span></div></div><div class="items" data-v-d81de50c><!--[--><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/model-types/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/model-types/dense/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Dense Models</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/model-types/mixture-of-experts/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Mixture of Experts (MoE)</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-8d50c081><section class="VPSidebarItem level-0 collapsible has-active" data-v-8d50c081 data-v-d81de50c><div class="item" role="button" tabindex="0" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><h2 class="text" data-v-d81de50c>Recommendations</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-d81de50c><span class="vpi-chevron-right caret-icon" data-v-d81de50c></span></div></div><div class="items" data-v-d81de50c><!--[--><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/recommendations/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/recommendations/coding/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Coding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/recommendations/instruct/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Instruct-Tuned</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/recommendations/personal-assistant/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Personal Assistant</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/recommendations/stem/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>STEM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-d81de50c data-v-d81de50c><div class="item" data-v-d81de50c><div class="indicator" data-v-d81de50c></div><a class="VPLink link link" href="/vitepress-llm-recommends/recommendations/storywriting/" data-v-d81de50c><!--[--><p class="text" data-v-d81de50c>Storywriting</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-1df9f90f data-v-aff0b8d7><div class="VPDoc has-sidebar has-aside" data-v-aff0b8d7 data-v-7011f0d8><!--[--><!--]--><div class="container" data-v-7011f0d8><div class="aside" data-v-7011f0d8><div class="aside-curtain" data-v-7011f0d8></div><div class="aside-container" data-v-7011f0d8><div class="aside-content" data-v-7011f0d8><div class="VPDocAside" data-v-7011f0d8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-60d5052e><div class="content" data-v-60d5052e><div class="outline-marker" data-v-60d5052e></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-60d5052e>On this page</div><ul class="VPDocOutlineItem root" data-v-60d5052e data-v-2d0bdf9b><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-7011f0d8><div class="content-container" data-v-7011f0d8><!--[--><!--]--><main class="main" data-v-7011f0d8><div style="position:relative;" class="vp-doc _vitepress-llm-recommends_recommendations_personal-assistant_" data-v-7011f0d8><div><h1 id="personal-assistant-use-case" tabindex="-1">Personal Assistant Use Case <a class="header-anchor" href="#personal-assistant-use-case" aria-label="Permalink to “Personal Assistant Use Case”">​</a></h1><p>Recommendations for models that excel at memory, context retention, and personalized interactions.</p><blockquote><p>💡 <strong>Note</strong>: For personal assistant tasks—such as recalling preferences, maintaining conversation history, managing schedules, or adapting tone over time—<strong>instruct-tuned models with strong long-context handling</strong> are preferred over thinking-tuned variants. These models prioritize coherence, empathy, and user-specific adaptation over raw analytical power.</p></blockquote><p>Use the selector below to find the best <strong>assistant-like</strong> model for your hardware:</p><div class="_modelSelector_1y9gd_2 _modelSelectorRecommendedWarning_1y9gd_32" style="border-color:rgb(255, 122, 0);"><div class="_controls_1y9gd_54"><div class="_controlGroup_1y9gd_61"><label for="ram-select-0.6240717304799128" class="_controlGroupLabel_1y9gd_67">RAM (GB)</label><select id="ram-select-0.6972810087200698" class="_controlGroupSelect_1y9gd_74"><!--[--><option value="16" selected>16</option><option value="32">32</option><option value="64">64</option><option value="128">128</option><!--]--></select></div><div class="_controlGroup_1y9gd_61"><label for="vram-select-0.9363672449107778" class="_controlGroupLabel_1y9gd_67">VRAM (GB)</label><select id="vram-select-0.909540737926863" class="_controlGroupSelect_1y9gd_74"><!--[--><option value="0">0</option><option value="4">4</option><option value="6">6</option><option value="8" selected>8</option><option value="12">12</option><option value="16">16</option><option value="24">24</option><option value="32">32</option><!--]--></select></div><div class="_contextControlGroup_1y9gd_96"><label class="_contextControlLabel_1y9gd_103">Context Size: 16384 tokens</label><div class="_contextSliderContainer_1y9gd_110"><div class="_contextSliderWrapper_1y9gd_116"><input type="range" min="0" max="8" value="5" class="_contextSlider_1y9gd_110"></div></div></div></div><div class="_result_1y9gd_177"><strong class="_resultStrong_1y9gd_182">Recommended model:</strong><span class="_modelName_1y9gd_189 _modelNameRecommendedWarning_1y9gd_220" style="background-color:var(--vp-c-bg-soft);color:var(--vp-c-text-1);border-color:rgb(255, 122, 0);">Gemma 3 12B 12B (Q4_K_XL)</span><a href="lmstudio://open_from_hf?model=unsloth/gemma-3-12b-it-GGUF" target="_blank" class="_lmstudioButton_1y9gd_372"> Use in LMStudio </a><!----><div class="_details_1y9gd_240"><small> Parameters: 12B | Quantization: Q4_K_XL</small></div></div><div class="_contextDetails_1y9gd_284"><strong>Context Overhead:</strong> 4.16GB (based on 16,384 token context) </div><div class="_memorySection_1y9gd_246"><table class="_memoryTable_1y9gd_306"><thead><tr><th class=""></th><th>Total</th><th>Context</th><th>Model</th><th>Leftover</th></tr></thead><tbody><tr><td class="">RAM</td><td class="">16.00GB</td><td class="">0.00GB</td><td class="">3.25GB</td><td class="">4.75GB</td></tr><tr><td class="">VRAM</td><td class="">8.00GB</td><td class="">4.16GB</td><td class="">2.75GB</td><td class="">0.09GB</td></tr></tbody></table></div></div><blockquote><p><strong>“Not recommended” means poor conversational memory and unreliable personalization</strong> On under-resourced systems, models may forget prior context within a few exchanges or fail to maintain consistent user preferences—making them ineffective as true personal assistants.</p></blockquote><hr><h2 id="how-to-use-instruct-tuned-models-as-a-personal-assistant" tabindex="-1">How to Use Instruct-Tuned Models as a Personal Assistant <a class="header-anchor" href="#how-to-use-instruct-tuned-models-as-a-personal-assistant" aria-label="Permalink to “How to Use Instruct-Tuned Models as a Personal Assistant”">​</a></h2><p>Personal assistant models thrive on <strong>long-term context awareness</strong>, <strong>empathetic tone</strong>, and <strong>user-specific adaptation</strong>. These prioritize natural dialogue, recall simulation, and task coordination.</p><h3 id="_1-choose-instruct-tuned-models" tabindex="-1">1. <strong>Choose Instruct-Tuned Models</strong> <a class="header-anchor" href="#_1-choose-instruct-tuned-models" aria-label="Permalink to “1. Choose Instruct-Tuned Models”">​</a></h3><ul><li>Use <strong><code>instruct</code></strong> variants (e.g., <code>qwen3 4b instruct</code>, <code>gpt-oss 20b</code>)—they&#39;re fine-tuned for: <ul><li>Following multi-turn instructions</li><li>Remembering stated preferences (“I prefer morning summaries”)</li><li>Managing to-do lists, reminders, or journaling prompts</li><li>Adapting tone (casual, professional, supportive)</li></ul></li><li>Avoid <code>thinking</code> models—they may over-analyze simple requests or ignore emotional nuance.</li></ul><h3 id="_2-prioritize-context-length-quantization" tabindex="-1">2. <strong>Prioritize Context Length &amp; Quantization</strong> <a class="header-anchor" href="#_2-prioritize-context-length-quantization" aria-label="Permalink to “2. Prioritize Context Length &amp; Quantization”">​</a></h3><table tabindex="0"><thead><tr><th>Quant</th><th>Assistant Impact</th></tr></thead><tbody><tr><td><code>bf16</code> / <code>f16</code></td><td>Best for full personality retention over long chats; ideal if you use 32K+ context</td></tr><tr><td><code>q8</code></td><td>Excellent balance—retains nuance while fitting in moderate VRAM</td></tr><tr><td><code>q6</code> / <code>q4</code></td><td>Usable for basic tasks, but may “forget” early conversation details in long sessions</td></tr></tbody></table><blockquote><p>🔹 <strong>Tip</strong>: For personal assistants, <strong>context length matters more than raw parameter count</strong>. A well-quantized 20B model with 32K context often outperforms a 30B model limited to 4K.</p></blockquote><h3 id="_3-enable-full-gpu-offload" tabindex="-1">3. <strong>Enable Full GPU Offload</strong> <a class="header-anchor" href="#_3-enable-full-gpu-offload" aria-label="Permalink to “3. Enable Full GPU Offload”">​</a></h3><ul><li>Always use <strong>full GPU offload</strong> (e.g., 48/48 layers) in LM Studio to keep conversation fast and responsive.</li><li>If LM Studio pre-selects offload settings for your model, <strong>do not override them</strong>.</li></ul><h3 id="_4-simulate-memory-with-prompt-engineering" tabindex="-1">4. <strong>Simulate Memory with Prompt Engineering</strong> <a class="header-anchor" href="#_4-simulate-memory-with-prompt-engineering" aria-label="Permalink to “4. Simulate Memory with Prompt Engineering”">​</a></h3><p>Since local models lack true persistent memory:</p><ul><li><strong>Seed your prompt</strong> with key facts:<div class="language-text"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span>You are my personal assistant. I&#39;m a developer who enjoys Blender 3D. I eat meals regularly and track my learning goals. Today is Thursday, October 23, 2025.</span></span></code></pre></div></li><li>Use <strong>structured recall cues</strong>: <ul><li>“Based on our last conversation about Blender shaders…”</li><li>“Remind me of my food preferences before suggesting dinner ideas.”</li></ul></li></ul><h3 id="_5-avoid-none-configurations" tabindex="-1">5. <strong>Avoid “None” Configurations</strong> <a class="header-anchor" href="#_5-avoid-none-configurations" aria-label="Permalink to “5. Avoid “None” Configurations”">​</a></h3><ul><li>Systems returning “none” lack the capacity to maintain even short-term conversational state.</li><li>Forcing a load via CPU offload leads to: <ul><li>Slow responses that break conversational flow</li></ul></li><li><strong>Minimum viable setup</strong>: ≥6 GB VRAM + <code>qwen3 4b instruct q4</code> for basic assistant duties.</li></ul><h3 id="_6-combine-with-external-memory-advanced" tabindex="-1">6. <strong>Combine with External Memory (Advanced)</strong> <a class="header-anchor" href="#_6-combine-with-external-memory-advanced" aria-label="Permalink to “6. Combine with External Memory (Advanced)”">​</a></h3><p>For true long-term memory:</p><ul><li>Log key interactions to a local file or database</li><li>Inject summarized memory into each new session:<div class="language-text"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span>[Memory Summary: User is learning Blender geometry nodes. Last discussed procedural terrain generation on Oct 20. Prefers vegetarian meal suggestions.]</span></span></code></pre></div></li><li>This turns even a 4B model into a surprisingly capable companion.</li></ul><h3 id="_7-monitor-resource-usage" tabindex="-1">7. <strong>Monitor Resource Usage</strong> <a class="header-anchor" href="#_7-monitor-resource-usage" aria-label="Permalink to “7. Monitor Resource Usage”">​</a></h3><ul><li>Keep VRAM usage <strong>below 90%</strong> to avoid swapping, which destroys real-time responsiveness.</li><li>On Windows, disable background apps (Discord, browsers) to free up the extra 1-2 GB needed for smooth 16K context.</li></ul><hr><p>By selecting the right instruct-tuned model for your hardware and structuring interactions to simulate memory, you can create a <strong>responsive, personalized, and helpful local AI assistant</strong>—without relying on cloud services or sacrificing privacy.</p></div></div></main><footer class="VPDocFooter" data-v-7011f0d8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/vitepress-llm-recommends/recommendations/instruct/" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>Instruct-Tuned</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/vitepress-llm-recommends/recommendations/stem/" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>STEM</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-1df9f90f data-v-c3855bb3><div class="container" data-v-c3855bb3><p class="message" data-v-c3855bb3>Released under the MIT License.</p><p class="copyright" data-v-c3855bb3>Copyright © 2025 Maximilian Kruse</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"evaluation_index.md\":\"C14o8Jc1\",\"index.md\":\"Ub8Qo3-D\",\"inference_index.md\":\"DP9GxRBj\",\"model-types_dense_index.md\":\"CbxFSXIU\",\"model-types_index.md\":\"DN1SIc1e\",\"model-types_mixture-of-experts_index.md\":\"BAe7wrUj\",\"recommendations_coding_index.md\":\"-v1BPGUh\",\"recommendations_index.md\":\"7YzTQJF3\",\"recommendations_instruct_index.md\":\"BDSPyuCA\",\"recommendations_personal-assistant_index.md\":\"B6d8-ReY\",\"recommendations_stem_index.md\":\"fiAHEwGz\",\"recommendations_storywriting_index.md\":\"DvKj-U_i\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"AI Model Guide\",\"description\":\"A practical guide to understanding and using modern AI models.\",\"base\":\"/vitepress-llm-recommends/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Evaluation\",\"link\":\"/evaluation/\"},{\"text\":\"Inference\",\"link\":\"/inference/\"},{\"text\":\"Model Types\",\"link\":\"/model-types/\"},{\"text\":\"Recommendations\",\"link\":\"/recommendations/\"}],\"sidebar\":[{\"text\":\"Introduction\",\"items\":[{\"text\":\"Home\",\"link\":\"/\"}]},{\"text\":\"Evaluation\",\"items\":[{\"text\":\"Overview\",\"link\":\"/evaluation/\"}]},{\"text\":\"Inference\",\"items\":[{\"text\":\"Overview\",\"link\":\"/inference/\"}]},{\"text\":\"Model Types\",\"collapsed\":false,\"items\":[{\"text\":\"Overview\",\"link\":\"/model-types/\"},{\"text\":\"Dense Models\",\"link\":\"/model-types/dense/\"},{\"text\":\"Mixture of Experts (MoE)\",\"link\":\"/model-types/mixture-of-experts/\"}]},{\"text\":\"Recommendations\",\"collapsed\":false,\"items\":[{\"text\":\"Overview\",\"link\":\"/recommendations/\"},{\"text\":\"Coding\",\"link\":\"/recommendations/coding/\"},{\"text\":\"Instruct-Tuned\",\"link\":\"/recommendations/instruct/\"},{\"text\":\"Personal Assistant\",\"link\":\"/recommendations/personal-assistant/\"},{\"text\":\"STEM\",\"link\":\"/recommendations/stem/\"},{\"text\":\"Storywriting\",\"link\":\"/recommendations/storywriting/\"}]}],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/MaxKruse/vitepress-llm-recommends/\"}],\"footer\":{\"message\":\"Released under the MIT License.\",\"copyright\":\"Copyright © 2025 Maximilian Kruse\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false,\"additionalConfig\":{}}");</script>
    
  </body>
</html>